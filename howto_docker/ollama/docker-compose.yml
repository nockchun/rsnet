version: "3.9"

services:
  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "${PORT:-11434}:11434"
    volumes:
      - ${OLLAMA_MODELS}:/root/.ollama
      - ${OLLAMA_CODE}:/code
    environment:
      - OLLAMA_HOST=0.0.0.0:${PORT}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE}
      - CUDA_VISIBLE_DEVICES=${GPU_ID}
      - NVIDIA_VISIBLE_DEVICES=${GPU_ID}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["${GPU_ID}"]
              capabilities: [gpu]
    networks: [ollama-docker]

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    depends_on: [ollama]
    restart: unless-stopped
    ports:
      - "${WEBUI_PORT:-8080}:8080"
    volumes:
      - ${WEBUI_DATA_DIR:-./ollama/ollama-webui}:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:${PORT}
      - OLLAMA_API_BASE_URL=http://ollama:${PORT}
      - WEBUI_AUTH=${WEBUI_AUTH:-False}
      - WEBUI_NAME=${WEBUI_NAME:-Open WebUI}
      - WEBUI_URL=${WEBUI_URL:-http://localhost:${WEBUI_PORT:-8080}}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-change-me}
    networks: [ollama-docker]

networks:
  ollama-docker:
    external: false

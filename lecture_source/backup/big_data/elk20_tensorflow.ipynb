{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b><center>Elasticsearch 7.7</center></b>\n",
    "    <b><center>데이터 모델링</center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "import numpy as np\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def embed_text(input):\n",
    "    vectors = model(input)\n",
    "    return [vector.numpy() for vector in vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Elephant\n",
      "Embedding size: 512\n",
      "Embedding: [0.008344489149749279, 0.0004808177181985229, 0.06595245748758316, ...]\n",
      "\n",
      "Message: I am a sentence for which I would like to get its embedding.\n",
      "Embedding size: 512\n",
      "Embedding: [0.0508086159825325, -0.016524311155080795, 0.015737785026431084, ...]\n",
      "\n",
      "Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\n",
      "Embedding size: 512\n",
      "Embedding: [-0.028332680463790894, -0.0558621883392334, -0.012941470369696617, ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_embeddings = embed_text(messages)\n",
    "\n",
    "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "    print(\"Message: {}\".format(messages[i]))\n",
    "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "    message_embedding_snippet = \", \".join(\n",
    "        (str(x) for x in message_embedding[:3]))\n",
    "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch Indices 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"posts\"\n",
    "INDEX_FILE = \"posts_index.json\"\n",
    "DATA_FILE = \"posts_data.json\"\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "SEARCH_SIZE = 5\n",
    "GPU_LIMIT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(hosts=\"elastic.rsnet\", port=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(INDEX_NAME)\n",
    "    \n",
    "with open(INDEX_FILE) as index_file:\n",
    "    source = index_file.read().strip()\n",
    "    es.indices.create(index=INDEX_NAME, body=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stackoverflow data 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "count = 0\n",
    "\n",
    "def index_batch(docs):\n",
    "    titles = [doc[\"title\"] for doc in docs]\n",
    "    title_vectors = embed_text(titles)\n",
    "\n",
    "    requests = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        request = doc\n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        request[\"title_vector\"] = title_vectors[i]\n",
    "        requests.append(request)\n",
    "    helpers.bulk(es, requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 50 documents.\n",
      "Indexed 100 documents.\n",
      "Indexed 150 documents.\n",
      "Indexed 200 documents.\n",
      "Indexed 250 documents.\n",
      "Indexed 300 documents.\n",
      "Indexed 350 documents.\n",
      "Indexed 400 documents.\n",
      "Indexed 450 documents.\n",
      "Indexed 500 documents.\n",
      "Indexed 550 documents.\n",
      "Indexed 600 documents.\n",
      "Indexed 650 documents.\n",
      "Indexed 700 documents.\n",
      "Indexed 750 documents.\n",
      "Indexed 800 documents.\n",
      "Indexed 850 documents.\n",
      "Indexed 900 documents.\n",
      "Indexed 950 documents.\n",
      "Indexed 1000 documents.\n",
      "Indexed 1050 documents.\n",
      "Indexed 1100 documents.\n",
      "Indexed 1150 documents.\n",
      "Indexed 1200 documents.\n",
      "Indexed 1250 documents.\n",
      "Indexed 1300 documents.\n",
      "Indexed 1350 documents.\n",
      "Indexed 1400 documents.\n",
      "Indexed 1450 documents.\n",
      "Indexed 1500 documents.\n",
      "Indexed 1550 documents.\n",
      "Indexed 1600 documents.\n",
      "Indexed 1650 documents.\n",
      "Indexed 1700 documents.\n",
      "Indexed 1750 documents.\n",
      "Indexed 1800 documents.\n",
      "Indexed 1850 documents.\n",
      "Indexed 1900 documents.\n",
      "Indexed 1950 documents.\n",
      "Indexed 2000 documents.\n",
      "Indexed 2050 documents.\n",
      "Indexed 2100 documents.\n",
      "Indexed 2150 documents.\n",
      "Indexed 2200 documents.\n",
      "Indexed 2250 documents.\n",
      "Indexed 2300 documents.\n",
      "Indexed 2350 documents.\n",
      "Indexed 2400 documents.\n",
      "Indexed 2450 documents.\n",
      "Indexed 2500 documents.\n",
      "Indexed 2550 documents.\n",
      "Indexed 2600 documents.\n",
      "Indexed 2650 documents.\n",
      "Indexed 2700 documents.\n",
      "Indexed 2750 documents.\n",
      "Indexed 2800 documents.\n",
      "Indexed 2850 documents.\n",
      "Indexed 2900 documents.\n",
      "Indexed 2950 documents.\n",
      "Indexed 3000 documents.\n",
      "Indexed 3050 documents.\n",
      "Indexed 3100 documents.\n",
      "Indexed 3150 documents.\n",
      "Indexed 3200 documents.\n",
      "Indexed 3250 documents.\n",
      "Indexed 3300 documents.\n",
      "Indexed 3350 documents.\n",
      "Indexed 3400 documents.\n",
      "Indexed 3450 documents.\n",
      "Indexed 3500 documents.\n",
      "Indexed 3550 documents.\n",
      "Indexed 3600 documents.\n",
      "Indexed 3650 documents.\n",
      "Indexed 3700 documents.\n",
      "Indexed 3750 documents.\n",
      "Indexed 3800 documents.\n",
      "Indexed 3850 documents.\n",
      "Indexed 3900 documents.\n",
      "Indexed 3950 documents.\n",
      "Indexed 3997 documents.\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_FILE) as data_file:\n",
    "    for line in data_file:\n",
    "        line = line.strip()\n",
    "\n",
    "        doc = json.loads(line)\n",
    "        if doc[\"type\"] != \"question\":\n",
    "            continue\n",
    "\n",
    "        docs.append(doc)\n",
    "        count += 1\n",
    "\n",
    "        if count % BATCH_SIZE == 0:\n",
    "            index_batch(docs)\n",
    "            docs = []\n",
    "            print(\"Indexed {} documents.\".format(count))\n",
    "    \n",
    "    if docs:\n",
    "        index_batch(docs)\n",
    "        print(\"Indexed {} documents.\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title Vector로 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_query():\n",
    "    query = input(\"Enter query: \")\n",
    "\n",
    "    embedding_start = time.time()\n",
    "    query_vector = embed_text([query])[0]\n",
    "    embedding_time = time.time() - embedding_start\n",
    "\n",
    "    script_query = {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\"match_all\": {}},\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['title_vector']) + 1.0\",\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_start = time.time()\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\": {\"includes\": [\"title\", \"body\"]}\n",
    "        }\n",
    "    )\n",
    "    search_time = time.time() - search_start\n",
    "\n",
    "    print()\n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"]))\n",
    "    print(\"embedding time: {:.2f} ms\".format(embedding_time * 1000))\n",
    "    print(\"search time: {:.2f} ms\".format(search_time * 1000))\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"]))\n",
    "        print(hit[\"_source\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

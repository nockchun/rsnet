{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, unicodedata, pathlib, random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 경로/디바이스\n",
    "DATA_DIR = pathlib.Path(\"data/ko_wikisource_clean\"); DATA_DIR.mkdir(exist_ok=True)\n",
    "RAW_TXT  = DATA_DIR/\"raw.txt\"\n",
    "NORM_TXT = DATA_DIR/\"norm_nfc.txt\"\n",
    "SPM_PREFIX = str(DATA_DIR/\"spm_ko\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요에 따라 더 추가하세요\n",
    "URLS = [\n",
    "    \"https://ko.wikisource.org/wiki/B%EB%85%80%EC%9D%98_%EC%86%8C%EB%AC%98\",  # B녀의 소묘\n",
    "    \"https://ko.wikisource.org/wiki/%EB%8C%80%EB%8F%99%EA%B0%95%EC%9D%80_%EC%86%8D%EC%82%AD%EC%9D%B8%EB%8B%A4\",  # 대동강은 속삭인다\n",
    "    \"https://ko.wikisource.org/wiki/%EB%8C%80%ED%83%95%EC%A7%80_%EC%95%84%EC%A3%BC%EB%A8%B8%EB%8B%88\",  # 대탕지 아주머니\n",
    "    \"https://ko.wikisource.org/wiki/%EB%8F%84%EC%8B%9C%EC%99%80_%EC%9C%A0%EB%A0%B9\",  # 도시와 유령\n",
    "    \"https://ko.wikisource.org/wiki/%EB%8F%84%EC%A0%95\", # 도정\n",
    "    \"https://ko.wikisource.org/wiki/%EB%A7%8C%EB%AC%B4%EB%B0%A9\", # 만무방\n",
    "    \"https://ko.wikisource.org/wiki/%EB%AC%B4%EB%AA%85%EC%B4%88\", # 무명초\n",
    "    \"https://ko.wikisource.org/wiki/%EB%AC%BC\", # 물\n",
    "    \"https://ko.wikisource.org/wiki/%EB%AC%BC%EB%A0%88%EB%B0%A9%EC%95%84\", # 물레방아\n",
    "    \"https://ko.wikisource.org/wiki/%EB%B0%98%EC%97%AD%EC%9E%90\", # 반역자\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9A%A9%EA%B3%BC_%EC%9A%A9%EC%9D%98_%EB%8C%80%EA%B2%A9%EC%A0%84\", # 용과 용의 대격전\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9A%B0%EC%97%B0%EC%9D%98_%EA%B8%B0%EC%A0%81\", # 우연의 기적\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9A%B4%EC%88%98_%EC%A2%8B%EC%9D%80_%EB%82%A0\", # 운수 좋은 날\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9B%90%EC%88%98%EB%A1%9C_%EC%9D%80%EC%9D%B8\", # 원수로 은인\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9C%A0%EB%AC%B4\", # 유무\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9C%A4%EA%B4%91%ED%98%B8\", # 윤광호\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9D%B4_%EC%9E%94%EC%9D%84\", # 이 잔을\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9D%B4%EC%8B%9D%EA%B3%BC_%EB%8F%84%EC%8A%B9\", # 이식과 도승\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9D%BC%ED%91%9C%EC%9D%98_%EA%B3%B5%EB%8A%A5\", # 일표의 공능\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9E%A1%EC%B4%88\", # 잡초\n",
    "    \"https://ko.wikisource.org/wiki/%EC%9E%A5%EB%AF%B8_%EB%B3%91%EB%93%A4%EB%8B%A4\", # 장미 병들다\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A0%81%EA%B4%B4%EC%9C%A0%EC%9D%98\", # 적괴유의\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A0%81%EB%A7%89%ED%95%9C_%EC%A0%80%EB%85%81\", # 적막한 저녁\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A0%81%EB%B9%88\", # 적빈\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A0%84%EC%A0%9C%EC%9E%90\", # 전제자\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A0%95%EC%97%B4%EC%9D%98_%EB%82%99%EB%9E%91%EA%B3%B5%EC%A3%BC\", # 정열의 낙랑공주\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A0%95%EC%A1%B0_(%EA%B9%80%EC%9C%A0%EC%A0%95)\", # 정조 (김유정)\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A0%95%ED%9D%AC\", # 정희\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A2%85%EC%83%9D%EA%B8%B0\", # 종생기\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A3%84%EC%99%80_%EB%B2%8C_(%EA%B9%80%EB%8F%99%EC%9D%B8)\", # 죄와 벌 (김동인)\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A7%80%EB%8F%84%EC%9D%98_%EC%95%94%EC%8B%A4\", # 지도의 암실\n",
    "    \"https://ko.wikisource.org/wiki/%EC%A7%80%ED%95%98%EC%B4%8C\", # 지하촌\n",
    "    \"https://ko.wikisource.org/wiki/%EC%AB%93%EA%B8%B0%EC%96%B4_%EA%B0%80%EB%8A%94_%EC%9D%B4%EB%93%A4\", # 쫓기어 가는 이들\n",
    "    \"https://ko.wikisource.org/wiki/%EC%B2%AD%EC%B6%98\", # 청춘\n",
    "    \"https://ko.wikisource.org/wiki/%EC%B4%88%EC%B7%8C%EC%97%B0%ED%99%94%ED%8E%B8\", # 초췌연화편\n",
    "    \"https://ko.wikisource.org/wiki/%EC%B4%9D%EA%B0%81%EA%B3%BC_%EB%A7%B9%EA%BD%81%EC%9D%B4\", # 총각과 맹꽁이\n",
    "    \"https://ko.wikisource.org/wiki/%EC%B9%98%EC%88%99\", # 치숙\n",
    "    \"https://ko.wikisource.org/wiki/%ED%83%9C%ED%98%95\", # 태형\n",
    "    \"https://ko.wikisource.org/wiki/%ED%88%AC%ED%99%98%EA%B8%88%EC%9D%80\", # 투환금은\n",
    "    \"https://ko.wikisource.org/wiki/%ED%95%B4%EB%8F%8B%EC%9D%B4\", # 해돋이\n",
    "    \"https://ko.wikisource.org/wiki/%ED%99%8D%EC%9C%A4%EC%84%B1%EA%B3%BC_%EC%A0%88%EB%B6%80\", # 홍윤성과 절부\n",
    "    \"https://ko.wikisource.org/wiki/%EC%82%AC%EA%B0%81%EC%A0%84%EA%B8%B0\", # 사각전기\n",
    "    \"https://ko.wikisource.org/wiki/%EC%82%AC%EC%83%9D%EC%95%84\", # 사생아\n",
    "    \"https://ko.wikisource.org/wiki/%EC%82%AC%EC%9C%84\", # 사위\n",
    "    \"https://ko.wikisource.org/wiki/%EC%82%B0%EA%B3%A8\", # 산골\n",
    "    \"https://ko.wikisource.org/wiki/%EC%82%B0%EA%B3%A8_%EB%82%98%EA%B7%B8%EB%84%A4\", # 산골 나그네\n",
    "    \"https://ko.wikisource.org/wiki/%EC%82%B0%EB%82%A8\", # 산남\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikisource_text(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "    r = requests.get(url, headers=headers, stream=True, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # 본문 컨테이너\n",
    "    content = soup.select_one(\"div.mw-parser-output\")\n",
    "    if content is None:\n",
    "        return \"\"\n",
    "\n",
    "    # 표/주석/각주 등 제거\n",
    "    for selector in [\"table\", \"div.reflist\", \"ol.references\", \"sup.reference\"]:\n",
    "        for tag in content.select(selector):\n",
    "            tag.decompose()\n",
    "\n",
    "    # 문단과 리스트 텍스트만 추출\n",
    "    texts = []\n",
    "    for tag in content.find_all([\"p\",\"li\",\"dd\",\"dt\",\"blockquote\"]):\n",
    "        txt = tag.get_text(\" \", strip=True)\n",
    "        if txt:\n",
    "            texts.append(txt)\n",
    "\n",
    "    text = \"\\n\".join(texts)\n",
    "\n",
    "    # 위키마크업 잔여 정리\n",
    "    text = re.sub(r\"\\[편집\\]|\\[.*?편집\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T11:11:51.442548Z",
     "iopub.status.busy": "2025-09-06T11:11:51.442237Z",
     "iopub.status.idle": "2025-09-06T11:11:51.445782Z",
     "shell.execute_reply": "2025-09-06T11:11:51.445028Z",
     "shell.execute_reply.started": "2025-09-06T11:11:51.442523Z"
    }
   },
   "source": [
    "# 수집 → NFC 정규화 → 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "for u in tqdm(URLS):\n",
    "    try:\n",
    "        t = fetch_wikisource_text(u)\n",
    "        print(f\"Fetched {len(t):,} chars from {u}\")\n",
    "        if len(t) > 1000:\n",
    "            all_texts.append(f\"\\n\\n### SOURCE: {u}\\n{t}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", u, e)\n",
    "\n",
    "raw_text = \"\\n\".join(all_texts)\n",
    "RAW_TXT.write_text(raw_text, encoding=\"utf-8\")\n",
    "\n",
    "# 한글 자모 분리 방지: NFC 정규화\n",
    "norm_text = unicodedata.normalize(\"NFC\", raw_text)\n",
    "# 공백 정리\n",
    "norm_text = re.sub(r\"\\r\\n?\", \"\\n\", norm_text)\n",
    "norm_text = re.sub(r\"[ \\t]+\", \" \", norm_text)\n",
    "norm_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", norm_text)\n",
    "\n",
    "NORM_TXT.write_text(norm_text, encoding=\"utf-8\")\n",
    "(len(raw_text), len(norm_text), str(NORM_TXT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# 1) 한글 음절만 required에 유지\n",
    "ko_chars = \"\".join(chr(c) for c in range(0xAC00, 0xD7A4))  # 가–힣\n",
    "\n",
    "# 2) 문장부호는 user_defined_symbols로만 (원래 리스트 그대로 사용 가능)\n",
    "user_syms = [\"《\",\"》\",\"〈\",\"〉\",\"—\",\"…\",\"“\",\"”\",\"‘\",\"’\",\"·\",\"『\",\"』\",\"「\",\"」\",\"‧\"]\n",
    "\n",
    "# 3) 혹시라도 겹치는 게 있으면 required에서 제거 (방어적)\n",
    "required = \"\".join(ch for ch in ko_chars if ch not in set(user_syms))\n",
    "\n",
    "kwargs = dict(\n",
    "    input=str(NORM_TXT),\n",
    "    model_prefix=SPM_PREFIX,\n",
    "    model_type=\"unigram\",\n",
    "    vocab_size=16000,\n",
    "    character_coverage=0.9999,\n",
    "    # ★ 라운드트립 보장용\n",
    "    normalization_rule_name=\"identity\",  # 원문 보존\n",
    "    byte_fallback=True,                  # 미등록 문자도 안전 복원\n",
    "    hard_vocab_limit=False,\n",
    "    add_dummy_prefix=True,               # 디코딩 시 원문 공백 복원에 문제 없음\n",
    "    unk_id=0, unk_surface=\"<unk>\",\n",
    "    bos_id=-1, eos_id=-1, pad_id=-1,\n",
    "    user_defined_symbols=user_syms,\n",
    "    max_sentence_length=100000,\n",
    "    input_sentence_size=4_000_000,\n",
    "    shuffle_input_sentence=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    spm.SentencePieceTrainer.train(required_chars=required, **kwargs)\n",
    "except TypeError:\n",
    "    # 구버전 sentencepiece면 required_chars 미지원 → 그냥 진행\n",
    "    print(\"[WARN] required_chars 미지원 → 커버리지/어휘 설정으로 진행합니다.\")\n",
    "    spm.SentencePieceTrainer.train(**kwargs)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(SPM_PREFIX + \".model\")\n",
    "\n",
    "print(\"vocab_size:\", sp.get_piece_size(),\n",
    "      \"unk_id:\", sp.unk_id(), \"unk_piece:\", sp.id_to_piece(sp.unk_id()))\n",
    "\n",
    "# --- 라운드트립 체크 (예시) ---\n",
    "def roundtrip_ok(txt: str) -> bool:\n",
    "    pieces = sp.encode(txt, out_type=str)\n",
    "    back = sp.decode(pieces)\n",
    "    return txt == back\n",
    "\n",
    "test_str = \"《테스트》 “따옴표”… — 『괄호』 「문장」 ‧ 끝!\"\n",
    "print(\"roundtrip:\", roundtrip_ok(test_str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 무손실 확인 (decode == 원문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def debug_roundtrip(s):\n",
    "    s_nfc = unicodedata.normalize(\"NFC\", s)\n",
    "    pieces = sp.encode(s_nfc, out_type=str)\n",
    "    ids    = sp.encode(s_nfc, out_type=int)\n",
    "    back_p = sp.decode(pieces)\n",
    "    back_i = sp.decode(ids)\n",
    "    print(\"pieces:\", pieces)\n",
    "    print(\"ids   :\", ids)\n",
    "    print(\"back_p:\", back_p)\n",
    "    print(\"back_i:\", back_i)\n",
    "    # UNK 존재 여부\n",
    "    unk_positions = [i for i,t in enumerate(ids) if t == sp.unk_id()]\n",
    "    if unk_positions:\n",
    "        print(\"⚠️  UNK at positions:\", unk_positions)\n",
    "    assert back_i == s_nfc, \"decode(encode(x))가 원문과 다릅니다!\"\n",
    "\n",
    "test_line = \"성춘향은 이 도령을 깊이 사모하였더라.\"\n",
    "debug_roundtrip(test_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언어모델용 Dataset/DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋\n",
    "ids_full = sp.encode(NORM_TXT.read_text(encoding=\"utf-8\"), out_type=int)\n",
    "\n",
    "# --- replace your LMDataset & split ---\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, ids, block=256, indices=None):\n",
    "        self.ids = torch.tensor(ids, dtype=torch.long)\n",
    "        self.block = block\n",
    "        N = len(self.ids) - block - 1\n",
    "        if indices is None:\n",
    "            self.indices = torch.arange(N)\n",
    "        else:\n",
    "            self.indices = indices\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        x = self.ids[i:i+self.block]\n",
    "        y = self.ids[i+1:i+self.block+1]\n",
    "        return x, y\n",
    "\n",
    "block_size, batch_size = 256, 64\n",
    "N = len(ids_full) - block_size - 1\n",
    "all_idx = torch.randperm(N)\n",
    "n_tr = int(N*0.9)\n",
    "train_idx, val_idx = all_idx[:n_tr], all_idx[n_tr:]\n",
    "\n",
    "train_ds = LMDataset(ids_full, block_size, train_idx)\n",
    "val_ds   = LMDataset(ids_full, block_size, val_idx)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델\n",
    "- LSTM 은 순차 문맥을 인코딩\n",
    "- nn.MultiheadAttention(batch_first=True) 로 자기어텐션을 적용\n",
    "- Causal mask 로 미래 토큰 차단 (언어모델 특성 유지)\n",
    "- 잔차(residual) + LayerNorm 으로 안정화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "class LSTMAttnLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb=384, hid=384, layers=2, heads=6, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb)\n",
    "        self.lstm = nn.LSTM(emb, hid, num_layers=layers, batch_first=True, dropout=drop)\n",
    "        self.attn = nn.MultiheadAttention(hid, heads, dropout=drop, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(hid)\n",
    "        self.fc = nn.Linear(hid, vocab_size, bias=False)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        # weight tying\n",
    "        if emb != hid:\n",
    "            self.proj = nn.Linear(hid, emb, bias=False)\n",
    "        else:\n",
    "            self.proj = nn.Identity()\n",
    "        # fc.weight shares with embedding\n",
    "        self.fc.weight = self.emb.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        h,_ = self.lstm(self.emb(x))           # (B,T,H)\n",
    "        T = h.size(1)\n",
    "        mask = torch.full((T,T), float(\"-inf\"), device=h.device).triu(1)\n",
    "        a,_ = self.attn(h, h, h, attn_mask=mask, need_weights=False)\n",
    "        y = self.ln(h + self.drop(a))\n",
    "        y = self.proj(y)                       # to emb dim if needed\n",
    "        return self.fc(y)\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "model = LSTMAttnLM(vocab_size).to(device)\n",
    "sum(p.numel() for p in model.parameters())/1e6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        if train: optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * y.numel()         # 토큰 수로 집계\n",
    "        total_tokens += y.numel()\n",
    "    if train: scheduler.step()\n",
    "    return total_loss / max(1, total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total, n = 0.0, 0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        if train: optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        total += loss.item()*x.size(0); n += x.size(0)\n",
    "    return total/max(1,n)\n",
    "\n",
    "best = 1e9\n",
    "for ep in range(1, 10):  # 데모: 5 epoch\n",
    "    tr = run_epoch(train_loader, True)\n",
    "    va = run_epoch(val_loader, False)\n",
    "    if va < best:\n",
    "        best = va\n",
    "        torch.save(model.state_dict(), DATA_DIR/\"best_lstm_attn.pt\")\n",
    "    print(f\"[{ep:02d}] train_ce={tr:.4f}  val_ce={va:.4f}\")\n",
    "torch.save(model.state_dict(), DATA_DIR/\"best_lstm_attn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, sp, prompt, max_new_tokens=300, temperature=0.9, top_k=50):\n",
    "    model.eval()\n",
    "    x = torch.tensor(sp.encode(prompt, out_type=int), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            x_cond = x[:, -block_size:]               # 길이 제한\n",
    "            logits = model(x_cond)                    # (1, T, V)\n",
    "            last = logits[:, -1, :] / max(1e-6, temperature)\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(last, k=min(top_k, last.size(-1)))\n",
    "                last = torch.where(last < v[:, -1].unsqueeze(-1), torch.full_like(last, -1e10), last)\n",
    "            probs = torch.softmax(last, dim=-1)\n",
    "            next_id = torch.multinomial(probs, 1)     # (1,1)\n",
    "            x = torch.cat([x, next_id], dim=1)\n",
    "    return sp.decode(x[0].tolist())\n",
    "\n",
    "model.load_state_dict(torch.load(DATA_DIR/\"best_lstm_attn.pt\", map_location=device))\n",
    "print(sample(model, sp, \"성춘향은\", max_new_tokens=300, temperature=0.9, top_k=50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

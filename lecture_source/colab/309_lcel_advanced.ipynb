{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR8E6URZK1X3"
      },
      "source": [
        "# 기본환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yLT1RFEeQgm"
      },
      "outputs": [],
      "source": [
        "# !pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esrEO7yteQgm"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_KEY = userdata.get(\"HF_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPGOz6I1JkBj"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login(HF_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_XVtfyiLiAi"
      },
      "source": [
        "# 모델 로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEBCNVjffATu"
      },
      "outputs": [],
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oQenpTCPMyh"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU6jp1fqeQgn"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btydbHNpeQgn"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = 1024*5, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    # device_map = {\"\": device}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPtcr98MeQgn"
      },
      "outputs": [],
      "source": [
        "model = FastModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Ej1_yjeQgn"
      },
      "source": [
        "# Custom ChatModel 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf4trrKVeQgn"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Iterable, Iterator, List, Literal, Optional, Type, Union\n",
        "from pydantic import BaseModel as PydanticBaseModel\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_core.messages import AIMessage, AIMessageChunk, HumanMessage, SystemMessage, BaseMessage\n",
        "from langchain_core.outputs import ChatGeneration, ChatResult, ChatGenerationChunk\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from transformers import TextIteratorStreamer\n",
        "import threading\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9o8PLUs-azj"
      },
      "outputs": [],
      "source": [
        "class GemmaChatModel(BaseChatModel):\n",
        "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9, verbose: bool = False, **kwargs: Any):\n",
        "        super().__init__()\n",
        "        object.__setattr__(self, \"model\", model)\n",
        "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
        "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
        "        object.__setattr__(self, \"do_sample\", do_sample)\n",
        "        object.__setattr__(self, \"temperature\", temperature)\n",
        "        object.__setattr__(self, \"top_p\", top_p)\n",
        "        object.__setattr__(self, \"verbose\", verbose)\n",
        "        object.__setattr__(self, \"_gen_lock\", threading.Lock())\n",
        "\n",
        "    ### 공통 유틸 ###########################################################################\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"gemma-chat\"\n",
        "\n",
        "    def _messages_to_conv(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:\n",
        "        conv: List[Dict[str, str]] = []\n",
        "        for m in messages:\n",
        "            if isinstance(m, SystemMessage):\n",
        "                conv.append({\"role\": \"system\", \"content\": m.content})\n",
        "            elif isinstance(m, HumanMessage):\n",
        "                conv.append({\"role\": \"user\", \"content\": m.content})\n",
        "            elif isinstance(m, AIMessage):\n",
        "                conv.append({\"role\": \"assistant\", \"content\": m.content})\n",
        "        return conv\n",
        "\n",
        "    def _format_messages(self, messages: List[BaseMessage]) -> str:\n",
        "        if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
        "            conv = self._messages_to_conv(messages)\n",
        "            formatted = self.tokenizer.apply_chat_template(\n",
        "                conv,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True,  # 어시스턴트 턴 시작만 넣고 종료 토큰은 모델이 생성\n",
        "            )\n",
        "            return formatted\n",
        "\n",
        "        # 폴백(토크나이저가 템플릿을 제공하지 않을 때만 사용)\n",
        "        prompt = \"\"\n",
        "        for m in messages:\n",
        "            if isinstance(m, SystemMessage):\n",
        "                prompt += \"<start_of_turn>system\\n\" + m.content + \"<end_of_turn>\\n\"\n",
        "            elif isinstance(m, HumanMessage):\n",
        "                prompt += \"<start_of_turn>user\\n\" + m.content + \"<end_of_turn>\\n\"\n",
        "            elif isinstance(m, AIMessage):\n",
        "                prompt += \"<start_of_turn>assistant\\n\" + m.content + \"<end_of_turn>\\n\"\n",
        "        prompt += \"<start_of_turn>assistant\\n\"\n",
        "        return prompt\n",
        "\n",
        "    def _apply_stop(self, text: str, stop: Optional[List[str]]) -> str:\n",
        "        if not stop:\n",
        "            return text\n",
        "        cut = len(text)\n",
        "        for s in stop:\n",
        "            idx = text.find(s)\n",
        "            if idx != -1:\n",
        "                cut = min(cut, idx)\n",
        "        return text[:cut]\n",
        "\n",
        "    def _build_gen_kwargs(self, **kwargs: Any) -> Dict[str, Any]:\n",
        "        pad_id = self.tokenizer.pad_token_id\n",
        "        if pad_id is None:\n",
        "            pad_id = self.tokenizer.eos_token_id\n",
        "\n",
        "        eot_id = None\n",
        "        try:\n",
        "            eot_id = self.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
        "            if isinstance(eot_id, list):\n",
        "                eot_id = None\n",
        "        except Exception:\n",
        "            eot_id = None\n",
        "\n",
        "        return {\n",
        "            \"max_new_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
        "            \"do_sample\": kwargs.get(\"do_sample\", self.do_sample),\n",
        "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
        "            \"top_p\": kwargs.get(\"top_p\", self.top_p),\n",
        "            \"eos_token_id\": eot_id or self.tokenizer.eos_token_id,\n",
        "            \"pad_token_id\": pad_id,\n",
        "        }\n",
        "\n",
        "    ### Invoke ############################################################################\n",
        "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> ChatResult:\n",
        "        prompt = self._format_messages(messages)\n",
        "        if getattr(self, \"verbose\", False):\n",
        "            print(\"\\n[GemmaChatModel/_generate] ==== FINAL PROMPT ====\")\n",
        "            print(prompt)\n",
        "            print(\"=================================================\\n\")\n",
        "\n",
        "        # [CHANGED] 기본 stop 시퀀스에 종료 마커 추가\n",
        "        if stop is None:\n",
        "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
        "\n",
        "        with self._gen_lock:\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "            gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "        # [CHANGED] 입력 길이 이후 생성 토큰만 디코딩\n",
        "        in_len = inputs[\"input_ids\"].shape[1]\n",
        "        gen_tokens = outputs[0][in_len:]\n",
        "        decoded = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        # [CHANGED] 후단 종료 마커 정리(안전망)\n",
        "        decoded = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", decoded)\n",
        "\n",
        "        decoded = self._apply_stop(decoded, stop)\n",
        "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=decoded))])\n",
        "\n",
        "    ### Batch #############################################################################\n",
        "    def _generate_batch(self, messages_list: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> List[ChatResult]:\n",
        "        \"\"\"\n",
        "        여러 개의 대화를 한 번에 패딩 인코딩하여 generate 가속.\n",
        "        \"\"\"\n",
        "        # [CHANGED] 각 대화를 chat template로 포맷\n",
        "        prompts = [self._format_messages(msgs) for msgs in messages_list]\n",
        "\n",
        "        # padding=True, truncation=True 로 배치 인코딩\n",
        "        tokenized = self.tokenizer(\n",
        "            prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "        )\n",
        "        tokenized = {k: v.to(self.model.device) for k, v in tokenized.items()}\n",
        "        attn = tokenized.get(\"attention_mask\", None)\n",
        "\n",
        "        gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
        "\n",
        "        # [CHANGED] 기본 stop 시퀀스\n",
        "        if stop is None:\n",
        "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**tokenized, **gen_kwargs)\n",
        "\n",
        "        results: List[ChatResult] = []\n",
        "        for i in range(len(prompts)):\n",
        "            # 각 샘플의 프롬프트 길이만큼 잘라서 신규 토큰만 디코딩\n",
        "            if attn is not None:\n",
        "                in_len = int(attn[i].sum().item())\n",
        "            else:\n",
        "                in_len = tokenized[\"input_ids\"][i].shape[0]\n",
        "\n",
        "            gen_tokens = outputs[i][in_len:]\n",
        "            text = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "            text = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", text)  # [CHANGED]\n",
        "            text = self._apply_stop(text, stop)\n",
        "\n",
        "            results.append(\n",
        "                ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])\n",
        "            )\n",
        "        return results\n",
        "\n",
        "    ### Stream ############################################################################\n",
        "    def _stream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> Iterator[ChatGenerationChunk]:\n",
        "        \"\"\"\n",
        "        LangChain의 Runnable .stream() 에서 호출되는 내부 스트리밍 제너레이터.\n",
        "        각 토큰 델타를 ChatGenerationChunk(AIMessageChunk) 로 내보냅니다.\n",
        "        \"\"\"\n",
        "        prompt = self._format_messages(messages)\n",
        "        if getattr(self, \"verbose\", False):\n",
        "            print(\"\\n[GemmaChatModel/_stream] ==== FINAL PROMPT ====\")\n",
        "            print(prompt)\n",
        "            print(\"================================================\\n\")\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
        "\n",
        "        # transformers 스트리머 설정\n",
        "        streamer = TextIteratorStreamer(\n",
        "            self.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        # [CHANGED] 기본 stop 시퀀스\n",
        "        if stop is None:\n",
        "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
        "\n",
        "        # generate를 백그라운드에서 수행\n",
        "        def _worker():\n",
        "            with torch.no_grad():\n",
        "                self.model.generate(**inputs, **gen_kwargs, streamer=streamer)\n",
        "\n",
        "        th = threading.Thread(target=_worker, daemon=True)\n",
        "        th.start()\n",
        "\n",
        "        # 누적 후 stop 시퀀스까지 안전하게 잘라서 내보내기\n",
        "        buffer = \"\"\n",
        "        emitted = 0\n",
        "\n",
        "        for piece in streamer:\n",
        "            buffer += piece\n",
        "            # [CHANGED] 실시간으로 후단 종료 마커 제거(시각적 잔여물 방지)\n",
        "            tmp = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", buffer)\n",
        "            trimmed = self._apply_stop(tmp, stop)\n",
        "\n",
        "            # 새로 생긴 구간만 델타로 방출\n",
        "            if len(trimmed) > emitted:\n",
        "                delta = trimmed[emitted:]\n",
        "                emitted = len(trimmed)\n",
        "                yield ChatGenerationChunk(message=AIMessageChunk(content=delta))\n",
        "\n",
        "            # stop 시퀀스 감지되면 중단\n",
        "            if stop and len(trimmed) < len(buffer):\n",
        "                break\n",
        "\n",
        "        # 스레드 정리(최대한 조용히 종료 대기)\n",
        "        th.join(timeout=0.1)\n",
        "\n",
        "    ### Structured output #################################################################\n",
        "    def _build_json_system_prompt(self, schema_text: str) -> str:\n",
        "        # 모델이 JSON만 내도록 강하게 지시 (hallucination 방지용 규칙 포함)\n",
        "        return (\n",
        "            \"You are a strict JSON generator.\\n\"\n",
        "            \"Return ONLY a single JSON object, no prose, no backticks, no explanations.\\n\"\n",
        "            \"Do not include trailing commas. Do not include comments.\\n\"\n",
        "            \"Conform exactly to the following JSON schema (fields, types, required):\\n\"\n",
        "            f\"{schema_text}\\n\"\n",
        "        )\n",
        "\n",
        "    def _ensure_pydantic(self):\n",
        "        if PydanticBaseModel is None:\n",
        "            raise RuntimeError(\n",
        "                \"Pydantic is not available. Install pydantic or pass a dict schema instead of a BaseModel.\"\n",
        "            )\n",
        "\n",
        "    def _schema_to_text(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]]) -> str:\n",
        "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(\n",
        "            schema, PydanticBaseModel\n",
        "        ):\n",
        "            try:\n",
        "                json_schema = schema.model_json_schema()  # pydantic v2\n",
        "            except Exception:\n",
        "                json_schema = schema.schema()  # pydantic v1\n",
        "            return json.dumps(json_schema, ensure_ascii=False, indent=2)\n",
        "        elif isinstance(schema, dict):\n",
        "            return json.dumps(schema, ensure_ascii=False, indent=2)\n",
        "        else:\n",
        "            raise TypeError(\n",
        "                \"schema must be a Pydantic BaseModel subclass or a dict JSON schema.\"\n",
        "            )\n",
        "\n",
        "    def _parse_structured(self, text: str, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], include_raw: bool):\n",
        "        # 코드블록 등 제거 시도(혹시 들어올 경우)\n",
        "        t = text.strip()\n",
        "        if t.startswith(\"```\"):\n",
        "            # ```json ... ``` 또는 ``` ... ```\n",
        "            t = t.strip(\"`\")\n",
        "            # 첫 줄에 json 명시가 들어있을 수 있음\n",
        "            t = \"\\n\".join(\n",
        "                line for line in t.splitlines() if not line.lower().startswith(\"json\")\n",
        "            )\n",
        "        # JSON 파싱\n",
        "        obj = json.loads(t)\n",
        "\n",
        "        # Pydantic 검증\n",
        "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(\n",
        "            schema, PydanticBaseModel\n",
        "        ):\n",
        "            validated = (\n",
        "                schema.model_validate(obj)\n",
        "                if hasattr(schema, \"model_validate\")\n",
        "                else schema.parse_obj(obj)\n",
        "            )\n",
        "            return {\"parsed\": validated, \"raw\": text} if include_raw else validated\n",
        "        else:\n",
        "            # dict 스키마는 별도 검증 없이 반환 (원하면 jsonschema로 검증 가능)\n",
        "            return {\"parsed\": obj, \"raw\": text} if include_raw else obj\n",
        "\n",
        "    def with_structured_output(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], *, method: Literal[\"json_mode\"] = \"json_mode\", include_raw: bool = False, system_prefix: Optional[str] = None, deterministic: bool = True):\n",
        "        schema_text = self._schema_to_text(schema)\n",
        "        sys_prompt = self._build_json_system_prompt(schema_text)\n",
        "        if system_prefix:\n",
        "            sys_prompt = system_prefix.rstrip() + \"\\n\\n\" + sys_prompt\n",
        "\n",
        "        def _invoke(messages_or_any):\n",
        "            # 입력을 메시지 리스트로 정규화\n",
        "            if isinstance(messages_or_any, list) and all(\n",
        "                isinstance(m, BaseMessage) for m in messages_or_any\n",
        "            ):\n",
        "                msgs = [SystemMessage(content=sys_prompt)] + messages_or_any\n",
        "            else:\n",
        "                # 문자열/딕셔너리 등도 처리\n",
        "                msgs = [\n",
        "                    SystemMessage(content=sys_prompt),\n",
        "                    HumanMessage(content=str(messages_or_any)),\n",
        "                ]\n",
        "\n",
        "            # 결정론 옵션\n",
        "            kw = {}\n",
        "            if deterministic:\n",
        "                kw = {\"do_sample\": False, \"temperature\": 0.0, \"top_p\": 1.0}\n",
        "\n",
        "            # 1차 시도\n",
        "            result = self._generate(msgs, **kw)\n",
        "            text = result.generations[0].message.content\n",
        "            try:\n",
        "                return self._parse_structured(text, schema, include_raw)\n",
        "            except Exception:\n",
        "                # 재시도: 더 강한 지시\n",
        "                retry_msgs = [\n",
        "                    SystemMessage(\n",
        "                        content=sys_prompt + \"\\nOutput must be valid JSON. Try again.\"\n",
        "                    )\n",
        "                ] + msgs[1:]\n",
        "                result2 = self._generate(retry_msgs, **kw)\n",
        "                text2 = result2.generations[0].message.content\n",
        "                return self._parse_structured(text2, schema, include_raw)\n",
        "\n",
        "        # Runnable 로 래핑해서 반환 (체인 파이프에 바로 사용 가능)\n",
        "        return RunnableLambda(_invoke)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L37Lhf-eQgn"
      },
      "outputs": [],
      "source": [
        "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP0-plMu-azj"
      },
      "source": [
        "# 기본적인 구성 및 기능 - Runnable: invoke, batch, stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoRMTczO-azj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHu6yJVn-azj"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7wAGp5C-azj"
      },
      "outputs": [],
      "source": [
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iumeC2tu-azk"
      },
      "outputs": [],
      "source": [
        "chain = prompt | chat_model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "165BPS0Y-azk"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke({\"dish\": \"카레\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-yk8Y5I-azk"
      },
      "outputs": [],
      "source": [
        "output = chain.batch([{\"dish\": \"카레\"}, {\"dish\": \"우동\"}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs7__K0I-azk"
      },
      "outputs": [],
      "source": [
        "print(output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prUrjzVl-azk"
      },
      "outputs": [],
      "source": [
        "print(output[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qevoju2e-azk"
      },
      "outputs": [],
      "source": [
        "for chunk in chain.stream({\"dish\": \"카레\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlkYu8bG-azk"
      },
      "source": [
        "# Few Shot Like 구성해 보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeAhqd0U-azk"
      },
      "outputs": [],
      "source": [
        "gemma_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*5, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FAQzDiP-azk"
      },
      "outputs": [],
      "source": [
        "cot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"사용자의 질문에 단계적으로 답변하세요.\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cot_chain = cot_prompt | gemma_model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGME0NIk-azk"
      },
      "outputs": [],
      "source": [
        "output = cot_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azNj2Bcl-azk"
      },
      "outputs": [],
      "source": [
        "summarize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"단계적으로 생각한 답변에서 결론만 추출하세요.\"),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "summarize_chain = summarize_prompt | gemma_model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYibjDBD-azk"
      },
      "outputs": [],
      "source": [
        "cot_summarize_chain = cot_chain | summarize_chain\n",
        "output = cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l97He4VO-azk"
      },
      "source": [
        "# 함수를 Chain에 붙이기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAZPQkPA-azk"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda, chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBRKznbq-azk"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwpdX25R-azk"
      },
      "outputs": [],
      "source": [
        "def upper(text: str) -> str:\n",
        "    return text.upper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz7ji18Q-azk"
      },
      "outputs": [],
      "source": [
        "chain_lambda = prompt | chat_model | output_parser | RunnableLambda(upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcWyMjNj-azk"
      },
      "outputs": [],
      "source": [
        "ai_message = chain_lambda.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUgXXu98-azk"
      },
      "outputs": [],
      "source": [
        "@chain\n",
        "def upper_deco(text: str) -> str:\n",
        "    return text.upper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtPhPWu2-azk"
      },
      "outputs": [],
      "source": [
        "chain_deco = prompt | chat_model | output_parser | upper_deco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP-B0S2w-azk"
      },
      "outputs": [],
      "source": [
        "ai_message = chain_deco.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk6nZarV-azr"
      },
      "source": [
        "# 함수를 Stream Chain에 붙이기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0L9dTsu-azr"
      },
      "outputs": [],
      "source": [
        "def upper(input_stream: Iterator[str]) -> Iterator[str]:\n",
        "    for text in input_stream:\n",
        "        yield text.upper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07W93-dr-azr"
      },
      "outputs": [],
      "source": [
        "chain_stream = prompt | chat_model | StrOutputParser() | upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqE7tK_T-azs"
      },
      "outputs": [],
      "source": [
        "for chunk in chain_stream.stream({\"input\": \"Hello!\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgEk5snn-azs"
      },
      "source": [
        "# Parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk3749E_-azs"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOZp9sw7-azs"
      },
      "source": [
        "## optimistic chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtqTQXUt-azs"
      },
      "outputs": [],
      "source": [
        "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 낙관주의자입니다. 사용자의 입력에 대해 낙관적인 의견을 제공하세요.\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PveNuJ8-azs"
      },
      "outputs": [],
      "source": [
        "optimistic_chain = optimistic_prompt | chat_model | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW_PfIDe-azs"
      },
      "source": [
        "## pessimistic chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReT3h1FS-azs"
      },
      "outputs": [],
      "source": [
        "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 비관주의자입니다. 사용자의 입력에 대해 비관적인 의견을 제공하세요.\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnrpqOLN-azs"
      },
      "outputs": [],
      "source": [
        "pessimistic_chain = pessimistic_prompt | chat_model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkRAbDTT-azs"
      },
      "outputs": [],
      "source": [
        "parallel_chain = RunnableParallel(\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG_Ykszm-azs"
      },
      "source": [
        "## 여럿 의견 수집"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5OYmZEK-azs"
      },
      "outputs": [],
      "source": [
        "output = parallel_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vqX3Bwx-azs"
      },
      "outputs": [],
      "source": [
        "print(output[\"optimistic_opinion\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UovKq1o-azs"
      },
      "outputs": [],
      "source": [
        "print(output[\"pessimistic_opinion\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi8RulPh-azs"
      },
      "source": [
        "## 의견 종합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R9nFlPF-azs"
      },
      "outputs": [],
      "source": [
        "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 객관적 AI입니다. 두 가지 의견을 종합하세요.\"),\n",
        "        (\"human\", \"낙관적 의견: {optimistic_opinion}\\n비관적 의견: {pessimistic_opinion}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyMSf1Tz-azs"
      },
      "outputs": [],
      "source": [
        "synthesize_chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"optimistic_opinion\": optimistic_chain,\n",
        "            \"pessimistic_opinion\": pessimistic_chain,\n",
        "        }\n",
        "    )\n",
        "    | synthesize_prompt\n",
        "    | chat_model\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj7ErLm--azt"
      },
      "outputs": [],
      "source": [
        "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79dJib8I-azt"
      },
      "outputs": [],
      "source": [
        "# RunnableParallel 자동 변환\n",
        "synthesize_chain = (\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "    }\n",
        "    | synthesize_prompt\n",
        "    | chat_model\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHhd92og-azt"
      },
      "outputs": [],
      "source": [
        "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzyybXYY-azt"
      },
      "source": [
        "# Parallel 사용 구조"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec7YNgKm-azt"
      },
      "outputs": [],
      "source": [
        "# operator는 파이썬의 연산자들을 “함수”로 제공하는 표준 라이브러리 모듈\n",
        "from operator import itemgetter\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG1mF2cV-azt"
      },
      "outputs": [],
      "source": [
        "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 낙관주의자입니다. 사용자의 입력에 대해 낙관적인 의견을 제공하세요.\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3bxgRgZ-azt"
      },
      "outputs": [],
      "source": [
        "optimistic_chain = optimistic_prompt | chat_model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSeIY6XE-azt"
      },
      "outputs": [],
      "source": [
        "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 비관주의자입니다. 사용자의 입력에 대해 비관적인 의견을 제공하세요.\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYFTgX50-azt"
      },
      "outputs": [],
      "source": [
        "pessimistic_chain = pessimistic_prompt | chat_model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osZy9-n2-azt"
      },
      "outputs": [],
      "source": [
        "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            textwrap.dedent(\"\"\"\n",
        "                당신은 객관적 AI입니다. '{topic}'에 대한 두 가지 의견을 종합하세요.\n",
        "\n",
        "                # 다음과 같은 일반적인 의견도 있습니다. 참고해 주세요.\n",
        "                {general}\n",
        "            \"\"\").strip()\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"낙관적 의견: {optimistic_opinion}\\n비관적 의견: {pessimistic_opinion}\",\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OvTOyHf-azt"
      },
      "outputs": [],
      "source": [
        "# invoke 에서 넘어오는 파라미터를 바로 사용해 보기\n",
        "synthesize_chain = (\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "        \"topic\": itemgetter(\"topic\"),\n",
        "        \"general\": itemgetter(\"general\"),\n",
        "    }\n",
        "    | synthesize_prompt\n",
        "    | chat_model\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5fT6OH7-azt"
      },
      "outputs": [],
      "source": [
        "general_opinion = \"\"\"\n",
        "생성형 AI는 단순히 인간의 작업을 자동화하는 도구를 넘어, 창의성과 지식 생산의 범위를 폭발적으로 확장하는 기술입니다.\n",
        "누구나 손쉽게 글·이미지·코드 등을 만들어낼 수 있게 함으로써 교육, 예술, 연구, 산업 전반에 혁신을 촉진합니다.\n",
        "그러나 이와 동시에, 진위 판단의 어려움, 저작권과 데이터 소유권 문제, 일자리 구조 변화 등 사회 전반의 새로운 도전과 책임을 수반합니다.\n",
        "\n",
        "따라서 생성형 AI의 사회적 의미는 단순한 기술 진보가 아니라, “인간과 기계가 어떻게 협력하고, 무엇을 신뢰하며, 어떤 가치를 지켜야 하는가”라는 집단적 선택의 시험대에 있다는 점입니다.\n",
        "기술의 잠재력을 최대화하면서도 부작용을 최소화하려면, 투명성과 공정성, 그리고 지속적인 윤리 논의가 필수적입니다.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRfV4ONJ-azt"
      },
      "outputs": [],
      "source": [
        "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\", \"general\": general_opinion})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biM1jdOk-azt"
      },
      "source": [
        "# Tavily 검색"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ragFAtJM-azt"
      },
      "outputs": [],
      "source": [
        "!pip install tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAhdIt_A-azt"
      },
      "outputs": [],
      "source": [
        "from tavily import TavilyClient\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWI6M_5u-azt"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElaD3mnc-azu"
      },
      "outputs": [],
      "source": [
        "client = TavilyClient()  # TAVILY_API_KEY를 env에서 읽음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIwXAW_p-azu"
      },
      "outputs": [],
      "source": [
        "res = client.search(\"서울 오늘 날씨 관련 최신 기사 요약\", max_results=3)\n",
        "print(json.dumps(res, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_q8_X7z-azu"
      },
      "source": [
        "# Passthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoO473CX-azu"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template('''\\\n",
        "다음 문맥만을 고려해 질문에 답하세요.\n",
        "\n",
        "문맥: \"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "질문: {question}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0eznVeY-azu"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tfYV2-4-azu"
      },
      "outputs": [],
      "source": [
        "retriever = TavilySearchAPIRetriever(k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA6g5qO3-azu"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0Xxqm6e-azu"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke(\"서울의 현재 날씨는? 비는 오나요?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbhUksKX-azu"
      },
      "source": [
        "## assign, pick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRApZ3-i-azu"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"question\": RunnablePassthrough(),\n",
        "            \"context\": retriever,\n",
        "        }\n",
        "    )\n",
        "    .assign(answer=prompt | chat_model | StrOutputParser())\n",
        "    .pick([\"context\", \"answer\", \"question\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdLNBZ9r-azu"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPeal2ED-azu"
      },
      "outputs": [],
      "source": [
        "print(output[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9V3KsdO-azu"
      },
      "source": [
        "# Database Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7JnomhA-azu"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "from uuid import uuid4\n",
        "import sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Glz0MlWf-azu"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 과거대화 내용을 바탕으로 상담을 해 주는 친절한 상담사 입니다.\"),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soqps_8a-azu"
      },
      "outputs": [],
      "source": [
        "chain = prompt | chat_model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAHheUR_-azu"
      },
      "outputs": [],
      "source": [
        "def respond(session_id: str, human_message: str) -> str:\n",
        "    chat_message_history = SQLChatMessageHistory(\n",
        "        connection=\"sqlite:///sqlite.db\",\n",
        "        table_name=\"chat_store\", # default: message_store\n",
        "        session_id=session_id,\n",
        "    )\n",
        "\n",
        "    ai_message = chain.invoke(\n",
        "        {\n",
        "            \"chat_history\": chat_message_history.get_messages(),\n",
        "            \"input\": human_message,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    chat_message_history.add_user_message(human_message)\n",
        "    chat_message_history.add_ai_message(ai_message)\n",
        "\n",
        "    return ai_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Dqj-Ey-azu"
      },
      "outputs": [],
      "source": [
        "session_id = uuid4().hex\n",
        "session_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjzFdORj-azv"
      },
      "outputs": [],
      "source": [
        "output1 = respond(\n",
        "    session_id=session_id,\n",
        "    human_message=\"안녕하세요! 제 이름은 존이라고 합니다!\",\n",
        ")\n",
        "print(output1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o77m8NsZ-azv"
      },
      "outputs": [],
      "source": [
        "output2 = respond(\n",
        "    session_id=session_id,\n",
        "    human_message=\"제 이름을 알고 계신가요?\",\n",
        ")\n",
        "print(output2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBlSWk10-azv"
      },
      "outputs": [],
      "source": [
        "chat_message_history = SQLChatMessageHistory(\n",
        "    connection=\"sqlite:///sqlite.db\",\n",
        "    table_name=\"chat_store\", # default: message_store\n",
        "    session_id=session_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90PJHCr_-azv"
      },
      "outputs": [],
      "source": [
        "msgs = chat_message_history.get_messages()\n",
        "for m in msgs:\n",
        "    print(type(m), m.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVQbQC2X-azv"
      },
      "outputs": [],
      "source": [
        "def get_all_session_ids(db_path=\"sqlite.db\") -> List[str]:\n",
        "    con = sqlite3.connect(db_path)\n",
        "    rows = con.execute(\"SELECT DISTINCT session_id FROM chat_store\").fetchall()\n",
        "    con.close()\n",
        "    return [r[0] for r in rows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUC6Yak7-azv"
      },
      "outputs": [],
      "source": [
        "for sid in get_all_session_ids():\n",
        "    history = SQLChatMessageHistory(session_id=sid, connection=\"sqlite:///sqlite.db\", table_name=\"chat_store\")\n",
        "    print(f\"[+] session: {sid}\")\n",
        "    msgs = history.get_messages()\n",
        "    for m in msgs:\n",
        "        print(type(m), m.content)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im8HEbZP-azv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF7aQvtN-azv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvlg-Wbv-azv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yLT1RFEeQgm"
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esrEO7yteQgm"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEBCNVjffATu"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oQenpTCPMyh"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU6jp1fqeQgn"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btydbHNpeQgn"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 1024*5, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    # device_map = {\"\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPtcr98MeQgn"
   },
   "outputs": [],
   "source": [
    "model = FastModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Ej1_yjeQgn"
   },
   "source": [
    "# Custom ChatModel 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf4trrKVeQgn"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, List, Union, Type, Literal\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from typing import Any, Dict, Optional, List, Union, Type, Literal\n",
    "import json, warnings\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel as PydanticBaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjKeUw07eQgn"
   },
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[BaseMessage]) -> str:\n",
    "        prompt = \"\"\n",
    "        for m in messages:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{m.content}</s>\\n\"\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{m.content}</s>\\n\"\n",
    "            elif isinstance(m, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{m.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _apply_stop(self, text: str, stop: Optional[List[str]]) -> str:\n",
    "        if not stop:\n",
    "            return text\n",
    "        cut = len(text)\n",
    "        for s in stop:\n",
    "            idx = text.find(s)\n",
    "            if idx != -1:\n",
    "                cut = min(cut, idx)\n",
    "        return text[:cut]\n",
    "\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"do_sample\": kwargs.get(\"do_sample\", self.do_sample),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"top_p\": kwargs.get(\"top_p\", self.top_p),\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # 마지막 assistant 턴 이후만 추출\n",
    "        if \"<|assistant|>\\n\" in decoded:\n",
    "            response = decoded.split(\"<|assistant|>\\n\")[-1]\n",
    "        else:\n",
    "            response = decoded\n",
    "        response = response.strip()\n",
    "        response = self._apply_stop(response, stop)\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])\n",
    "\n",
    "    ### Structured output support #######################################################################################\n",
    "    def _build_json_system_prompt(self, schema_text: str) -> str:\n",
    "        # 모델이 JSON만 내도록 강하게 지시 (hallucination 방지용 규칙 포함)\n",
    "        return (\n",
    "            \"You are a strict JSON generator.\\n\"\n",
    "            \"Return ONLY a single JSON object, no prose, no backticks, no explanations.\\n\"\n",
    "            \"Do not include trailing commas. Do not include comments.\\n\"\n",
    "            \"Conform exactly to the following JSON schema (fields, types, required):\\n\"\n",
    "            f\"{schema_text}\\n\"\n",
    "        )\n",
    "\n",
    "    def _ensure_pydantic(self):\n",
    "        if PydanticBaseModel is None:\n",
    "            raise RuntimeError(\n",
    "                \"Pydantic is not available. Install pydantic or pass a dict schema instead of a BaseModel.\"\n",
    "            )\n",
    "\n",
    "    def _schema_to_text(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]]) -> str:\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(schema, PydanticBaseModel):\n",
    "            # pydantic 스키마를 JSON 스키마로 직렬화\n",
    "            try:\n",
    "                # v1/v2 호환 직렬화\n",
    "                json_schema = schema.model_json_schema()  # pydantic v2\n",
    "            except Exception:\n",
    "                json_schema = schema.schema()  # pydantic v1\n",
    "            return json.dumps(json_schema, ensure_ascii=False, indent=2)\n",
    "        elif isinstance(schema, dict):\n",
    "            return json.dumps(schema, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            raise TypeError(\"schema must be a Pydantic BaseModel subclass or a dict JSON schema.\")\n",
    "\n",
    "    def _parse_structured(self, text: str, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], include_raw: bool):\n",
    "        # 코드블록 등 제거 시도(혹시 들어올 경우)\n",
    "        t = text.strip()\n",
    "        if t.startswith(\"```\"):\n",
    "            # ```json ... ``` 또는 ``` ... ```\n",
    "            t = t.strip(\"`\")\n",
    "            # 첫 줄에 json 명시가 들어있을 수 있음\n",
    "            t = \"\\n\".join(line for line in t.splitlines() if not line.lower().startswith(\"json\"))\n",
    "        # JSON 파싱\n",
    "        obj = json.loads(t)\n",
    "\n",
    "        # Pydantic 검증\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(schema, PydanticBaseModel):\n",
    "            validated = schema.model_validate(obj) if hasattr(schema, \"model_validate\") else schema.parse_obj(obj)\n",
    "            return {\"parsed\": validated, \"raw\": text} if include_raw else validated\n",
    "        else:\n",
    "            # dict 스키마는 별도 검증 없이 반환 (원하면 jsonschema로 검증 가능)\n",
    "            return {\"parsed\": obj, \"raw\": text} if include_raw else obj\n",
    "\n",
    "    def with_structured_output(\n",
    "        self,\n",
    "        schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]],\n",
    "        *,\n",
    "        method: Literal[\"json_mode\"] = \"json_mode\",\n",
    "        include_raw: bool = False,\n",
    "        system_prefix: Optional[str] = None,\n",
    "        deterministic: bool = True,\n",
    "    ):\n",
    "        schema_text = self._schema_to_text(schema)\n",
    "        sys_prompt = self._build_json_system_prompt(schema_text)\n",
    "        if system_prefix:\n",
    "            sys_prompt = system_prefix.rstrip() + \"\\n\\n\" + sys_prompt\n",
    "\n",
    "        def _invoke(messages_or_any):\n",
    "            # 입력을 메시지 리스트로 정규화\n",
    "            if isinstance(messages_or_any, list) and all(isinstance(m, BaseMessage) for m in messages_or_any):\n",
    "                msgs = [SystemMessage(content=sys_prompt)] + messages_or_any\n",
    "            else:\n",
    "                # 문자열/딕셔너리 등도 처리\n",
    "                msgs = [SystemMessage(content=sys_prompt), HumanMessage(content=str(messages_or_any))]\n",
    "\n",
    "            # 결정론 옵션\n",
    "            kw = {}\n",
    "            if deterministic:\n",
    "                kw = {\"do_sample\": False, \"temperature\": 0.0, \"top_p\": 1.0}\n",
    "\n",
    "            # 1차 시도\n",
    "            result = self._generate(msgs, **kw)\n",
    "            text = result.generations[0].message.content\n",
    "            try:\n",
    "                return self._parse_structured(text, schema, include_raw)\n",
    "            except Exception:\n",
    "                # 재시도: 더 강한 지시\n",
    "                retry_msgs = [SystemMessage(content=sys_prompt + \"\\nOutput must be valid JSON. Try again.\")] + msgs[1:]\n",
    "                result2 = self._generate(retry_msgs, **kw)\n",
    "                text2 = result2.generations[0].message.content\n",
    "                return self._parse_structured(text2, schema, include_raw)\n",
    "\n",
    "        # Runnable 로 래핑해서 반환 (체인 파이프에 바로 사용 가능)\n",
    "        return RunnableLambda(_invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4L37Lhf-eQgn"
   },
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GitLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = loader.load()\n",
    "print(len(raw_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharacterTextSplitter\n",
    "- 사용자가 separator 인자로 지정한 단일 문자(예: \"\\n\")을 기준으로 분할합니다.\n",
    "- 사용자가 분할 기준으로 직접 정할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, # 목표치\n",
    "    chunk_overlap=0,\n",
    "    separator=\"\\n\\n\", #사용자가 직접 지정 가능\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_docs = text_splitter.split_documents(raw_docs)\n",
    "print(len(chunk_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"청크 개수: {len(chunk_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(chunk_docs[:2]):\n",
    "    print(f\"\\n--- 조각 {i+1} ({len(doc.page_content)}) ---\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecursiveCharacterTextSplitter\n",
    "- [\"\\n\\n\", \"\\n\", \" \", \"\"] 순서로, 큰 단위(문단)부터 작은 단위(단어)로 재귀적으로 분할합니다.\n",
    "- \"\" : 문자(character)단위로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],  # 여러 구분자를 순차적으로 적용\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_docs = text_splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"청크 개수: {len(chunk_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(chunk_docs[:2]):\n",
    "    print(f\"\\n--- 조각 {i+1} ({len(doc.page_content)}) ---\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TokenTextSplitter\n",
    "- 언어 모델의 토큰 단위로 chunk_size 길이에 맞추어 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai 토그나이저로 자르기\n",
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_docs = text_splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"청크 개수: {len(chunk_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(chunk_docs[:2]):\n",
    "    print(f\"\\n--- 조각 {i+1} ({len(doc.page_content)}) ---\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Tokenizer로 자르기\n",
    "text_splitter = TokenTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer.tokenizer,  # Gemma3Processor 객체 안에서 토크나이저를 찾아 넣어야 함.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Tokenizer로 자르기 : 문맥을 좀더 효율적으로 찾아서 분리.\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer.tokenizer,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,             # 필요시 100~200 정도로 조정\n",
    "    add_start_index=False        # 청크 시작 인덱스가 필요하면 True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTKTextSplitter\n",
    "\n",
    "내부적으로 nltk.sent_tokenize() → Punkt 문장 분리기 사용  \n",
    "즉, 문장 경계 후보(., ?, !, …) + Punkt 규칙(약어/숫자/대문자 등)을 기반으로 문장을 나눔  \n",
    "\n",
    "* 기호 후보: ., ?, !, …는 잠정적 문장 끝 후보\n",
    "* 대문자 시작: 구분 기호 뒤 단어가 대문자로 시작 → 문장 끝일 가능성 ↑\n",
    "* 약어 인식: Dr., Mr., e.g. 등은 문장 끝이 아님\n",
    "* 이니셜/다중 점: U.S.A., J. R. R. 같은 패턴은 내부로 처리\n",
    "* 숫자/소수점: 3.14, No. 5 같은 경우는 문장 끝 아님\n",
    "* 엘립시스(...): 상황에 따라 문장 끝일 수도 있고 아닐 수도 있음\n",
    "* 따옴표/괄호: .\", !) 같은 닫는 부호는 함께 문장 끝으로 취급\n",
    "* 특수 패턴: URL, 이메일, 파일명은 경계로 오인하지 않음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, os\n",
    "\n",
    "download_dir = os.path.join(os.getcwd(), 'nltk_data')\n",
    "nltk.download('punkt', download_dir=download_dir)\n",
    "nltk.download('punkt_tab', download_dir=download_dir)\n",
    "nltk.data.path.append(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_docs = text_splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"청크 개수: {len(chunk_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(chunk_docs[:2]):\n",
    "    print(f\"\\n--- 조각 {i+1} ({len(doc.page_content)}) ---\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MarkdownTextSplitter\n",
    "- 마크다운 텍스트를 구조적 요소(헤더(#), 리스트(-, *), 코드 블록(```))로, chunk_size 길이에 맞추어 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown-Based Splitting\n",
    "크기를 우선으로 텍스트를 분할 -> 마크다운 문법을 존중하며 지정된 chunk_size를 넘지 않도록 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = MarkdownTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_docs = text_splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"청크 개수: {len(chunk_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(chunk_docs[:2]):\n",
    "    print(f\"\\n--- 조각 {i+1} ({len(doc.page_content)}) ---\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Header-Based Splitting\n",
    "헤더( #, ##)에 따른 구조적 조각 생성 -> headers_to_split_on에 지정된 헤더 태그가 나타날 때마다 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_docs = text_splitter.split_text(raw_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"청크 개수: {len(chunk_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(chunk_docs[:2]):\n",
    "    print(f\"\\n--- 조각 {i+1} ({len(doc.page_content)}) --- {doc.metadata}\\n\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTMLHeaderTextSplitter\n",
    "- HTML 문서를 \\<h1>, \\<h2> 등 Header 태그 기준으로 분할합니다. chunk_size, chunk_overlap 파라미터를 받지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./res/ICSA.html', 'r', encoding='utf-8') as f:\n",
    "    html_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Title\"),\n",
    "    (\"h2\", \"Section\"),\n",
    "    (\"h3\", \"Subsection\"),\n",
    "    (\"h4\", \"Details\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = HTMLHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_docs = text_splitter.split_text(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"청크 개수: {len(chunk_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(chunk_docs[:2]):\n",
    "    print(f\"\\n--- 조각 {i+1} ({len(doc.page_content)}) ---\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LanguageSpecificTextSplitter\n",
    "- class, def (Python의 경우) 등 선택한 프로그래밍 언어의 주요 구문(클래스, 함수 정의 등)을 기준으로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res/sample_python.py', 'r', encoding='utf-8') as f:\n",
    "    python_code = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, \n",
    "    chunk_size=400, \n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_docs = text_splitter.split_text(python_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"청크 개수: {len(chunk_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(chunk_docs[:10]):\n",
    "    print(f\"\\n--- 조각 {i+1} ({len(doc)}) ---\\n{doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Splitter 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = MarkdownTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_EMBED = \"intfloat/multilingual-e5-base\" # sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
    "embedding = HuggingFaceEmbeddings(model_name=MODEL_EMBED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"AWS의 S3에서 데이터를 읽어 들이기 위한 Document loader가 있나요?\"\n",
    "\n",
    "vector = embedding.embed_query(query)\n",
    "print(len(vector))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(docs, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"AWS의 S3에서 데이터를 읽어 들이기 위한 Document loader가 있나요?\"\n",
    "\n",
    "context_docs = retriever.invoke(query)\n",
    "print(f\"len = {len(context_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc = context_docs[0]\n",
    "print(f\"metadata = {first_doc.metadata}\")\n",
    "print(first_doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL을 사용한 RAG 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template('''\n",
    "다음 문맥만을 바탕으로 질문에 답변해 주세요.\n",
    "\n",
    "문맥: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "질문: {question}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

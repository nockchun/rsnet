{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yLT1RFEeQgm"
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esrEO7yteQgm"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEBCNVjffATu"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:25:36.782357Z",
     "iopub.status.busy": "2025-08-27T01:25:36.782165Z",
     "iopub.status.idle": "2025-08-27T01:25:36.784219Z",
     "shell.execute_reply": "2025-08-27T01:25:36.783940Z",
     "shell.execute_reply.started": "2025-08-27T01:25:36.782338Z"
    },
    "id": "1oQenpTCPMyh"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:25:36.964679Z",
     "iopub.status.busy": "2025-08-27T01:25:36.964491Z",
     "iopub.status.idle": "2025-08-27T01:25:36.966586Z",
     "shell.execute_reply": "2025-08-27T01:25:36.966291Z",
     "shell.execute_reply.started": "2025-08-27T01:25:36.964667Z"
    },
    "id": "CU6jp1fqeQgn"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:25:37.107311Z",
     "iopub.status.busy": "2025-08-27T01:25:37.107120Z",
     "iopub.status.idle": "2025-08-27T01:25:49.949106Z",
     "shell.execute_reply": "2025-08-27T01:25:49.948713Z",
     "shell.execute_reply.started": "2025-08-27T01:25:37.107299Z"
    },
    "id": "btydbHNpeQgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.9: Fast Gemma3 patching. Transformers: 4.55.2. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 2. Max memory: 23.494 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 1024*5, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    # device_map = {\"\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:25:49.949652Z",
     "iopub.status.busy": "2025-08-27T01:25:49.949545Z",
     "iopub.status.idle": "2025-08-27T01:25:49.954753Z",
     "shell.execute_reply": "2025-08-27T01:25:49.954511Z",
     "shell.execute_reply.started": "2025-08-27T01:25:49.949642Z"
    },
    "id": "sPtcr98MeQgn"
   },
   "outputs": [],
   "source": [
    "model = FastModel.for_inference(model)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Ej1_yjeQgn"
   },
   "source": [
    "# Custom ChatModel 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:25:49.955084Z",
     "iopub.status.busy": "2025-08-27T01:25:49.954999Z",
     "iopub.status.idle": "2025-08-27T01:25:49.983168Z",
     "shell.execute_reply": "2025-08-27T01:25:49.982832Z",
     "shell.execute_reply.started": "2025-08-27T01:25:49.955076Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, Iterator, List, Literal, Optional, Type, Union\n",
    "from pydantic import BaseModel as PydanticBaseModel\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk, HumanMessage, SystemMessage, BaseMessage, ToolMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult, ChatGenerationChunk\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.runnables import Runnable, RunnableLambda\n",
    "from transformers import TextIteratorStreamer\n",
    "import threading\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:25:49.983910Z",
     "iopub.status.busy": "2025-08-27T01:25:49.983788Z",
     "iopub.status.idle": "2025-08-27T01:25:50.017287Z",
     "shell.execute_reply": "2025-08-27T01:25:50.016882Z",
     "shell.execute_reply.started": "2025-08-27T01:25:49.983900Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9, verbose: bool = False, **kwargs: Any):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "        object.__setattr__(self, \"verbose\", verbose)\n",
    "        object.__setattr__(self, \"_gen_lock\", threading.Lock())\n",
    "\n",
    "    ### 공통 유틸 ###########################################################################\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[BaseMessage]) -> str:\n",
    "        conv: List[Dict[str, str]] = []\n",
    "    \n",
    "        # 1) system 메시지는 처음에만 모아 붙입니다.\n",
    "        sys_buf = []\n",
    "        for m in messages:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                sys_buf.append(m.content)\n",
    "        if sys_buf:\n",
    "            conv.append({\"role\": \"system\", \"content\": \"\\n\".join(sys_buf)})\n",
    "    \n",
    "        # 2) 나머지 메시지 변환\n",
    "        for m in messages:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                continue\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                conv.append({\"role\": \"user\", \"content\": m.content})\n",
    "            elif isinstance(m, AIMessage):\n",
    "                # ⛳️ 중요: 'model' 이 아니라 'assistant'\n",
    "                conv.append({\"role\": \"assistant\", \"content\": m.content or \"\"})\n",
    "            elif isinstance(m, ToolMessage):\n",
    "                # 템플릿이 'tool' 역할을 모를 수 있으므로 user로 안전 변환\n",
    "                name = getattr(m, \"name\", \"tool\")\n",
    "                conv.append({\"role\": \"user\", \"content\": f\"[{name} 결과]\\n{m.content}\"})\n",
    "            else:\n",
    "                conv.append({\"role\": \"user\", \"content\": str(getattr(m, 'content', m))})\n",
    "    \n",
    "        # 3) 역할 번갈음 보장: 동일 역할 연속이면 합치기\n",
    "        fixed = []\n",
    "        for m in conv:\n",
    "            if fixed and fixed[-1][\"role\"] == m[\"role\"]:\n",
    "                fixed[-1][\"content\"] += \"\\n\" + m[\"content\"]\n",
    "            else:\n",
    "                fixed.append(m)\n",
    "    \n",
    "        # add_generation_prompt 는 마지막이 assistant가 아닐 때만 True\n",
    "        add_gen = not fixed or fixed[-1][\"role\"] != \"assistant\"\n",
    "    \n",
    "        # 4) 템플릿 적용, 실패하면 폴백\n",
    "        try:\n",
    "            return self.tokenizer.apply_chat_template(\n",
    "                fixed,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=add_gen,\n",
    "            )\n",
    "        except Exception:\n",
    "            # 아주 단순한 포맷으로 폴백\n",
    "            s = []\n",
    "            for m in fixed:\n",
    "                s.append(f\"{m['role'].upper()}: {m['content']}\")\n",
    "            if add_gen:\n",
    "                s.append(\"ASSISTANT: \")\n",
    "            return \"\\n\".join(s)\n",
    "\n",
    "    def _apply_stop(self, text: str, stop: Optional[List[str]]) -> str:\n",
    "        if not stop:\n",
    "            return text\n",
    "        cut = len(text)\n",
    "        for s in stop:\n",
    "            idx = text.find(s)\n",
    "            if idx != -1:\n",
    "                cut = min(cut, idx)\n",
    "        return text[:cut]\n",
    "\n",
    "    def _build_gen_kwargs(self, **kwargs: Any) -> Dict[str, Any]:\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        if pad_id is None:\n",
    "            pad_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        eot_id = None\n",
    "        try:\n",
    "            eot_id = self.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "            if isinstance(eot_id, list):\n",
    "                eot_id = None\n",
    "        except Exception:\n",
    "            eot_id = None\n",
    "\n",
    "        return {\n",
    "            \"max_new_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"do_sample\": kwargs.get(\"do_sample\", self.do_sample),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"top_p\": kwargs.get(\"top_p\", self.top_p),\n",
    "            \"eos_token_id\": eot_id or self.tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": pad_id,\n",
    "        }\n",
    "\n",
    "    ### Invoke ############################################################################\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        if getattr(self, \"verbose\", False):\n",
    "            print(\"\\n[GemmaChatModel/_generate] ==== FINAL PROMPT ====\\n\" + prompt + \"\\n=================================================\\n\")\n",
    "        # 기본 stop 시퀀스 (ReAct 루프에서 유용)\n",
    "        if stop is None:\n",
    "            stop = [\"\\nObservation:\", \"\\nFinal Answer:\"]\n",
    "        with self._gen_lock:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "        in_len = inputs[\"input_ids\"].shape[1]\n",
    "        gen_tokens = outputs[0][in_len:]\n",
    "        decoded = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "        # 안전망: 종료 마커 제거\n",
    "        decoded = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", decoded)\n",
    "        decoded = self._apply_stop(decoded, stop)\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=decoded))])\n",
    "\n",
    "    ### Batch #############################################################################\n",
    "    def _generate_batch(self, messages_list: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> List[ChatResult]:\n",
    "        \"\"\"\n",
    "        여러 개의 대화를 한 번에 패딩 인코딩하여 generate 가속.\n",
    "        \"\"\"\n",
    "        # [CHANGED] 각 대화를 chat template로 포맷\n",
    "        prompts = [self._format_messages(msgs) for msgs in messages_list]\n",
    "\n",
    "        # padding=True, truncation=True 로 배치 인코딩\n",
    "        tokenized = self.tokenizer(\n",
    "            prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        tokenized = {k: v.to(self.model.device) for k, v in tokenized.items()}\n",
    "        attn = tokenized.get(\"attention_mask\", None)\n",
    "\n",
    "        gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
    "\n",
    "        # [CHANGED] 기본 stop 시퀀스\n",
    "        if stop is None:\n",
    "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**tokenized, **gen_kwargs)\n",
    "\n",
    "        results: List[ChatResult] = []\n",
    "        for i in range(len(prompts)):\n",
    "            # 각 샘플의 프롬프트 길이만큼 잘라서 신규 토큰만 디코딩\n",
    "            if attn is not None:\n",
    "                in_len = int(attn[i].sum().item())\n",
    "            else:\n",
    "                in_len = tokenized[\"input_ids\"][i].shape[0]\n",
    "\n",
    "            gen_tokens = outputs[i][in_len:]\n",
    "            text = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "            text = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", text)  # [CHANGED]\n",
    "            text = self._apply_stop(text, stop)\n",
    "\n",
    "            results.append(\n",
    "                ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    ### Stream ############################################################################\n",
    "    def _stream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"\n",
    "        LangChain의 Runnable .stream() 에서 호출되는 내부 스트리밍 제너레이터.\n",
    "        각 토큰 델타를 ChatGenerationChunk(AIMessageChunk) 로 내보냅니다.\n",
    "        \"\"\"\n",
    "        prompt = self._format_messages(messages)\n",
    "        if getattr(self, \"verbose\", False):\n",
    "            print(\"\\n[GemmaChatModel/_stream] ==== FINAL PROMPT ====\")\n",
    "            print(prompt)\n",
    "            print(\"================================================\\n\")\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
    "\n",
    "        # transformers 스트리머 설정\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # [CHANGED] 기본 stop 시퀀스\n",
    "        if stop is None:\n",
    "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
    "\n",
    "        # generate를 백그라운드에서 수행\n",
    "        def _worker():\n",
    "            with torch.no_grad():\n",
    "                self.model.generate(**inputs, **gen_kwargs, streamer=streamer)\n",
    "\n",
    "        th = threading.Thread(target=_worker, daemon=True)\n",
    "        th.start()\n",
    "\n",
    "        # 누적 후 stop 시퀀스까지 안전하게 잘라서 내보내기\n",
    "        buffer = \"\"\n",
    "        emitted = 0\n",
    "\n",
    "        for piece in streamer:\n",
    "            buffer += piece\n",
    "            # [CHANGED] 실시간으로 후단 종료 마커 제거(시각적 잔여물 방지)\n",
    "            tmp = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", buffer)\n",
    "            trimmed = self._apply_stop(tmp, stop)\n",
    "\n",
    "            # 새로 생긴 구간만 델타로 방출\n",
    "            if len(trimmed) > emitted:\n",
    "                delta = trimmed[emitted:]\n",
    "                emitted = len(trimmed)\n",
    "                yield ChatGenerationChunk(message=AIMessageChunk(content=delta))\n",
    "\n",
    "            # stop 시퀀스 감지되면 중단\n",
    "            if stop and len(trimmed) < len(buffer):\n",
    "                break\n",
    "\n",
    "        # 스레드 정리(최대한 조용히 종료 대기)\n",
    "        th.join(timeout=0.1)\n",
    "\n",
    "    ### Structured output #################################################################\n",
    "    def _build_json_system_prompt(self, schema_text: str) -> str:\n",
    "        # 모델이 JSON만 내도록 강하게 지시 (hallucination 방지용 규칙 포함)\n",
    "        return (\n",
    "            \"You are a strict JSON generator.\\n\"\n",
    "            \"Return ONLY a single JSON object, no prose, no backticks, no explanations.\\n\"\n",
    "            \"Do not include trailing commas. Do not include comments.\\n\"\n",
    "            \"Conform exactly to the following JSON schema (fields, types, required):\\n\"\n",
    "            f\"{schema_text}\\n\"\n",
    "        )\n",
    "\n",
    "    def _ensure_pydantic(self):\n",
    "        if PydanticBaseModel is None:\n",
    "            raise RuntimeError(\n",
    "                \"Pydantic is not available. Install pydantic or pass a dict schema instead of a BaseModel.\"\n",
    "            )\n",
    "\n",
    "    def _schema_to_text(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]]) -> str:\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(\n",
    "            schema, PydanticBaseModel\n",
    "        ):\n",
    "            try:\n",
    "                json_schema = schema.model_json_schema()  # pydantic v2\n",
    "            except Exception:\n",
    "                json_schema = schema.schema()  # pydantic v1\n",
    "            return json.dumps(json_schema, ensure_ascii=False, indent=2)\n",
    "        elif isinstance(schema, dict):\n",
    "            return json.dumps(schema, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"schema must be a Pydantic BaseModel subclass or a dict JSON schema.\"\n",
    "            )\n",
    "\n",
    "    def _parse_structured(self, text: str, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], include_raw: bool):\n",
    "        # 코드블록 등 제거 시도(혹시 들어올 경우)\n",
    "        t = text.strip()\n",
    "        if t.startswith(\"```\"):\n",
    "            # ```json ... ``` 또는 ``` ... ```\n",
    "            t = t.strip(\"`\")\n",
    "            # 첫 줄에 json 명시가 들어있을 수 있음\n",
    "            t = \"\\n\".join(\n",
    "                line for line in t.splitlines() if not line.lower().startswith(\"json\")\n",
    "            )\n",
    "        # JSON 파싱\n",
    "        obj = json.loads(t)\n",
    "\n",
    "        # Pydantic 검증\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(\n",
    "            schema, PydanticBaseModel\n",
    "        ):\n",
    "            validated = (\n",
    "                schema.model_validate(obj)\n",
    "                if hasattr(schema, \"model_validate\")\n",
    "                else schema.parse_obj(obj)\n",
    "            )\n",
    "            return {\"parsed\": validated, \"raw\": text} if include_raw else validated\n",
    "        else:\n",
    "            # dict 스키마는 별도 검증 없이 반환 (원하면 jsonschema로 검증 가능)\n",
    "            return {\"parsed\": obj, \"raw\": text} if include_raw else obj\n",
    "\n",
    "    def with_structured_output(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], *, method: Literal[\"json_mode\"] = \"json_mode\", include_raw: bool = False, system_prefix: Optional[str] = None, deterministic: bool = True):\n",
    "        schema_text = self._schema_to_text(schema)\n",
    "        sys_prompt = self._build_json_system_prompt(schema_text)\n",
    "        if system_prefix:\n",
    "            sys_prompt = system_prefix.rstrip() + \"\\n\\n\" + sys_prompt\n",
    "\n",
    "        def _invoke(messages_or_any):\n",
    "            # 입력을 메시지 리스트로 정규화\n",
    "            if isinstance(messages_or_any, list) and all(\n",
    "                isinstance(m, BaseMessage) for m in messages_or_any\n",
    "            ):\n",
    "                msgs = [SystemMessage(content=sys_prompt)] + messages_or_any\n",
    "            else:\n",
    "                # 문자열/딕셔너리 등도 처리\n",
    "                msgs = [\n",
    "                    SystemMessage(content=sys_prompt),\n",
    "                    HumanMessage(content=str(messages_or_any)),\n",
    "                ]\n",
    "\n",
    "            # 결정론 옵션\n",
    "            kw = {}\n",
    "            if deterministic:\n",
    "                kw = {\"do_sample\": False, \"temperature\": 0.0, \"top_p\": 1.0}\n",
    "\n",
    "            # 1차 시도\n",
    "            result = self._generate(msgs, **kw)\n",
    "            text = result.generations[0].message.content\n",
    "            try:\n",
    "                return self._parse_structured(text, schema, include_raw)\n",
    "            except Exception:\n",
    "                # 재시도: 더 강한 지시\n",
    "                retry_msgs = [\n",
    "                    SystemMessage(\n",
    "                        content=sys_prompt + \"\\nOutput must be valid JSON. Try again.\"\n",
    "                    )\n",
    "                ] + msgs[1:]\n",
    "                result2 = self._generate(retry_msgs, **kw)\n",
    "                text2 = result2.generations[0].message.content\n",
    "                return self._parse_structured(text2, schema, include_raw)\n",
    "\n",
    "        # Runnable 로 래핑해서 반환 (체인 파이프에 바로 사용 가능)\n",
    "        return RunnableLambda(_invoke)\n",
    "\n",
    "    ### Bind Tools ########################################################################\n",
    "    def _tools_to_specs(self, tools: List[BaseTool]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"LangChain Tool -> 간단한 사양(dict)로 변환\"\"\"\n",
    "        specs = []\n",
    "        for t in tools:\n",
    "            # 설명\n",
    "            desc = getattr(t, \"description\", None) or getattr(t, \"description_md\", \"\") or \"\"\n",
    "            # 인자 스키마 (가능하면 pydantic schema, 아니면 빈 object)\n",
    "            schema: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"additionalProperties\": True}\n",
    "            args_schema = getattr(t, \"args_schema\", None)\n",
    "            if args_schema is not None:\n",
    "                try:\n",
    "                    if hasattr(args_schema, \"model_json_schema\"):\n",
    "                        schema = args_schema.model_json_schema()  # pydantic v2\n",
    "                    elif hasattr(args_schema, \"schema\"):\n",
    "                        schema = args_schema.schema()             # pydantic v1\n",
    "                except Exception:\n",
    "                    pass\n",
    "            specs.append({\n",
    "                \"name\": t.name,\n",
    "                \"description\": desc.strip(),\n",
    "                \"parameters\": schema\n",
    "            })\n",
    "        return specs\n",
    "\n",
    "    def _build_tool_router_prompt(self, tool_specs: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"도구 선택/호출을 위한 시스템 프롬프트\"\"\"\n",
    "        tools_json = json.dumps(tool_specs, ensure_ascii=False)\n",
    "        return (\n",
    "            \"You are a tool-using assistant. You may either CALL ONE tool from the list below, \"\n",
    "            \"or answer directly. Return ONLY ONE JSON object and NOTHING else.\\n\\n\"\n",
    "            f\"TOOLS (name, description, JSON parameters schema):\\n{tools_json}\\n\\n\"\n",
    "            \"OUTPUT FORMAT (choose exactly one):\\n\"\n",
    "            \"1) Tool call:\\n\"\n",
    "            \"{ \\\"tool_name\\\": \\\"<one of tool names>\\\", \\\"tool_args\\\": { ... } }\\n\"\n",
    "            \"2) Final answer (no tool needed):\\n\"\n",
    "            \"{ \\\"final\\\": \\\"<your answer>\\\" }\\n\"\n",
    "            \"Notes:\\n\"\n",
    "            \"- Do not include prose, code fences, or explanations.\\n\"\n",
    "            \"- Ensure tool_args strictly follow the tool's JSON schema when possible.\\n\"\n",
    "        )\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: List[BaseTool],\n",
    "        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> Runnable:\n",
    "        \"\"\"\n",
    "        LangChain 표준: 모델에 툴을 바인딩. 반환 Runnable은 BaseMessage를 돌려야 함.\n",
    "        - native tool-calling이 없는 로컬 LLM을 프롬프트+파서로 래핑\n",
    "        \"\"\"\n",
    "        tool_specs = self._tools_to_specs(tools)\n",
    "        sys_prompt = self._build_tool_router_prompt(tool_specs)\n",
    "        name_set = {t[\"name\"] for t in tool_specs}\n",
    "\n",
    "        # 고정 tool_choice 지원(선택사항): {\"type\":\"tool\",\"name\":\"...\"} 또는 \"tool_name\"\n",
    "        forced_tool: Optional[str] = None\n",
    "        if isinstance(tool_choice, str):\n",
    "            forced_tool = tool_choice\n",
    "        elif isinstance(tool_choice, dict) and tool_choice.get(\"type\") == \"tool\":\n",
    "            forced_tool = tool_choice.get(\"name\")\n",
    "\n",
    "        def _runner(inputs: Union[str, List[BaseMessage], Dict[str, Any]]) -> BaseMessage:\n",
    "            # 입력 정규화: list[BaseMessage] 또는 {\"messages\":[...]} 또는 str\n",
    "            if isinstance(inputs, dict) and \"messages\" in inputs:\n",
    "                messages = inputs[\"messages\"]\n",
    "            elif isinstance(inputs, list) and (len(inputs) == 0 or isinstance(inputs[0], BaseMessage)):\n",
    "                messages = inputs\n",
    "            else:\n",
    "                messages = [HumanMessage(content=str(inputs))]\n",
    "\n",
    "            # tool_choice가 강제된 경우, 시스템 프롬프트에 힌트 추가\n",
    "            sys_msg = SystemMessage(content=sys_prompt if forced_tool is None\n",
    "                                    else sys_prompt + f\"\\nYou MUST call this tool: {forced_tool}\")\n",
    "\n",
    "            result = self._generate([sys_msg] + messages)  # 기존 _generate 재사용\n",
    "            text = result.generations[0].message.content or \"\"\n",
    "\n",
    "            # 코드펜스 제거 및 JSON 파싱\n",
    "            t = text.strip()\n",
    "            if t.startswith(\"```\"):\n",
    "                t = t.strip(\"`\")\n",
    "                lines = [ln for ln in t.splitlines() if ln.lower().strip() not in (\"json\", \"jsonc\")]\n",
    "                t = \"\\n\".join(lines)\n",
    "            try:\n",
    "                obj = json.loads(t)\n",
    "            except Exception:\n",
    "                # JSON 실패 → 도구호출 없이 일반 응답으로 간주\n",
    "                return AIMessage(content=text)\n",
    "\n",
    "            # 최종 분기: tool call 또는 final\n",
    "            if isinstance(obj, dict) and \"tool_name\" in obj:\n",
    "                tool_name = str(obj.get(\"tool_name\", \"\")).strip()\n",
    "                tool_args = obj.get(\"tool_args\", {}) or {}\n",
    "                if forced_tool and tool_name != forced_tool:\n",
    "                    # 강제 도구와 다르게 나오면 강제로 덮어쓰기\n",
    "                    tool_name = forced_tool\n",
    "                if tool_name not in name_set:\n",
    "                    # 존재하지 않는 이름이면 일반 답변 처리\n",
    "                    return AIMessage(content=text)\n",
    "\n",
    "                call_id = f\"call_{uuid.uuid4().hex[:10]}\"\n",
    "                return AIMessage(\n",
    "                    content=\"\",\n",
    "                    tool_calls=[ToolCall(name=tool_name, args=tool_args, id=call_id)]\n",
    "                )\n",
    "\n",
    "            if isinstance(obj, dict) and \"final\" in obj:\n",
    "                return AIMessage(content=str(obj.get(\"final\", \"\")))\n",
    "\n",
    "            # 예상 포맷이 아니면 그냥 텍스트로\n",
    "            return AIMessage(content=text)\n",
    "\n",
    "        return RunnableLambda(_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:25:50.017747Z",
     "iopub.status.busy": "2025-08-27T01:25:50.017640Z",
     "iopub.status.idle": "2025-08-27T01:25:50.045780Z",
     "shell.execute_reply": "2025-08-27T01:25:50.045297Z",
     "shell.execute_reply.started": "2025-08-27T01:25:50.017736Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본적인 구성 및 기능 확인 - Runnable: invoke, batch, stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:25:50.046443Z",
     "iopub.status.busy": "2025-08-27T01:25:50.046254Z",
     "iopub.status.idle": "2025-08-27T01:25:50.066854Z",
     "shell.execute_reply": "2025-08-27T01:25:50.066303Z",
     "shell.execute_reply.started": "2025-08-27T01:25:50.046427Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:25:50.068100Z",
     "iopub.status.busy": "2025-08-27T01:25:50.067903Z",
     "iopub.status.idle": "2025-08-27T01:26:31.842236Z",
     "shell.execute_reply": "2025-08-27T01:26:31.841818Z",
     "shell.execute_reply.started": "2025-08-27T01:25:50.068086Z"
    }
   },
   "outputs": [],
   "source": [
    "res = chat_model.invoke([\n",
    "    SystemMessage(content=\"You are a helpful assistant. Reply in Korean.\"),\n",
    "    HumanMessage(content=\"로컬 Qwen3를 LangChain과 함께 쓰는 법을 요약해줘.\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:26:31.842830Z",
     "iopub.status.busy": "2025-08-27T01:26:31.842691Z",
     "iopub.status.idle": "2025-08-27T01:26:31.844829Z",
     "shell.execute_reply": "2025-08-27T01:26:31.844598Z",
     "shell.execute_reply.started": "2025-08-27T01:26:31.842816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 로컬 Qwen3를 LangChain과 함께 사용하는 방법을 요약해 드리겠습니다. LangChain은 로컬 모델을 쉽게 통합할 수 있도록 도와주는 프레임워크입니다. 다음은 주요 단계와 고려 사항입니다.\n",
      "\n",
      "**1. 환경 설정:**\n",
      "\n",
      "*   **Qwen3 모델 다운로드:** Qwen3 모델을 로컬에 다운로드해야 합니다. Hugging Face Hub에서 다운로드하거나, 직접 다운로드할 수 있습니다. (모델 종류 및 크기에 따라 다운로드 시간이 다를 수 있습니다.)\n",
      "*   **LangChain 설치:** `pip install langchain huggingface_hub` 명령어를 사용하여 LangChain과 Hugging Face Hub를 설치합니다.\n",
      "*   **필요한 라이브러리 설치:**  `pip install sentence_transformers accelerate` 등 LangChain에서 필요로 하는 라이브러리를 설치합니다.\n",
      "*   **GPU 설정 (권장):** GPU를 사용하면 추론 속도를 크게 향상시킬 수 있습니다. CUDA가 제대로 설치되어 있는지 확인하고, LangChain에서 GPU를 사용할 수 있도록 설정합니다.\n",
      "\n",
      "**2. 모델 로드 및 통합:**\n",
      "\n",
      "*   **Hugging Face Hub 모델 로드:** LangChain의 `HuggingFacePipeline`을 사용하여 Qwen3 모델을 로드합니다. 모델 이름, 모델 경로, 추론 설정(temperature, max_tokens 등)을 지정합니다.\n",
      "*   **모델 인스턴스 생성:** `HuggingFacePipeline`을 사용하여 모델 인스턴스를 생성합니다.\n",
      "*   **LangChain 체인 구성:**  Qwen3 모델을 사용하여 텍스트 생성, 질문 답변, 요약 등 다양한 작업을 수행하는 LangChain 체인을 구성합니다.\n",
      "\n",
      "**3. 코드 예시 (간단한 예시):**\n",
      "\n",
      "```python\n",
      "from langchain.llms import HuggingFacePipeline\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "# 모델 로드 (모델 이름은 실제 모델 이름으로 변경)\n",
      "llm = HuggingFacePipeline(model_name=\"your_qwen3_model_name\", model_kwargs={\"temperature\": 0.7})\n",
      "\n",
      "# 프롬프트 템플릿 정의\n",
      "template = \"\"\"다음 질문에 대한 답변을 생성하세요.\n",
      "\n",
      "질문: {question}\n",
      "\n",
      "답변:\"\"\"\n",
      "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
      "\n",
      "# LLM 체인 생성\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# 질문\n",
      "question = \"오늘 날씨 어때?\"\n",
      "\n",
      "# 답변 생성\n",
      "answer = chain.run(question)\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "**4. 고려 사항:**\n",
      "\n",
      "*   **모델 크기:** Qwen3 모델은 크기가 크므로, 충분한 메모리(RAM, GPU 메모리)가 필요합니다.\n",
      "*   **추론 설정:** `temperature`, `max_tokens` 등 추론 설정을 조정하여 원하는 결과 품질과 응답 길이를 얻을 수 있습니다.\n",
      "*   **GPU 메모리 부족:** GPU 메모리가 부족하면 모델 로드 또는 추론 시 오류가 발생할 수 있습니다.  모델 크기를 줄이거나, 배치 사이즈를 줄이는 등의 방법을 고려해야 합니다.\n",
      "*   **Hugging Face Hub 인증:**  Hugging Face Hub에 저장된 모델에 접근하려면 Hugging Face API 키가 필요할 수 있습니다.\n",
      "*   **최신 정보 확인:** LangChain과 Qwen3 관련 최신 정보는 공식 문서를 참고하세요: [https://www.langchain.com/](https://www.langchain.com/) 및 [https://huggingface.co/qwen](https://huggingface.co/qwen)\n",
      "\n",
      "**팁:**\n",
      "\n",
      "*   LangChain의 공식 튜토리얼 및 예제를 참고하면 더욱 쉽게 Qwen3를 통합할 수 있습니다.\n",
      "*   로컬 환경에서 Qwen3를 사용하는 것은 인터넷 연결이 필요하지 않으므로, 오프라인 환경에서도 사용할 수 있다는 장점이 있습니다.\n",
      "\n",
      "궁금한 점이 있다면 언제든지 질문해주세요!  어떤 부분을 더 자세히 알고 싶으신가요? 예를 들어, 특정 체인 구성 방법이나, 문제 해결 방법 등이 있을 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:26:31.845129Z",
     "iopub.status.busy": "2025-08-27T01:26:31.845050Z",
     "iopub.status.idle": "2025-08-27T01:26:31.874158Z",
     "shell.execute_reply": "2025-08-27T01:26:31.873838Z",
     "shell.execute_reply.started": "2025-08-27T01:26:31.845120Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:26:31.875498Z",
     "iopub.status.busy": "2025-08-27T01:26:31.875292Z",
     "iopub.status.idle": "2025-08-27T01:27:16.753666Z",
     "shell.execute_reply": "2025-08-27T01:27:16.753338Z",
     "shell.execute_reply.started": "2025-08-27T01:26:31.875486Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_inputs = [\n",
    "    [HumanMessage(content=\"한 문장으로 자기소개해줘.\")],\n",
    "    [HumanMessage(content=\"파이썬 제너레이터를 간단히 설명해줘.\")],\n",
    "    [HumanMessage(content=\"서울의 대표 관광지 3곳만 알려줘.\")],\n",
    "]\n",
    "outs = chat_model.batch(batch_inputs, config={\"max_concurrency\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:16.754085Z",
     "iopub.status.busy": "2025-08-27T01:27:16.753972Z",
     "iopub.status.idle": "2025-08-27T01:27:16.755908Z",
     "shell.execute_reply": "2025-08-27T01:27:16.755672Z",
     "shell.execute_reply.started": "2025-08-27T01:27:16.754073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 저는 사용자의 질문에 답변하고 다양한 작업을 수행하는 데 도움을 드리는 인공지능 챗봇입니다.\n",
      ">> 파이썬 제너레이터는 **이터레이터를 생성하는 특별한 함수**입니다. 이터레이터는 필요할 때마다 값을 하나씩 생성하여 반환하는 객체입니다. 제너레이터는 메모리를 효율적으로 사용하면서 무한한 시퀀스를 생성하거나, 데이터 스트림을 처리하는 데 유용합니다.\n",
      "\n",
      "**핵심 개념:**\n",
      "\n",
      "* **`yield` 키워드:** 제너레이터 함수는 `yield` 키워드를 사용하여 값을 반환합니다. `yield` 키워드가 호출되면 함수는 현재 상태를 저장하고, 다음 값을 생성하여 반환합니다. 이후 다시 호출되면 저장된 상태에서부터 실행을 재개합니다.\n",
      "* **이터레이터:** 제너레이터 함수는 이터레이터 객체를 반환합니다. 이터레이터는 `next()` 함수를 사용하여 다음 값을 얻을 수 있습니다.\n",
      "* **메모리 효율성:** 제너레이터는 모든 값을 한 번에 메모리에 저장하지 않고, 필요할 때마다 값을 생성하기 때문에 메모리 사용량을 줄일 수 있습니다. 특히 무한한 시퀀스를 처리할 때 유용합니다.\n",
      "\n",
      "**예제:**\n",
      "\n",
      "```python\n",
      "def my_generator(n):\n",
      "  \"\"\"n개의 숫자를 순서대로 생성하는 제너레이터 함수\"\"\"\n",
      "  for i in range(n):\n",
      "    yield i\n",
      "\n",
      "# 제너레이터 객체 생성\n",
      "gen = my_generator(5)\n",
      "\n",
      "# 이터레이터 사용\n",
      "print(next(gen))  # 0 출력\n",
      "print(next(gen))  # 1 출력\n",
      "print(next(gen))  # 2 출력\n",
      "\n",
      "# 제너레이터가 모두 소비되면 StopIteration 예외 발생\n",
      "# print(next(gen))  # StopIteration 예외 발생\n",
      "```\n",
      "\n",
      "**설명:**\n",
      "\n",
      "1. `my_generator(n)` 함수는 `n`개의 숫자를 순서대로 생성하는 제너레이터 함수입니다.\n",
      "2. `yield i`는 `i` 값을 반환하고 함수를 일시 중단합니다.\n",
      "3. `gen = my_generator(5)`는 제너레이터 객체 `gen`을 생성합니다.\n",
      "4. `next(gen)`은 제너레이터 객체에서 다음 값을 가져옵니다.\n",
      "5. 제너레이터가 모든 값을 생성하면 `StopIteration` 예외가 발생합니다.\n",
      "\n",
      "**제너레이터의 장점:**\n",
      "\n",
      "* **메모리 효율성:**  큰 데이터 세트를 처리할 때 메모리 사용량을 줄일 수 있습니다.\n",
      "* **무한 시퀀스 처리:**  무한한 시퀀스를 처리할 수 있습니다.\n",
      "* **코드 가독성 향상:**  복잡한 반복 로직을 간결하게 표현할 수 있습니다.\n",
      "* **비동기 프로그래밍:**  비동기 프로그래밍에서 데이터 스트림을 처리하는 데 유용합니다.\n",
      "\n",
      "**사용 사례:**\n",
      "\n",
      "* 파일 내용을 한 줄씩 읽는 경우\n",
      "* 무한한 스트리밍 데이터 처리\n",
      "* 무한 루프를 사용하는 경우\n",
      "* 데이터베이스에서 결과를 한 번에 처리하는 경우\n",
      "\n",
      "**요약:**\n",
      "\n",
      "제너레이터는 메모리를 효율적으로 사용하면서 값을 생성하는 강력한 도구입니다. `yield` 키워드를 사용하여 이터레이터를 생성하고, `next()` 함수를 사용하여 값을 얻을 수 있습니다.  데이터 스트림 처리, 무한 시퀀스 처리, 메모리 효율성 등의 장점을 제공합니다.\n",
      "\n",
      "궁금한 점이 있다면 언제든지 질문해주세요.\n",
      ">> 서울의 대표적인 관광지 3곳을 알려드릴게요!\n",
      "\n",
      "1.  **경복궁:** 조선시대의 대표적인 왕궁으로, 아름다운 건축물과 넓은 정원을 자랑합니다. 특히, 한복을 입고 궁궐을 거닐며 사진을 찍는 것이 인기 있는 체험입니다.\n",
      "\n",
      "2.  **남산타워:** 서울의 랜드마크인 남산 정상에 위치한 남산타워는 멋진 서울 시내 전망을 감상할 수 있습니다. 케이블카를 타고 올라가거나, 도보로 올라가는 것도 좋습니다.\n",
      "\n",
      "3.  **명동:** 쇼핑과 먹거리가 가득한 활기 넘치는 곳입니다. 다양한 브랜드 매장과 길거리 음식, 화려한 네온사인 등이 어우러져 독특한 분위기를 느낄 수 있습니다.\n",
      "\n",
      "이 외에도 서울에는 다양한 매력적인 관광지가 많으니, 취향에 맞게 방문해보세요!\n"
     ]
    }
   ],
   "source": [
    "for o in outs:\n",
    "    print(\">>\", o.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:16.756287Z",
     "iopub.status.busy": "2025-08-27T01:27:16.756170Z",
     "iopub.status.idle": "2025-08-27T01:27:16.785676Z",
     "shell.execute_reply": "2025-08-27T01:27:16.785272Z",
     "shell.execute_reply.started": "2025-08-27T01:27:16.756274Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:16.786169Z",
     "iopub.status.busy": "2025-08-27T01:27:16.786053Z",
     "iopub.status.idle": "2025-08-27T01:27:51.990446Z",
     "shell.execute_reply": "2025-08-27T01:27:51.990126Z",
     "shell.execute_reply.started": "2025-08-27T01:27:16.786157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3 모델은 중국의 Alibaba Cloud에서 개발한 대규모 언어 모델(LLM)로, 뛰어난 성능과 효율성을 자랑하며 여러 가지 장점과 단점을 가지고 있습니다.\n",
      "\n",
      "**장점:**\n",
      "\n",
      "*   **뛰어난 성능:**\n",
      "    *   **다양한 벤치마크에서 높은 순위:** Qwen3은 다양한 벤치마크 테스트에서 경쟁 모델들을 능가하는 성능을 보여줍니다. 특히, 중국어 및 영어 기반의 질문 답변, 텍스트 생성, 요약 등 다양한 작업에서 우수한 성능을 보입니다.\n",
      "    *   **멀티태스크 능력:** Qwen3은 텍스트 생성, 번역, 요약, 질의응답 등 다양한 작업을 동시에 수행할 수 있는 멀티태스크 능력을 갖추고 있습니다.\n",
      "    *   **긴 문맥 처리 능력:** Qwen3은 긴 문맥을 처리하고 이해하는 능력이 뛰어나, 장문의 텍스트를 기반으로 답변을 생성하거나 요약하는 데 효과적입니다.\n",
      "*   **효율성:**\n",
      "    *   **경량화 모델:** Qwen3은 이전 모델에 비해 파라미터 수를 줄이고 효율적인 아키텍처를 사용하여 더 적은 컴퓨팅 자원으로도 높은 성능을 낼 수 있습니다.\n",
      "    *   **빠른 추론 속도:** Qwen3은 빠른 추론 속도를 제공하여 실시간 응답이 필요한 서비스에 적합합니다.\n",
      "*   **다양한 버전:** Qwen3은 다양한 크기와 성능을 가진 여러 버전으로 제공되어 사용자의 요구사항에 맞게 선택할 수 있습니다. (예: Qwen3-7B, Qwen3-13B, Qwen3-70B)\n",
      "*   **오픈 소스:** Qwen3은 오픈 소스로 제공되어 연구 및 개발에 자유롭게 활용할 수 있습니다. (라이선스 확인 필요)\n",
      "*   **중국어 특화:** 중국어 데이터셋을 기반으로 학습되어 중국어 관련 작업에서 뛰어난 성능을 보입니다.\n",
      "\n",
      "**단점:**\n",
      "\n",
      "*   **영어 성능:** Qwen3은 중국어에 비해 영어 성능이 상대적으로 낮을 수 있습니다.\n",
      "*   **환각 (Hallucination):** LLM의 일반적인 문제점이지만, Qwen3 역시 때때로 사실과 다른 내용을 생성하거나 환각 현상을 일으킬 수 있습니다.\n",
      "*   **편향성:** Qwen3은 학습 데이터에 포함된 편향성을 반영할 수 있으며, 이는 생성된 텍스트에 나타날 수 있습니다.\n",
      "*   **최신 정보 부족:** Qwen3은 학습 데이터의 시점을 기준으로 하며, 최신 정보를 반영하지 못할 수 있습니다. (최신 정보 업데이트 필요)\n",
      "*   **상용 API 사용:** Qwen3을 직접 사용하기 위해서는 Alibaba Cloud의 상용 API를 사용해야 합니다. (비용 발생 가능)\n",
      "\n",
      "**참고 자료:**\n",
      "\n",
      "*   **Qwen3 공식 홈페이지:** [https://www.qwen.ai/](https://www.qwen.ai/)\n",
      "*   **Qwen3 GitHub:** [https://github.com/beyondcmp/Qwen](https://github.com/beyondcmp/Qwen)\n",
      "\n",
      "**결론:**\n",
      "\n",
      "Qwen3은 뛰어난 성능과 효율성을 갖춘 LLM으로, 특히 중국어 관련 작업에 유용합니다. 하지만 영어 성능, 환각, 편향성 등의 단점을 고려하여 사용해야 합니다.\n",
      "\n",
      "궁금한 점이 있다면 언제든지 질문해주세요."
     ]
    }
   ],
   "source": [
    "for chunk in chat_model.stream([HumanMessage(content=\"Qwen3 모델의 장점과 단점을 알려줘.\")]):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:51.990827Z",
     "iopub.status.busy": "2025-08-27T01:27:51.990736Z",
     "iopub.status.idle": "2025-08-27T01:27:51.992391Z",
     "shell.execute_reply": "2025-08-27T01:27:51.992168Z",
     "shell.execute_reply.started": "2025-08-27T01:27:51.990818Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:51.992688Z",
     "iopub.status.busy": "2025-08-27T01:27:51.992606Z",
     "iopub.status.idle": "2025-08-27T01:27:52.023775Z",
     "shell.execute_reply": "2025-08-27T01:27:52.023456Z",
     "shell.execute_reply.started": "2025-08-27T01:27:51.992680Z"
    }
   },
   "outputs": [],
   "source": [
    "class MovieInfo(BaseModel):\n",
    "    title: str = Field(..., description=\"영화 제목\")\n",
    "    year: int = Field(..., description=\"개봉 연도\")\n",
    "    genres: list[str] = Field(..., description=\"장르\")\n",
    "    rating: float = Field(..., description=\"10점 만점 평점\")\n",
    "\n",
    "structured_llm = chat_model.with_structured_output(MovieInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:52.024155Z",
     "iopub.status.busy": "2025-08-27T01:27:52.024070Z",
     "iopub.status.idle": "2025-08-27T01:27:55.132706Z",
     "shell.execute_reply": "2025-08-27T01:27:55.132375Z",
     "shell.execute_reply.started": "2025-08-27T01:27:52.024146Z"
    }
   },
   "outputs": [],
   "source": [
    "result: MovieInfo = structured_llm.invoke(\n",
    "    \"한국 영화 '괴물'의 제목, 개봉연도, 장르들, 대략적 평점을 JSON으로만 답해줘.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:55.133074Z",
     "iopub.status.busy": "2025-08-27T01:27:55.132987Z",
     "iopub.status.idle": "2025-08-27T01:27:55.134908Z",
     "shell.execute_reply": "2025-08-27T01:27:55.134690Z",
     "shell.execute_reply.started": "2025-08-27T01:27:55.133065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='괴물' year=2016 genres=['스릴러', '미스터리', '호러'] rating=8.2\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공통 유틸/도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:55.135190Z",
     "iopub.status.busy": "2025-08-27T01:27:55.135115Z",
     "iopub.status.idle": "2025-08-27T01:27:55.162545Z",
     "shell.execute_reply": "2025-08-27T01:27:55.162197Z",
     "shell.execute_reply.started": "2025-08-27T01:27:55.135182Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:55.162937Z",
     "iopub.status.busy": "2025-08-27T01:27:55.162849Z",
     "iopub.status.idle": "2025-08-27T01:27:55.190637Z",
     "shell.execute_reply": "2025-08-27T01:27:55.190316Z",
     "shell.execute_reply.started": "2025-08-27T01:27:55.162928Z"
    }
   },
   "outputs": [],
   "source": [
    "@tool(\"multiply\")\n",
    "def multiply_tool(expr: str) -> str:\n",
    "    \"\"\"\n",
    "    두 수의 곱을 반환합니다.\n",
    "    입력 형식 예: \"12 7\", \"12,7\", \"12 x 7\", \"곱하기 12와 7\"\n",
    "    \"\"\"\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", expr)\n",
    "    if len(nums) < 2:\n",
    "        return \"오류: 두 개의 숫자를 찾아야 합니다. 예: '12 7'\"\n",
    "    x, y = float(nums[0]), float(nums[1])\n",
    "    return str(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:55.190997Z",
     "iopub.status.busy": "2025-08-27T01:27:55.190914Z",
     "iopub.status.idle": "2025-08-27T01:27:55.215945Z",
     "shell.execute_reply": "2025-08-27T01:27:55.215601Z",
     "shell.execute_reply.started": "2025-08-27T01:27:55.190989Z"
    }
   },
   "outputs": [],
   "source": [
    "CITY_TO_COUNTRY: Dict[str, str] = {\n",
    "    \"Seoul\": \"South Korea\",\n",
    "    \"Tokyo\": \"Japan\",\n",
    "    \"Paris\": \"France\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:55.216320Z",
     "iopub.status.busy": "2025-08-27T01:27:55.216219Z",
     "iopub.status.idle": "2025-08-27T01:27:55.245377Z",
     "shell.execute_reply": "2025-08-27T01:27:55.245058Z",
     "shell.execute_reply.started": "2025-08-27T01:27:55.216310Z"
    }
   },
   "outputs": [],
   "source": [
    "@tool(\"lookup_country\")\n",
    "def lookup_country_tool(city_text: str) -> str:\n",
    "    \"\"\"\n",
    "    도시가 속한 국가를 반환합니다. (모르면 'unknown')\n",
    "    입력은 도시명만 주거나 문장 속에 포함해도 됩니다. 예: \"Paris\", \"도시는 Seoul\"\n",
    "    \"\"\"\n",
    "    # 가장 그럴듯한 '단어' 하나를 도시로 취급\n",
    "    # (간단히 첫 영문 단어를 우선, 없으면 첫 한글/영문 토큰)\n",
    "    m = re.search(r\"[A-Za-z]+\", city_text)\n",
    "    city = m.group(0) if m else city_text.strip().split()[0]\n",
    "    return CITY_TO_COUNTRY.get(city, \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:55.245720Z",
     "iopub.status.busy": "2025-08-27T01:27:55.245623Z",
     "iopub.status.idle": "2025-08-27T01:27:55.272680Z",
     "shell.execute_reply": "2025-08-27T01:27:55.272363Z",
     "shell.execute_reply.started": "2025-08-27T01:27:55.245711Z"
    }
   },
   "outputs": [],
   "source": [
    "TOOLS = [multiply_tool, lookup_country_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct : 도구 설명을 보고, 필요한 순간에 툴을 호출하며 추론-행동을 번갈아 수행하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:55.273028Z",
     "iopub.status.busy": "2025-08-27T01:27:55.272943Z",
     "iopub.status.idle": "2025-08-27T01:27:55.295009Z",
     "shell.execute_reply": "2025-08-27T01:27:55.294631Z",
     "shell.execute_reply.started": "2025-08-27T01:27:55.273019Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:55.295376Z",
     "iopub.status.busy": "2025-08-27T01:27:55.295287Z",
     "iopub.status.idle": "2025-08-27T01:27:55.322817Z",
     "shell.execute_reply": "2025-08-27T01:27:55.322565Z",
     "shell.execute_reply.started": "2025-08-27T01:27:55.295367Z"
    }
   },
   "outputs": [],
   "source": [
    "react_agent = initialize_agent(\n",
    "        tools=TOOLS,\n",
    "        llm=chat_model,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # 간단한 ReAct 프롬프트 내장\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:27:55.323151Z",
     "iopub.status.busy": "2025-08-27T01:27:55.323067Z",
     "iopub.status.idle": "2025-08-27T01:28:07.232500Z",
     "shell.execute_reply": "2025-08-27T01:28:07.232083Z",
     "shell.execute_reply.started": "2025-08-27T01:27:55.323142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to multiply 12 and 7, and also find out which country Paris is the capital of. I should respond in Korean.\n",
      "Action: multiply\n",
      "Action Input: \"12 7\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m84.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the result of the multiplication. Next, I need to find out which country Paris is the capital of.\n",
      "Action: lookup_country\n",
      "Action Input: \"Paris\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mFrance\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the result of the multiplication and the country Paris is in.\n",
      "Final Answer: 12와 7을 곱하면 84입니다. 파리는 프랑스 수도입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = react_agent.invoke(\n",
    "    {\"input\": \"12와 7을 곱하고, 'Paris'가 어느 나라 수도인지도 알려줘. 대답은 한국어로 해줘.\"}\n",
    ")[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:07.232930Z",
     "iopub.status.busy": "2025-08-27T01:28:07.232834Z",
     "iopub.status.idle": "2025-08-27T01:28:07.234679Z",
     "shell.execute_reply": "2025-08-27T01:28:07.234469Z",
     "shell.execute_reply.started": "2025-08-27T01:28:07.232919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12와 7을 곱하면 84입니다. 파리는 프랑스 수도입니다.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational : 대화 기록을 활용하여 맥락을 유지하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:07.234969Z",
     "iopub.status.busy": "2025-08-27T01:28:07.234892Z",
     "iopub.status.idle": "2025-08-27T01:28:07.267819Z",
     "shell.execute_reply": "2025-08-27T01:28:07.267476Z",
     "shell.execute_reply.started": "2025-08-27T01:28:07.234961Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:07.269273Z",
     "iopub.status.busy": "2025-08-27T01:28:07.269174Z",
     "iopub.status.idle": "2025-08-27T01:28:07.295435Z",
     "shell.execute_reply": "2025-08-27T01:28:07.295174Z",
     "shell.execute_reply.started": "2025-08-27T01:28:07.269263Z"
    }
   },
   "outputs": [],
   "source": [
    "history_store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:07.295771Z",
     "iopub.status.busy": "2025-08-27T01:28:07.295679Z",
     "iopub.status.idle": "2025-08-27T01:28:07.322836Z",
     "shell.execute_reply": "2025-08-27T01:28:07.322565Z",
     "shell.execute_reply.started": "2025-08-27T01:28:07.295762Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in history_store:\n",
    "        history_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return history_store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:07.323194Z",
     "iopub.status.busy": "2025-08-27T01:28:07.323110Z",
     "iopub.status.idle": "2025-08-27T01:28:07.348398Z",
     "shell.execute_reply": "2025-08-27T01:28:07.348060Z",
     "shell.execute_reply.started": "2025-08-27T01:28:07.323185Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 유용하고 간결한 한국어 어시스턴트야.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:07.348717Z",
     "iopub.status.busy": "2025-08-27T01:28:07.348636Z",
     "iopub.status.idle": "2025-08-27T01:28:07.373367Z",
     "shell.execute_reply": "2025-08-27T01:28:07.373083Z",
     "shell.execute_reply.started": "2025-08-27T01:28:07.348708Z"
    }
   },
   "outputs": [],
   "source": [
    "base_chain = prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:07.373705Z",
     "iopub.status.busy": "2025-08-27T01:28:07.373621Z",
     "iopub.status.idle": "2025-08-27T01:28:07.401156Z",
     "shell.execute_reply": "2025-08-27T01:28:07.400875Z",
     "shell.execute_reply.started": "2025-08-27T01:28:07.373696Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_agent = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:07.401521Z",
     "iopub.status.busy": "2025-08-27T01:28:07.401426Z",
     "iopub.status.idle": "2025-08-27T01:28:07.428812Z",
     "shell.execute_reply": "2025-08-27T01:28:07.428538Z",
     "shell.execute_reply.started": "2025-08-27T01:28:07.401511Z"
    }
   },
   "outputs": [],
   "source": [
    "session_cfg = {\"configurable\": {\"session_id\": \"demo-user-1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:07.429181Z",
     "iopub.status.busy": "2025-08-27T01:28:07.429095Z",
     "iopub.status.idle": "2025-08-27T01:28:24.533108Z",
     "shell.execute_reply": "2025-08-27T01:28:24.532753Z",
     "shell.execute_reply.started": "2025-08-27T01:28:07.429173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] first question\n",
      "Gemma는 구글에서 개발한 **오픈 웨이트 언어 모델(LLM)**입니다. \n",
      "\n",
      "**핵심 특징:**\n",
      "\n",
      "*   **작지만 강력:** Gemma는 다양한 크기의 모델(Gemma 2B, Gemma 7B)로 제공되어, 비교적 적은 컴퓨팅 자원으로도 효율적인 성능을 낼 수 있습니다.\n",
      "*   **다재다능:** 텍스트 생성, 번역, 질의 응답 등 다양한 자연어 처리 작업에 활용 가능합니다.\n",
      "*   **오픈 웨이트:** 모델 가중치를 공개하여 누구나 자유롭게 사용, 수정, 배포할 수 있습니다.\n",
      "*   **안전성 강조:** 유해하거나 편향된 답변을 줄이기 위한 안전 필터링 기술이 적용되어 있습니다.\n",
      "\n",
      "**주요 정보:**\n",
      "\n",
      "*   **개발:** 구글\n",
      "*   **출시:** 2023년 1월\n",
      "*   **라이선스:** Apache 2.0\n",
      "\n",
      "**더 자세한 정보는 다음 링크에서 확인하실 수 있습니다:**\n",
      "\n",
      "*   [Gemma 모델 공식 페이지](https://ai.google.dev/gemma)\n",
      "\n",
      "궁금한 점이 있다면 언제든지 질문해주세요.\n",
      "\n",
      "[+] second question\n",
      "Gemma에는 **Gemma 2B**와 **Gemma 7B** 두 가지 버전이 있습니다.\n",
      "\n",
      "*   **Gemma 2B:** 더 작고 가벼운 모델로, 비교적 적은 컴퓨팅 자원으로도 빠르게 실행할 수 있습니다.\n",
      "*   **Gemma 7B:** 더 큰 모델로, 더 복잡한 작업이나 더 높은 성능이 필요한 경우에 적합합니다.\n",
      "\n",
      "각 모델은 크기와 성능, 그리고 필요한 컴퓨팅 자원 간의 균형을 제공합니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"[+] first question\")\n",
    "print(conv_agent.invoke({\"input\": \"Gemma 모델에 대해 설명해 주세요.\"}, config=session_cfg))\n",
    "print(\"\\n[+] second question\")\n",
    "print(conv_agent.invoke({\"input\": \"어떤 버전이 있다고 했었나요?\"}, config=session_cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning : 복잡한 목표를 받아 계획을 세운 뒤 순차적으로 정답을 찾아가는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:24.533562Z",
     "iopub.status.busy": "2025-08-27T01:28:24.533449Z",
     "iopub.status.idle": "2025-08-27T01:28:24.535206Z",
     "shell.execute_reply": "2025-08-27T01:28:24.534952Z",
     "shell.execute_reply.started": "2025-08-27T01:28:24.533550Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:24.535606Z",
     "iopub.status.busy": "2025-08-27T01:28:24.535500Z",
     "iopub.status.idle": "2025-08-27T01:28:24.564719Z",
     "shell.execute_reply": "2025-08-27T01:28:24.564371Z",
     "shell.execute_reply.started": "2025-08-27T01:28:24.535595Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.plan_and_execute import (\n",
    "    load_chat_planner,\n",
    "    load_agent_executor,\n",
    "    PlanAndExecute,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:24.565093Z",
     "iopub.status.busy": "2025-08-27T01:28:24.565007Z",
     "iopub.status.idle": "2025-08-27T01:28:24.593539Z",
     "shell.execute_reply": "2025-08-27T01:28:24.593226Z",
     "shell.execute_reply.started": "2025-08-27T01:28:24.565085Z"
    }
   },
   "outputs": [],
   "source": [
    "planner = load_chat_planner(chat_model)\n",
    "executor = load_agent_executor(chat_model, TOOLS, verbose=True)  # 내부적으로 ReAct 계열\n",
    "plan_agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:24.593920Z",
     "iopub.status.busy": "2025-08-27T01:28:24.593823Z",
     "iopub.status.idle": "2025-08-27T01:28:40.234568Z",
     "shell.execute_reply": "2025-08-27T01:28:40.234213Z",
     "shell.execute_reply.started": "2025-08-27T01:28:24.593911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PlanAndExecute chain...\u001b[0m\n",
      "steps=[Step(value=' Calculate 12 multiplied by 7.'), Step(value=' Determine the capital of France.'), Step(value=' Provide the answer in Korean.'), Step(value=' Calculate 12 * 7 = 84.'), Step(value=' France is a country with Paris as its capital.'), Step(value=\" Given the above steps taken, please respond to the users original question.\\n\\n12와 7을 곱하면 84입니다. 그리고 'Paris'는 프랑스의 수도입니다. (12 * 7 = 84, Paris is the capital of France.) \")]\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:```json\n",
      "{\n",
      "  \"action\": \"multiply\",\n",
      "  \"action_input\": \"12 7\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m84.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:```json\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"84\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step:  Calculate 12 multiplied by 7.\n",
      "\n",
      "Response: 84\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"lookup_country\",\n",
      "  \"action_input\": \"France\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step:  Determine the capital of France.\n",
      "\n",
      "Response: {\n",
      "  \"action\": \"lookup_country\",\n",
      "  \"action_input\": \"France\"\n",
      "}\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"열이십 사\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step:  Provide the answer in Korean.\n",
      "\n",
      "Response: {\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"열이십 사\"\n",
      "}\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"84\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step:  Calculate 12 * 7 = 84.\n",
      "\n",
      "Response: {\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"84\"\n",
      "}\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"열이십 사\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step:  France is a country with Paris as its capital.\n",
      "\n",
      "Response: {\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"열이십 사\"\n",
      "}\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"12와 7을 곱하면 84입니다. 그리고 파리는 프랑스의 수도입니다.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step:  Given the above steps taken, please respond to the users original question.\n",
      "\n",
      "12와 7을 곱하면 84입니다. 그리고 'Paris'는 프랑스의 수도입니다. (12 * 7 = 84, Paris is the capital of France.) \n",
      "\n",
      "Response: {\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"12와 7을 곱하면 84입니다. 그리고 파리는 프랑스의 수도입니다.\"\n",
      "}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = plan_agent.invoke(\n",
    "    {\"input\": \"12와 7을 곱하고, 'Paris'가 어느 나라 수도인지도 알려줘. 대답은 한국어로 해줘.\"}\n",
    ")[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:40.234961Z",
     "iopub.status.busy": "2025-08-27T01:28:40.234843Z",
     "iopub.status.idle": "2025-08-27T01:28:40.236720Z",
     "shell.execute_reply": "2025-08-27T01:28:40.236474Z",
     "shell.execute_reply.started": "2025-08-27T01:28:40.234950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"12와 7을 곱하면 84입니다. 그리고 파리는 프랑스의 수도입니다.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning : 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:40.237055Z",
     "iopub.status.busy": "2025-08-27T01:28:40.236966Z",
     "iopub.status.idle": "2025-08-27T01:28:40.259924Z",
     "shell.execute_reply": "2025-08-27T01:28:40.259578Z",
     "shell.execute_reply.started": "2025-08-27T01:28:40.237046Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:40.260333Z",
     "iopub.status.busy": "2025-08-27T01:28:40.260239Z",
     "iopub.status.idle": "2025-08-27T01:28:40.280933Z",
     "shell.execute_reply": "2025-08-27T01:28:40.280522Z",
     "shell.execute_reply.started": "2025-08-27T01:28:40.260323Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Planner: 입력 목표를 보고 JSON 배열로 \"간단한 단계 목록\"을 생성\n",
    "PLANNER_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"너는 계획 수립 보조자야. 사용자의 목표를 달성하기 위한 \"\n",
    "     \"간단하고 실행 가능한 단계(step)만 JSON 배열로 출력해. \"\n",
    "     \"설명/코드블록 없이 JSON만. 각 원소는 한국어 한 문장으로.\"),\n",
    "    (\"human\",\n",
    "     \"목표: {goal}\\n\"\n",
    "     \"출력 형식 예시: [\\\"수치 A와 B의 곱을 계산한다\\\", \\\"도시 X의 국가를 조회한다\\\", \\\"결과를 한 문장으로 정리한다\\\"]\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:40.281439Z",
     "iopub.status.busy": "2025-08-27T01:28:40.281313Z",
     "iopub.status.idle": "2025-08-27T01:28:40.297249Z",
     "shell.execute_reply": "2025-08-27T01:28:40.296844Z",
     "shell.execute_reply.started": "2025-08-27T01:28:40.281424Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2) Plan-and-Execute 래퍼\n",
    "class SimplePlanAndExecute:\n",
    "    def __init__(self, llm, tools):\n",
    "        self.llm = llm\n",
    "        self.planner = PLANNER_PROMPT | chat_model | StrOutputParser()\n",
    "        self.executor = initialize_agent(tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)  # 스텝 실행은 ReAct로\n",
    "\n",
    "    def invoke(self, inputs: dict) -> dict:\n",
    "        goal = inputs[\"input\"] if \"input\" in inputs else inputs.get(\"goal\", \"\")\n",
    "        # (a) 계획 생성\n",
    "        plan_text = self.planner.invoke({\"goal\": goal}).strip()\n",
    "        try:\n",
    "            steps: List[str] = json.loads(plan_text)\n",
    "            assert isinstance(steps, list) and all(isinstance(s, str) for s in steps)\n",
    "        except Exception:\n",
    "            # 혹시 JSON이 아니면 아주 단순 폴백\n",
    "            steps = [goal]\n",
    "\n",
    "        # (b) 스텝별 실행 (각 스텝을 자연어 태스크로 ReAct 에이전트에 던짐)\n",
    "        observations = []\n",
    "        for i, step in enumerate(steps, 1):\n",
    "            out = self.executor.invoke({\"input\": step})\n",
    "            obs = out[\"output\"] if isinstance(out, dict) and \"output\" in out else str(out)\n",
    "            observations.append(f\"[{i}] {step} -> {obs}\")\n",
    "\n",
    "        # (c) 요약/정리 한 번 더 요청(선택)\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"다음 관찰 로그를 간결하게 한국어로 종합 보고서 한 문장으로 요약해.\"),\n",
    "            (\"human\", \"{log}\")\n",
    "        ])\n",
    "        summarizer = summary_prompt | self.llm | StrOutputParser()\n",
    "        summary = summarizer.invoke({\"log\": \"\\n\".join(observations)})\n",
    "\n",
    "        return {\n",
    "            \"plan\": steps,\n",
    "            \"observations\": observations,\n",
    "            \"output\": summary.strip(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:40.297775Z",
     "iopub.status.busy": "2025-08-27T01:28:40.297655Z",
     "iopub.status.idle": "2025-08-27T01:28:40.318610Z",
     "shell.execute_reply": "2025-08-27T01:28:40.318274Z",
     "shell.execute_reply.started": "2025-08-27T01:28:40.297763Z"
    }
   },
   "outputs": [],
   "source": [
    "planning_agent = SimplePlanAndExecute(chat_model, TOOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:40.319005Z",
     "iopub.status.busy": "2025-08-27T01:28:40.318909Z",
     "iopub.status.idle": "2025-08-27T01:28:55.044660Z",
     "shell.execute_reply": "2025-08-27T01:28:55.044278Z",
     "shell.execute_reply.started": "2025-08-27T01:28:40.318995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: multiply\n",
      "Action Input: \"15 4\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m60.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: lookup_country\n",
      "Action Input: \"Seoul\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mSouth Korea\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m15와 4를 곱한 결과는 60이고, Seoul은 대한민국입니다.\n",
      "Final Answer: 15와 4를 곱한 결과는 60이고, Seoul은 대한민국입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res = planning_agent.invoke({\n",
    "    \"input\": \"1) 15와 4를 곱하고 2) 그 결과와 함께 Seoul의 국가를 한 문장으로 정리\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T01:28:55.045063Z",
     "iopub.status.busy": "2025-08-27T01:28:55.044974Z",
     "iopub.status.idle": "2025-08-27T01:28:55.047108Z",
     "shell.execute_reply": "2025-08-27T01:28:55.046878Z",
     "shell.execute_reply.started": "2025-08-27T01:28:55.045053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [Planning — no experimental] ===\n",
      "Plan: ['1) 15와 4를 곱하고 2) 그 결과와 함께 Seoul의 국가를 한 문장으로 정리']\n",
      "Observations:\n",
      "[1] 1) 15와 4를 곱하고 2) 그 결과와 함께 Seoul의 국가를 한 문장으로 정리 -> 15와 4를 곱한 결과는 60이고, Seoul은 대한민국입니다.\n",
      "Final: 15와 4를 곱한 결과는 60이고, 서울은 대한민국입니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== [Planning — no experimental] ===\")\n",
    "print(\"Plan:\", res[\"plan\"])\n",
    "print(\"Observations:\", *res[\"observations\"], sep=\"\\n\")\n",
    "print(\"Final:\", res[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

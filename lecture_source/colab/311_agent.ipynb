{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yLT1RFEeQgm"
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esrEO7yteQgm"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEBCNVjffATu"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oQenpTCPMyh"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU6jp1fqeQgn"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btydbHNpeQgn"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 1024*5, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    # device_map = {\"\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPtcr98MeQgn"
   },
   "outputs": [],
   "source": [
    "model = FastModel.for_inference(model)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Ej1_yjeQgn"
   },
   "source": [
    "# Custom ChatModel 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, Iterator, List, Literal, Optional, Type, Union\n",
    "from pydantic import BaseModel as PydanticBaseModel\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk, HumanMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult, ChatGenerationChunk\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from transformers import TextIteratorStreamer\n",
    "import threading\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9, verbose: bool = False, **kwargs: Any):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "        object.__setattr__(self, \"verbose\", verbose)\n",
    "        object.__setattr__(self, \"_gen_lock\", threading.Lock())\n",
    "\n",
    "    ### 공통 유틸 ###########################################################################\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[BaseMessage]) -> str:\n",
    "        conv: List[Dict[str, str]] = []\n",
    "        for m in messages:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                conv.append({\"role\": \"system\", \"content\": m.content})\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                conv.append({\"role\": \"user\", \"content\": m.content})\n",
    "            elif isinstance(m, AIMessage):\n",
    "                conv.append({\"role\": \"model\", \"content\": m.content})\n",
    "\n",
    "        formatted = self.tokenizer.apply_chat_template(\n",
    "            conv,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,  # 어시스턴트 턴 시작만 넣고 종료 토큰은 모델이 생성\n",
    "        )\n",
    "        return formatted\n",
    "\n",
    "    def _apply_stop(self, text: str, stop: Optional[List[str]]) -> str:\n",
    "        if not stop:\n",
    "            return text\n",
    "        cut = len(text)\n",
    "        for s in stop:\n",
    "            idx = text.find(s)\n",
    "            if idx != -1:\n",
    "                cut = min(cut, idx)\n",
    "        return text[:cut]\n",
    "\n",
    "    def _build_gen_kwargs(self, **kwargs: Any) -> Dict[str, Any]:\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        if pad_id is None:\n",
    "            pad_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        eot_id = None\n",
    "        try:\n",
    "            eot_id = self.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "            if isinstance(eot_id, list):\n",
    "                eot_id = None\n",
    "        except Exception:\n",
    "            eot_id = None\n",
    "\n",
    "        return {\n",
    "            \"max_new_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"do_sample\": kwargs.get(\"do_sample\", self.do_sample),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"top_p\": kwargs.get(\"top_p\", self.top_p),\n",
    "            \"eos_token_id\": eot_id or self.tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": pad_id,\n",
    "        }\n",
    "\n",
    "    ### Invoke ############################################################################\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        if getattr(self, \"verbose\", False):\n",
    "            print(\"\\n[GemmaChatModel/_generate] ==== FINAL PROMPT ====\\n\" + prompt + \"\\n=================================================\\n\")\n",
    "        # 기본 stop 시퀀스 (ReAct 루프에서 유용)\n",
    "        if stop is None:\n",
    "            stop = [\"\\nObservation:\", \"\\nFinal Answer:\"]\n",
    "        with self._gen_lock:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "        in_len = inputs[\"input_ids\"].shape[1]\n",
    "        gen_tokens = outputs[0][in_len:]\n",
    "        decoded = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "        # 안전망: 종료 마커 제거\n",
    "        decoded = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", decoded)\n",
    "        decoded = self._apply_stop(decoded, stop)\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=decoded))])\n",
    "\n",
    "    ### Batch #############################################################################\n",
    "    def _generate_batch(self, messages_list: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> List[ChatResult]:\n",
    "        \"\"\"\n",
    "        여러 개의 대화를 한 번에 패딩 인코딩하여 generate 가속.\n",
    "        \"\"\"\n",
    "        # [CHANGED] 각 대화를 chat template로 포맷\n",
    "        prompts = [self._format_messages(msgs) for msgs in messages_list]\n",
    "\n",
    "        # padding=True, truncation=True 로 배치 인코딩\n",
    "        tokenized = self.tokenizer(\n",
    "            prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        tokenized = {k: v.to(self.model.device) for k, v in tokenized.items()}\n",
    "        attn = tokenized.get(\"attention_mask\", None)\n",
    "\n",
    "        gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
    "\n",
    "        # [CHANGED] 기본 stop 시퀀스\n",
    "        if stop is None:\n",
    "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**tokenized, **gen_kwargs)\n",
    "\n",
    "        results: List[ChatResult] = []\n",
    "        for i in range(len(prompts)):\n",
    "            # 각 샘플의 프롬프트 길이만큼 잘라서 신규 토큰만 디코딩\n",
    "            if attn is not None:\n",
    "                in_len = int(attn[i].sum().item())\n",
    "            else:\n",
    "                in_len = tokenized[\"input_ids\"][i].shape[0]\n",
    "\n",
    "            gen_tokens = outputs[i][in_len:]\n",
    "            text = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "            text = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", text)  # [CHANGED]\n",
    "            text = self._apply_stop(text, stop)\n",
    "\n",
    "            results.append(\n",
    "                ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    ### Stream ############################################################################\n",
    "    def _stream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"\n",
    "        LangChain의 Runnable .stream() 에서 호출되는 내부 스트리밍 제너레이터.\n",
    "        각 토큰 델타를 ChatGenerationChunk(AIMessageChunk) 로 내보냅니다.\n",
    "        \"\"\"\n",
    "        prompt = self._format_messages(messages)\n",
    "        if getattr(self, \"verbose\", False):\n",
    "            print(\"\\n[GemmaChatModel/_stream] ==== FINAL PROMPT ====\")\n",
    "            print(prompt)\n",
    "            print(\"================================================\\n\")\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
    "\n",
    "        # transformers 스트리머 설정\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # [CHANGED] 기본 stop 시퀀스\n",
    "        if stop is None:\n",
    "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
    "\n",
    "        # generate를 백그라운드에서 수행\n",
    "        def _worker():\n",
    "            with torch.no_grad():\n",
    "                self.model.generate(**inputs, **gen_kwargs, streamer=streamer)\n",
    "\n",
    "        th = threading.Thread(target=_worker, daemon=True)\n",
    "        th.start()\n",
    "\n",
    "        # 누적 후 stop 시퀀스까지 안전하게 잘라서 내보내기\n",
    "        buffer = \"\"\n",
    "        emitted = 0\n",
    "\n",
    "        for piece in streamer:\n",
    "            buffer += piece\n",
    "            # [CHANGED] 실시간으로 후단 종료 마커 제거(시각적 잔여물 방지)\n",
    "            tmp = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", buffer)\n",
    "            trimmed = self._apply_stop(tmp, stop)\n",
    "\n",
    "            # 새로 생긴 구간만 델타로 방출\n",
    "            if len(trimmed) > emitted:\n",
    "                delta = trimmed[emitted:]\n",
    "                emitted = len(trimmed)\n",
    "                yield ChatGenerationChunk(message=AIMessageChunk(content=delta))\n",
    "\n",
    "            # stop 시퀀스 감지되면 중단\n",
    "            if stop and len(trimmed) < len(buffer):\n",
    "                break\n",
    "\n",
    "        # 스레드 정리(최대한 조용히 종료 대기)\n",
    "        th.join(timeout=0.1)\n",
    "\n",
    "    ### Structured output #################################################################\n",
    "    def _build_json_system_prompt(self, schema_text: str) -> str:\n",
    "        # 모델이 JSON만 내도록 강하게 지시 (hallucination 방지용 규칙 포함)\n",
    "        return (\n",
    "            \"You are a strict JSON generator.\\n\"\n",
    "            \"Return ONLY a single JSON object, no prose, no backticks, no explanations.\\n\"\n",
    "            \"Do not include trailing commas. Do not include comments.\\n\"\n",
    "            \"Conform exactly to the following JSON schema (fields, types, required):\\n\"\n",
    "            f\"{schema_text}\\n\"\n",
    "        )\n",
    "\n",
    "    def _ensure_pydantic(self):\n",
    "        if PydanticBaseModel is None:\n",
    "            raise RuntimeError(\n",
    "                \"Pydantic is not available. Install pydantic or pass a dict schema instead of a BaseModel.\"\n",
    "            )\n",
    "\n",
    "    def _schema_to_text(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]]) -> str:\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(\n",
    "            schema, PydanticBaseModel\n",
    "        ):\n",
    "            try:\n",
    "                json_schema = schema.model_json_schema()  # pydantic v2\n",
    "            except Exception:\n",
    "                json_schema = schema.schema()  # pydantic v1\n",
    "            return json.dumps(json_schema, ensure_ascii=False, indent=2)\n",
    "        elif isinstance(schema, dict):\n",
    "            return json.dumps(schema, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"schema must be a Pydantic BaseModel subclass or a dict JSON schema.\"\n",
    "            )\n",
    "\n",
    "    def _parse_structured(self, text: str, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], include_raw: bool):\n",
    "        # 코드블록 등 제거 시도(혹시 들어올 경우)\n",
    "        t = text.strip()\n",
    "        if t.startswith(\"```\"):\n",
    "            # ```json ... ``` 또는 ``` ... ```\n",
    "            t = t.strip(\"`\")\n",
    "            # 첫 줄에 json 명시가 들어있을 수 있음\n",
    "            t = \"\\n\".join(\n",
    "                line for line in t.splitlines() if not line.lower().startswith(\"json\")\n",
    "            )\n",
    "        # JSON 파싱\n",
    "        obj = json.loads(t)\n",
    "\n",
    "        # Pydantic 검증\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(\n",
    "            schema, PydanticBaseModel\n",
    "        ):\n",
    "            validated = (\n",
    "                schema.model_validate(obj)\n",
    "                if hasattr(schema, \"model_validate\")\n",
    "                else schema.parse_obj(obj)\n",
    "            )\n",
    "            return {\"parsed\": validated, \"raw\": text} if include_raw else validated\n",
    "        else:\n",
    "            # dict 스키마는 별도 검증 없이 반환 (원하면 jsonschema로 검증 가능)\n",
    "            return {\"parsed\": obj, \"raw\": text} if include_raw else obj\n",
    "\n",
    "    def with_structured_output(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], *, method: Literal[\"json_mode\"] = \"json_mode\", include_raw: bool = False, system_prefix: Optional[str] = None, deterministic: bool = True):\n",
    "        schema_text = self._schema_to_text(schema)\n",
    "        sys_prompt = self._build_json_system_prompt(schema_text)\n",
    "        if system_prefix:\n",
    "            sys_prompt = system_prefix.rstrip() + \"\\n\\n\" + sys_prompt\n",
    "\n",
    "        def _invoke(messages_or_any):\n",
    "            # 입력을 메시지 리스트로 정규화\n",
    "            if isinstance(messages_or_any, list) and all(\n",
    "                isinstance(m, BaseMessage) for m in messages_or_any\n",
    "            ):\n",
    "                msgs = [SystemMessage(content=sys_prompt)] + messages_or_any\n",
    "            else:\n",
    "                # 문자열/딕셔너리 등도 처리\n",
    "                msgs = [\n",
    "                    SystemMessage(content=sys_prompt),\n",
    "                    HumanMessage(content=str(messages_or_any)),\n",
    "                ]\n",
    "\n",
    "            # 결정론 옵션\n",
    "            kw = {}\n",
    "            if deterministic:\n",
    "                kw = {\"do_sample\": False, \"temperature\": 0.0, \"top_p\": 1.0}\n",
    "\n",
    "            # 1차 시도\n",
    "            result = self._generate(msgs, **kw)\n",
    "            text = result.generations[0].message.content\n",
    "            try:\n",
    "                return self._parse_structured(text, schema, include_raw)\n",
    "            except Exception:\n",
    "                # 재시도: 더 강한 지시\n",
    "                retry_msgs = [\n",
    "                    SystemMessage(\n",
    "                        content=sys_prompt + \"\\nOutput must be valid JSON. Try again.\"\n",
    "                    )\n",
    "                ] + msgs[1:]\n",
    "                result2 = self._generate(retry_msgs, **kw)\n",
    "                text2 = result2.generations[0].message.content\n",
    "                return self._parse_structured(text2, schema, include_raw)\n",
    "\n",
    "        # Runnable 로 래핑해서 반환 (체인 파이프에 바로 사용 가능)\n",
    "        return RunnableLambda(_invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본적인 구성 및 기능 확인 - Runnable: invoke, batch, stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat_model.invoke([\n",
    "    SystemMessage(content=\"You are a helpful assistant. Reply in Korean.\"),\n",
    "    HumanMessage(content=\"로컬 Qwen3를 LangChain과 함께 쓰는 법을 요약해줘.\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = [\n",
    "    [HumanMessage(content=\"한 문장으로 자기소개해줘.\")],\n",
    "    [HumanMessage(content=\"파이썬 제너레이터를 간단히 설명해줘.\")],\n",
    "    [HumanMessage(content=\"서울의 대표 관광지 3곳만 알려줘.\")],\n",
    "]\n",
    "outs = chat_model.batch(batch_inputs, config={\"max_concurrency\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in outs:\n",
    "    print(\">>\", o.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chat_model.stream([HumanMessage(content=\"Qwen3 모델의 장점과 단점을 알려줘.\")]):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieInfo(BaseModel):\n",
    "    title: str = Field(..., description=\"영화 제목\")\n",
    "    year: int = Field(..., description=\"개봉 연도\")\n",
    "    genres: list[str] = Field(..., description=\"장르\")\n",
    "    rating: float = Field(..., description=\"10점 만점 평점\")\n",
    "\n",
    "structured_llm = chat_model.with_structured_output(MovieInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result: MovieInfo = structured_llm.invoke(\n",
    "    \"한국 영화 '괴물'의 제목, 개봉연도, 장르들, 대략적 평점을 JSON으로만 답해줘.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공통 유틸/도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"multiply\")\n",
    "def multiply_tool(expr: str) -> str:\n",
    "    \"\"\"\n",
    "    두 수의 곱을 반환합니다.\n",
    "    입력 형식 예: \"12 7\", \"12,7\", \"12 x 7\", \"곱하기 12와 7\"\n",
    "    \"\"\"\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", expr)\n",
    "    if len(nums) < 2:\n",
    "        return \"오류: 두 개의 숫자를 찾아야 합니다. 예: '12 7'\"\n",
    "    x, y = float(nums[0]), float(nums[1])\n",
    "    return str(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY_TO_COUNTRY: Dict[str, str] = {\n",
    "    \"Seoul\": \"South Korea\",\n",
    "    \"Tokyo\": \"Japan\",\n",
    "    \"Paris\": \"France\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"lookup_country\")\n",
    "def lookup_country_tool(city_text: str) -> str:\n",
    "    \"\"\"\n",
    "    도시가 속한 국가를 반환합니다. (모르면 'unknown')\n",
    "    입력은 도시명만 주거나 문장 속에 포함해도 됩니다. 예: \"Paris\", \"도시는 Seoul\"\n",
    "    \"\"\"\n",
    "    # 가장 그럴듯한 '단어' 하나를 도시로 취급\n",
    "    # (간단히 첫 영문 단어를 우선, 없으면 첫 한글/영문 토큰)\n",
    "    m = re.search(r\"[A-Za-z]+\", city_text)\n",
    "    city = m.group(0) if m else city_text.strip().split()[0]\n",
    "    return CITY_TO_COUNTRY.get(city, \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = [multiply_tool, lookup_country_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct : 도구 설명을 보고, 필요한 순간에 툴을 호출하며 추론-행동을 번갈아 수행하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_agent = initialize_agent(\n",
    "        tools=TOOLS,\n",
    "        llm=chat_model,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # 간단한 ReAct 프롬프트 내장\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = react_agent.invoke(\n",
    "    {\"input\": \"12와 7을 곱하고, 'Paris'가 어느 나라 수도인지도 알려줘. 대답은 한국어로 해줘.\"}\n",
    ")[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational : 대화 기록을 활용하여 맥락을 유지하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in history_store:\n",
    "        history_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return history_store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 유용하고 간결한 한국어 어시스턴트야.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_chain = prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_agent = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_cfg = {\"configurable\": {\"session_id\": \"demo-user-1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[+] first question\")\n",
    "print(conv_agent.invoke({\"input\": \"Gemma 모델에 대해 설명해 주세요.\"}, config=session_cfg))\n",
    "print(\"\\n[+] second question\")\n",
    "print(conv_agent.invoke({\"input\": \"어떤 버전이 있다고 했었나요?\"}, config=session_cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning : 복잡한 목표를 받아 계획을 세운 뒤 순차적으로 정답을 찾아가는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.plan_and_execute import (\n",
    "    load_chat_planner,\n",
    "    load_agent_executor,\n",
    "    PlanAndExecute,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = load_chat_planner(chat_model)\n",
    "executor = load_agent_executor(chat_model, TOOLS, verbose=True)  # 내부적으로 ReAct 계열\n",
    "plan_agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = plan_agent.invoke(\n",
    "    {\"input\": \"12와 7을 곱하고, 'Paris'가 어느 나라 수도인지도 알려줘. 대답은 한국어로 해줘.\"}\n",
    ")[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning : 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Planner: 입력 목표를 보고 JSON 배열로 \"간단한 단계 목록\"을 생성\n",
    "PLANNER_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"너는 계획 수립 보조자야. 사용자의 목표를 달성하기 위한 \"\n",
    "     \"간단하고 실행 가능한 단계(step)만 JSON 배열로 출력해. \"\n",
    "     \"설명/코드블록 없이 JSON만. 각 원소는 한국어 한 문장으로.\"),\n",
    "    (\"human\",\n",
    "     \"목표: {goal}\\n\"\n",
    "     \"출력 형식 예시: [\\\"수치 A와 B의 곱을 계산한다\\\", \\\"도시 X의 국가를 조회한다\\\", \\\"결과를 한 문장으로 정리한다\\\"]\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Plan-and-Execute 래퍼\n",
    "class SimplePlanAndExecute:\n",
    "    def __init__(self, llm, tools):\n",
    "        self.llm = llm\n",
    "        self.planner = PLANNER_PROMPT | chat_model | StrOutputParser()\n",
    "        self.executor = initialize_agent(tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)  # 스텝 실행은 ReAct로\n",
    "\n",
    "    def invoke(self, inputs: dict) -> dict:\n",
    "        goal = inputs[\"input\"] if \"input\" in inputs else inputs.get(\"goal\", \"\")\n",
    "        # (a) 계획 생성\n",
    "        plan_text = self.planner.invoke({\"goal\": goal}).strip()\n",
    "        try:\n",
    "            steps: List[str] = json.loads(plan_text)\n",
    "            assert isinstance(steps, list) and all(isinstance(s, str) for s in steps)\n",
    "        except Exception:\n",
    "            # 혹시 JSON이 아니면 아주 단순 폴백\n",
    "            steps = [goal]\n",
    "\n",
    "        # (b) 스텝별 실행 (각 스텝을 자연어 태스크로 ReAct 에이전트에 던짐)\n",
    "        observations = []\n",
    "        for i, step in enumerate(steps, 1):\n",
    "            out = self.executor.invoke({\"input\": step})\n",
    "            obs = out[\"output\"] if isinstance(out, dict) and \"output\" in out else str(out)\n",
    "            observations.append(f\"[{i}] {step} -> {obs}\")\n",
    "\n",
    "        # (c) 요약/정리 한 번 더 요청(선택)\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"다음 관찰 로그를 간결하게 한국어로 종합 보고서 한 문장으로 요약해.\"),\n",
    "            (\"human\", \"{log}\")\n",
    "        ])\n",
    "        summarizer = summary_prompt | self.llm | StrOutputParser()\n",
    "        summary = summarizer.invoke({\"log\": \"\\n\".join(observations)})\n",
    "\n",
    "        return {\n",
    "            \"plan\": steps,\n",
    "            \"observations\": observations,\n",
    "            \"output\": summary.strip(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_agent = SimplePlanAndExecute(chat_model, TOOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = planning_agent.invoke({\n",
    "    \"input\": \"1) 15와 4를 곱하고 2) 그 결과와 함께 Seoul의 국가를 한 문장으로 정리\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== [Planning — no experimental] ===\")\n",
    "print(\"Plan:\", res[\"plan\"])\n",
    "print(\"Observations:\", *res[\"observations\"], sep=\"\\n\")\n",
    "print(\"Final:\", res[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

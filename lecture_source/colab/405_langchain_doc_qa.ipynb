{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR8E6URZK1X3"
      },
      "source": [
        "# ê¸°ë³¸í™˜ê²½ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFskm2tlKykU"
      },
      "outputs": [],
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsQAGydsKykU"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_KEY = userdata.get(\"HF_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPGOz6I1JkBj"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login(HF_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_XVtfyiLiAi"
      },
      "source": [
        "# ëª¨ë¸ ë¡œë”©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oQenpTCPMyh"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFbLQySFKykV"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/gemma-3-4b-it\",\n",
        "    load_in_4bit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8qqMr_OKykV"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5dHC8DWKykV"
      },
      "outputs": [],
      "source": [
        "# ì„ë² ë”© ìƒì„±ê¸° (í•œêµ­ì–´ í¬í•¨ ëª¨ë¸)\n",
        "# embedding = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-base\")\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSabR55HKykV"
      },
      "source": [
        "# Custom ChatModel í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaQecaFzKykV"
      },
      "outputs": [],
      "source": [
        "from typing import List, Any, ClassVar\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_core.outputs import ChatResult, ChatGeneration\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrJ2GkihKykV"
      },
      "outputs": [],
      "source": [
        "class GemmaChatModel(BaseChatModel):\n",
        "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
        "        super().__init__()\n",
        "        object.__setattr__(self, \"model\", model)\n",
        "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
        "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
        "        object.__setattr__(self, \"do_sample\", do_sample)\n",
        "        object.__setattr__(self, \"temperature\", temperature)\n",
        "        object.__setattr__(self, \"top_p\", top_p)\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"gemma-chat\"\n",
        "\n",
        "    def _format_messages(self, messages: List[Any]) -> str:\n",
        "        prompt = \"\"\n",
        "        for message in messages:\n",
        "            if isinstance(message, SystemMessage):\n",
        "                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
        "            elif isinstance(message, HumanMessage):\n",
        "                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
        "            elif isinstance(message, AIMessage):\n",
        "                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
        "        prompt += \"<|assistant|>\\n\"\n",
        "        return prompt\n",
        "\n",
        "    def _generate(self, messages: List[Any], **kwargs) -> ChatResult:\n",
        "        prompt = self._format_messages(messages)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=self.max_tokens,\n",
        "                do_sample=kwargs.get(\"do_sample\", self.do_sample),\n",
        "                temperature=kwargs.get(\"temperature\", self.temperature),\n",
        "                top_p=kwargs.get(\"top_p\", self.top_p),\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = decoded.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "\n",
        "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58YCpAz2KykV"
      },
      "outputs": [],
      "source": [
        "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iGzg0OpKykV"
      },
      "source": [
        "# Documents ì¤€ë¹„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ocyga5GsKykV"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygakERGdKykV"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3aFQlToKykV"
      },
      "outputs": [],
      "source": [
        "# ë¬¸ì„œ ì¤€ë¹„\n",
        "texts = [\n",
        "    \"ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤.\",\n",
        "    \"ë¶€ì‚°ì€ í•œêµ­ì—ì„œ ë‘ ë²ˆì§¸ë¡œ í° ë„ì‹œì…ë‹ˆë‹¤.\",\n",
        "    \"ì œì£¼ëŠ” ì•„ë¦„ë‹¤ìš´ ì„¬ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1DvslSvKykV"
      },
      "outputs": [],
      "source": [
        "docs = [Document(page_content=t) for t in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lkSPZx9KykW"
      },
      "outputs": [],
      "source": [
        "# í…ìŠ¤íŠ¸ ë¶„í• \n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=32)\n",
        "split_docs = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88nWIOT4KykW"
      },
      "outputs": [],
      "source": [
        "# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
        "vectordb = FAISS.from_documents(split_docs, embedding=embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH6O0YvwKykW"
      },
      "outputs": [],
      "source": [
        "# RetrievalQA ì²´ì¸ ìƒì„±\n",
        "retrieval_chain = RetrievalQA.from_chain_type(\n",
        "    llm=chat_model,                    # ë¡œì»¬ Gemma ëª¨ë¸\n",
        "    chain_type=\"stuff\",                # ê°„ë‹¨í•œ ì²´ì¸ ìœ í˜•\n",
        "    retriever=vectordb.as_retriever(), # ë²¡í„° ê²€ìƒ‰ê¸°\n",
        "    return_source_documents=True       # ì¶œì²˜ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahhu81b_KykW"
      },
      "outputs": [],
      "source": [
        "query = \"í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?\"\n",
        "result = retrieval_chain.invoke(query)\n",
        "\n",
        "print(\"ğŸ’¬ ë‹µë³€:\", result[\"result\"])\n",
        "print(\"ğŸ“„ ì‚¬ìš©ëœ ë¬¸ì„œ:\", [doc.page_content for doc in result[\"source_documents\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jR2v6phKykW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OioqwGIhKykW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_iEIqxvKykW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mZdlYTFKykW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22lk3P9hKykW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# ê¸°ë³¸í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:53:37.197676Z",
     "iopub.status.busy": "2025-08-06T06:53:37.197485Z",
     "iopub.status.idle": "2025-08-06T06:53:37.199521Z",
     "shell.execute_reply": "2025-08-06T06:53:37.199212Z",
     "shell.execute_reply.started": "2025-08-06T06:53:37.197663Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# ëª¨ë¸ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:32.540264Z",
     "iopub.status.busy": "2025-08-06T06:50:32.540176Z",
     "iopub.status.idle": "2025-08-06T06:50:38.935146Z",
     "shell.execute_reply": "2025-08-06T06:50:38.934832Z",
     "shell.execute_reply.started": "2025-08-06T06:50:32.540254Z"
    },
    "id": "1oQenpTCPMyh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-06 15:50:37 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:38.935518Z",
     "iopub.status.busy": "2025-08-06T06:50:38.935425Z",
     "iopub.status.idle": "2025-08-06T06:50:52.774979Z",
     "shell.execute_reply": "2025-08-06T06:50:52.774477Z",
     "shell.execute_reply.started": "2025-08-06T06:50:38.935508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.8: Fast Gemma3 patching. Transformers: 4.54.0. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 2. Max memory: 23.494 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-4b-it\",\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:52.775401Z",
     "iopub.status.busy": "2025-08-06T06:50:52.775313Z",
     "iopub.status.idle": "2025-08-06T06:50:52.778558Z",
     "shell.execute_reply": "2025-08-06T06:50:52.778271Z",
     "shell.execute_reply.started": "2025-08-06T06:50:52.775391Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:52.785433Z",
     "iopub.status.busy": "2025-08-06T06:50:52.785343Z",
     "iopub.status.idle": "2025-08-06T06:50:56.056770Z",
     "shell.execute_reply": "2025-08-06T06:50:56.056385Z",
     "shell.execute_reply.started": "2025-08-06T06:50:52.785425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62757/3640099990.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© ìƒì„±ê¸° (í•œêµ­ì–´ í¬í•¨ ëª¨ë¸)\n",
    "# embedding = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-base\")\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ChatModel í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:56.057206Z",
     "iopub.status.busy": "2025-08-06T06:50:56.057109Z",
     "iopub.status.idle": "2025-08-06T06:50:56.125883Z",
     "shell.execute_reply": "2025-08-06T06:50:56.125553Z",
     "shell.execute_reply.started": "2025-08-06T06:50:56.057196Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Any, ClassVar\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:08.102850Z",
     "iopub.status.busy": "2025-08-06T06:58:08.102649Z",
     "iopub.status.idle": "2025-08-06T06:58:08.108671Z",
     "shell.execute_reply": "2025-08-06T06:58:08.108342Z",
     "shell.execute_reply.started": "2025-08-06T06:58:08.102838Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[Any]) -> str:\n",
    "        prompt = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _generate(self, messages: List[Any], **kwargs) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                do_sample=kwargs.get(\"do_sample\", self.do_sample),\n",
    "                temperature=kwargs.get(\"temperature\", self.temperature),\n",
    "                top_p=kwargs.get(\"top_p\", self.top_p),\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = decoded.split(\"<|assistant|>\\n\")[-1].strip()\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:09.542409Z",
     "iopub.status.busy": "2025-08-06T06:58:09.542219Z",
     "iopub.status.idle": "2025-08-06T06:58:09.544326Z",
     "shell.execute_reply": "2025-08-06T06:58:09.544072Z",
     "shell.execute_reply.started": "2025-08-06T06:58:09.542396Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:10.118046Z",
     "iopub.status.busy": "2025-08-06T06:58:10.117818Z",
     "iopub.status.idle": "2025-08-06T06:58:10.119908Z",
     "shell.execute_reply": "2025-08-06T06:58:10.119636Z",
     "shell.execute_reply.started": "2025-08-06T06:58:10.118029Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:10.499577Z",
     "iopub.status.busy": "2025-08-06T06:58:10.499382Z",
     "iopub.status.idle": "2025-08-06T06:58:10.501478Z",
     "shell.execute_reply": "2025-08-06T06:58:10.501213Z",
     "shell.execute_reply.started": "2025-08-06T06:58:10.499565Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:10.816263Z",
     "iopub.status.busy": "2025-08-06T06:58:10.816073Z",
     "iopub.status.idle": "2025-08-06T06:58:10.818144Z",
     "shell.execute_reply": "2025-08-06T06:58:10.817863Z",
     "shell.execute_reply.started": "2025-08-06T06:58:10.816249Z"
    }
   },
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ì¤€ë¹„\n",
    "texts = [\n",
    "    \"ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤.\",\n",
    "    \"ë¶€ì‚°ì€ í•œêµ­ì—ì„œ ë‘ ë²ˆì§¸ë¡œ í° ë„ì‹œì…ë‹ˆë‹¤.\",\n",
    "    \"ì œì£¼ëŠ” ì•„ë¦„ë‹¤ìš´ ì„¬ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:11.183014Z",
     "iopub.status.busy": "2025-08-06T06:58:11.182819Z",
     "iopub.status.idle": "2025-08-06T06:58:11.184968Z",
     "shell.execute_reply": "2025-08-06T06:58:11.184642Z",
     "shell.execute_reply.started": "2025-08-06T06:58:11.183001Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:11.518148Z",
     "iopub.status.busy": "2025-08-06T06:58:11.517956Z",
     "iopub.status.idle": "2025-08-06T06:58:11.520277Z",
     "shell.execute_reply": "2025-08-06T06:58:11.519991Z",
     "shell.execute_reply.started": "2025-08-06T06:58:11.518135Z"
    }
   },
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=32)\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:11.965909Z",
     "iopub.status.busy": "2025-08-06T06:58:11.965710Z",
     "iopub.status.idle": "2025-08-06T06:58:11.992729Z",
     "shell.execute_reply": "2025-08-06T06:58:11.992316Z",
     "shell.execute_reply.started": "2025-08-06T06:58:11.965896Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "vectordb = FAISS.from_documents(split_docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:27.316909Z",
     "iopub.status.busy": "2025-08-06T06:58:27.316710Z",
     "iopub.status.idle": "2025-08-06T06:58:27.319133Z",
     "shell.execute_reply": "2025-08-06T06:58:27.318844Z",
     "shell.execute_reply.started": "2025-08-06T06:58:27.316897Z"
    }
   },
   "outputs": [],
   "source": [
    "# RetrievalQA ì²´ì¸ ìƒì„±\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,                    # ë¡œì»¬ Gemma ëª¨ë¸\n",
    "    chain_type=\"stuff\",                # ê°„ë‹¨í•œ ì²´ì¸ ìœ í˜•\n",
    "    retriever=vectordb.as_retriever(), # ë²¡í„° ê²€ìƒ‰ê¸°\n",
    "    return_source_documents=True       # ì¶œì²˜ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:28.240436Z",
     "iopub.status.busy": "2025-08-06T06:58:28.240250Z",
     "iopub.status.idle": "2025-08-06T06:58:28.688978Z",
     "shell.execute_reply": "2025-08-06T06:58:28.688653Z",
     "shell.execute_reply.started": "2025-08-06T06:58:28.240423Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ ë‹µë³€: ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤.\n",
      "ğŸ“„ ì‚¬ìš©ëœ ë¬¸ì„œ: ['ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤.', 'ë¶€ì‚°ì€ í•œêµ­ì—ì„œ ë‘ ë²ˆì§¸ë¡œ í° ë„ì‹œì…ë‹ˆë‹¤.', 'ì œì£¼ëŠ” ì•„ë¦„ë‹¤ìš´ ì„¬ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.']\n"
     ]
    }
   ],
   "source": [
    "query = \"í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?\"\n",
    "result = retrieval_chain.invoke(query)\n",
    "\n",
    "print(\"ğŸ’¬ ë‹µë³€:\", result[\"result\"])\n",
    "print(\"ğŸ“„ ì‚¬ìš©ëœ ë¬¸ì„œ:\", [doc.page_content for doc in result[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:53:37.197676Z",
     "iopub.status.busy": "2025-08-06T06:53:37.197485Z",
     "iopub.status.idle": "2025-08-06T06:53:37.199521Z",
     "shell.execute_reply": "2025-08-06T06:53:37.199212Z",
     "shell.execute_reply.started": "2025-08-06T06:53:37.197663Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:32.540264Z",
     "iopub.status.busy": "2025-08-06T06:50:32.540176Z",
     "iopub.status.idle": "2025-08-06T06:50:38.935146Z",
     "shell.execute_reply": "2025-08-06T06:50:38.934832Z",
     "shell.execute_reply.started": "2025-08-06T06:50:32.540254Z"
    },
    "id": "1oQenpTCPMyh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-06 15:50:37 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:38.935518Z",
     "iopub.status.busy": "2025-08-06T06:50:38.935425Z",
     "iopub.status.idle": "2025-08-06T06:50:52.774979Z",
     "shell.execute_reply": "2025-08-06T06:50:52.774477Z",
     "shell.execute_reply.started": "2025-08-06T06:50:38.935508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.8: Fast Gemma3 patching. Transformers: 4.54.0. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 2. Max memory: 23.494 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-4b-it\",\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:52.775401Z",
     "iopub.status.busy": "2025-08-06T06:50:52.775313Z",
     "iopub.status.idle": "2025-08-06T06:50:52.778558Z",
     "shell.execute_reply": "2025-08-06T06:50:52.778271Z",
     "shell.execute_reply.started": "2025-08-06T06:50:52.775391Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:52.785433Z",
     "iopub.status.busy": "2025-08-06T06:50:52.785343Z",
     "iopub.status.idle": "2025-08-06T06:50:56.056770Z",
     "shell.execute_reply": "2025-08-06T06:50:56.056385Z",
     "shell.execute_reply.started": "2025-08-06T06:50:52.785425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62757/3640099990.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 생성기 (한국어 포함 모델)\n",
    "# embedding = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-base\")\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ChatModel 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:50:56.057206Z",
     "iopub.status.busy": "2025-08-06T06:50:56.057109Z",
     "iopub.status.idle": "2025-08-06T06:50:56.125883Z",
     "shell.execute_reply": "2025-08-06T06:50:56.125553Z",
     "shell.execute_reply.started": "2025-08-06T06:50:56.057196Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Any, ClassVar\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:08.102850Z",
     "iopub.status.busy": "2025-08-06T06:58:08.102649Z",
     "iopub.status.idle": "2025-08-06T06:58:08.108671Z",
     "shell.execute_reply": "2025-08-06T06:58:08.108342Z",
     "shell.execute_reply.started": "2025-08-06T06:58:08.102838Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[Any]) -> str:\n",
    "        prompt = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _generate(self, messages: List[Any], **kwargs) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                do_sample=kwargs.get(\"do_sample\", self.do_sample),\n",
    "                temperature=kwargs.get(\"temperature\", self.temperature),\n",
    "                top_p=kwargs.get(\"top_p\", self.top_p),\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = decoded.split(\"<|assistant|>\\n\")[-1].strip()\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:09.542409Z",
     "iopub.status.busy": "2025-08-06T06:58:09.542219Z",
     "iopub.status.idle": "2025-08-06T06:58:09.544326Z",
     "shell.execute_reply": "2025-08-06T06:58:09.544072Z",
     "shell.execute_reply.started": "2025-08-06T06:58:09.542396Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:10.118046Z",
     "iopub.status.busy": "2025-08-06T06:58:10.117818Z",
     "iopub.status.idle": "2025-08-06T06:58:10.119908Z",
     "shell.execute_reply": "2025-08-06T06:58:10.119636Z",
     "shell.execute_reply.started": "2025-08-06T06:58:10.118029Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:10.499577Z",
     "iopub.status.busy": "2025-08-06T06:58:10.499382Z",
     "iopub.status.idle": "2025-08-06T06:58:10.501478Z",
     "shell.execute_reply": "2025-08-06T06:58:10.501213Z",
     "shell.execute_reply.started": "2025-08-06T06:58:10.499565Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:10.816263Z",
     "iopub.status.busy": "2025-08-06T06:58:10.816073Z",
     "iopub.status.idle": "2025-08-06T06:58:10.818144Z",
     "shell.execute_reply": "2025-08-06T06:58:10.817863Z",
     "shell.execute_reply.started": "2025-08-06T06:58:10.816249Z"
    }
   },
   "outputs": [],
   "source": [
    "# 문서 준비\n",
    "texts = [\n",
    "    \"서울은 대한민국의 수도입니다.\",\n",
    "    \"부산은 한국에서 두 번째로 큰 도시입니다.\",\n",
    "    \"제주는 아름다운 섬으로 유명합니다.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:11.183014Z",
     "iopub.status.busy": "2025-08-06T06:58:11.182819Z",
     "iopub.status.idle": "2025-08-06T06:58:11.184968Z",
     "shell.execute_reply": "2025-08-06T06:58:11.184642Z",
     "shell.execute_reply.started": "2025-08-06T06:58:11.183001Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:11.518148Z",
     "iopub.status.busy": "2025-08-06T06:58:11.517956Z",
     "iopub.status.idle": "2025-08-06T06:58:11.520277Z",
     "shell.execute_reply": "2025-08-06T06:58:11.519991Z",
     "shell.execute_reply.started": "2025-08-06T06:58:11.518135Z"
    }
   },
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=32)\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:11.965909Z",
     "iopub.status.busy": "2025-08-06T06:58:11.965710Z",
     "iopub.status.idle": "2025-08-06T06:58:11.992729Z",
     "shell.execute_reply": "2025-08-06T06:58:11.992316Z",
     "shell.execute_reply.started": "2025-08-06T06:58:11.965896Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 벡터 스토어 생성\n",
    "vectordb = FAISS.from_documents(split_docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:27.316909Z",
     "iopub.status.busy": "2025-08-06T06:58:27.316710Z",
     "iopub.status.idle": "2025-08-06T06:58:27.319133Z",
     "shell.execute_reply": "2025-08-06T06:58:27.318844Z",
     "shell.execute_reply.started": "2025-08-06T06:58:27.316897Z"
    }
   },
   "outputs": [],
   "source": [
    "# RetrievalQA 체인 생성\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,                    # 로컬 Gemma 모델\n",
    "    chain_type=\"stuff\",                # 간단한 체인 유형\n",
    "    retriever=vectordb.as_retriever(), # 벡터 검색기\n",
    "    return_source_documents=True       # 출처 문서 포함 여부\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:58:28.240436Z",
     "iopub.status.busy": "2025-08-06T06:58:28.240250Z",
     "iopub.status.idle": "2025-08-06T06:58:28.688978Z",
     "shell.execute_reply": "2025-08-06T06:58:28.688653Z",
     "shell.execute_reply.started": "2025-08-06T06:58:28.240423Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 답변: 서울은 대한민국의 수도입니다.\n",
      "📄 사용된 문서: ['서울은 대한민국의 수도입니다.', '부산은 한국에서 두 번째로 큰 도시입니다.', '제주는 아름다운 섬으로 유명합니다.']\n"
     ]
    }
   ],
   "source": [
    "query = \"한국의 수도는 어디야?\"\n",
    "result = retrieval_chain.invoke(query)\n",
    "\n",
    "print(\"💬 답변:\", result[\"result\"])\n",
    "print(\"📄 사용된 문서:\", [doc.page_content for doc in result[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yLT1RFEeQgm"
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esrEO7yteQgm"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEBCNVjffATu"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oQenpTCPMyh"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU6jp1fqeQgn"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btydbHNpeQgn"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 1024*5, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    device_map = {\"\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPtcr98MeQgn"
   },
   "outputs": [],
   "source": [
    "model = FastModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Ej1_yjeQgn"
   },
   "source": [
    "# Custom ChatModel 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf4trrKVeQgn"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjKeUw07eQgn"
   },
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[BaseMessage]) -> str:\n",
    "        prompt = \"\"\n",
    "        for m in messages:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{m.content}</s>\\n\"\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{m.content}</s>\\n\"\n",
    "            elif isinstance(m, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{m.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _apply_stop(self, text: str, stop: Optional[List[str]]) -> str:\n",
    "        if not stop:\n",
    "            return text\n",
    "        cut = len(text)\n",
    "        for s in stop:\n",
    "            idx = text.find(s)\n",
    "            if idx != -1:\n",
    "                cut = min(cut, idx)\n",
    "        return text[:cut]\n",
    "\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"do_sample\": kwargs.get(\"do_sample\", self.do_sample),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"top_p\": kwargs.get(\"top_p\", self.top_p),\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # 마지막 assistant 턴 이후만 추출\n",
    "        if \"<|assistant|>\\n\" in decoded:\n",
    "            response = decoded.split(\"<|assistant|>\\n\")[-1]\n",
    "        else:\n",
    "            response = decoded\n",
    "        response = response.strip()\n",
    "        response = self._apply_stop(response, stop)\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4L37Lhf-eQgn"
   },
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool, Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"문자열 수식을 계산합니다. 예: '12 * (3 + 4) / 5'\"\"\"\n",
    "    import math\n",
    "    safe = {k: v for k, v in math.__dict__.items() if not k.startswith(\"__\")}\n",
    "    try:\n",
    "        return str(eval(expression, {\"__builtins__\": {}}, safe))\n",
    "    except Exception as e:\n",
    "        return f\"계산 실패: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_time(_: str = \"\") -> str:\n",
    "    \"\"\"현재 시간을 ISO8601 형식으로 반환합니다.\"\"\"\n",
    "    from datetime import datetime, timezone\n",
    "    return datetime.now(timezone.utc).astimezone().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator_tool = Tool(\n",
    "    name=\"CalculatorTool\",\n",
    "    func=calculator,\n",
    "    description=\"문자열 수식을 계산합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tool = Tool(\n",
    "    name=\"TimeTool\",\n",
    "    func=get_time,\n",
    "    description=\"현재 시간을 ISO8601 형식으로 반환합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [calculator_tool, time_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 에이전트 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, Tool, initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=chat_model,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 예시\n",
    "res = agent.invoke({\"input\": \"계산기로 (12 * (3 + 4)) / 5 값을 구해줘.\"})\n",
    "print(res[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(12 * (3 + 4)) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

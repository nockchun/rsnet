{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yLT1RFEeQgm"
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esrEO7yteQgm"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEBCNVjffATu"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oQenpTCPMyh"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU6jp1fqeQgn"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btydbHNpeQgn"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 1024*5, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    device_map = {\"\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPtcr98MeQgn"
   },
   "outputs": [],
   "source": [
    "model = FastModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Ej1_yjeQgn"
   },
   "source": [
    "# Custom ChatModel 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf4trrKVeQgn"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, List, Union, Type, Literal\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from typing import Any, Dict, Optional, List, Union, Type, Literal\n",
    "import json, warnings\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel as PydanticBaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjKeUw07eQgn"
   },
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[BaseMessage]) -> str:\n",
    "        prompt = \"\"\n",
    "        for m in messages:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{m.content}</s>\\n\"\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{m.content}</s>\\n\"\n",
    "            elif isinstance(m, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{m.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _apply_stop(self, text: str, stop: Optional[List[str]]) -> str:\n",
    "        if not stop:\n",
    "            return text\n",
    "        cut = len(text)\n",
    "        for s in stop:\n",
    "            idx = text.find(s)\n",
    "            if idx != -1:\n",
    "                cut = min(cut, idx)\n",
    "        return text[:cut]\n",
    "\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"do_sample\": kwargs.get(\"do_sample\", self.do_sample),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"top_p\": kwargs.get(\"top_p\", self.top_p),\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # 마지막 assistant 턴 이후만 추출\n",
    "        if \"<|assistant|>\\n\" in decoded:\n",
    "            response = decoded.split(\"<|assistant|>\\n\")[-1]\n",
    "        else:\n",
    "            response = decoded\n",
    "        response = response.strip()\n",
    "        response = self._apply_stop(response, stop)\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])\n",
    "\n",
    "    ### Structured output support #######################################################################################\n",
    "    def _build_json_system_prompt(self, schema_text: str) -> str:\n",
    "        # 모델이 JSON만 내도록 강하게 지시 (hallucination 방지용 규칙 포함)\n",
    "        return (\n",
    "            \"You are a strict JSON generator.\\n\"\n",
    "            \"Return ONLY a single JSON object, no prose, no backticks, no explanations.\\n\"\n",
    "            \"Do not include trailing commas. Do not include comments.\\n\"\n",
    "            \"Conform exactly to the following JSON schema (fields, types, required):\\n\"\n",
    "            f\"{schema_text}\\n\"\n",
    "        )\n",
    "\n",
    "    def _ensure_pydantic(self):\n",
    "        if PydanticBaseModel is None:\n",
    "            raise RuntimeError(\n",
    "                \"Pydantic is not available. Install pydantic or pass a dict schema instead of a BaseModel.\"\n",
    "            )\n",
    "\n",
    "    def _schema_to_text(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]]) -> str:\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(schema, PydanticBaseModel):\n",
    "            # pydantic 스키마를 JSON 스키마로 직렬화\n",
    "            try:\n",
    "                # v1/v2 호환 직렬화\n",
    "                json_schema = schema.model_json_schema()  # pydantic v2\n",
    "            except Exception:\n",
    "                json_schema = schema.schema()  # pydantic v1\n",
    "            return json.dumps(json_schema, ensure_ascii=False, indent=2)\n",
    "        elif isinstance(schema, dict):\n",
    "            return json.dumps(schema, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            raise TypeError(\"schema must be a Pydantic BaseModel subclass or a dict JSON schema.\")\n",
    "\n",
    "    def _parse_structured(self, text: str, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], include_raw: bool):\n",
    "        # 코드블록 등 제거 시도(혹시 들어올 경우)\n",
    "        t = text.strip()\n",
    "        if t.startswith(\"```\"):\n",
    "            # ```json ... ``` 또는 ``` ... ```\n",
    "            t = t.strip(\"`\")\n",
    "            # 첫 줄에 json 명시가 들어있을 수 있음\n",
    "            t = \"\\n\".join(line for line in t.splitlines() if not line.lower().startswith(\"json\"))\n",
    "        # JSON 파싱\n",
    "        obj = json.loads(t)\n",
    "\n",
    "        # Pydantic 검증\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(schema, PydanticBaseModel):\n",
    "            validated = schema.model_validate(obj) if hasattr(schema, \"model_validate\") else schema.parse_obj(obj)\n",
    "            return {\"parsed\": validated, \"raw\": text} if include_raw else validated\n",
    "        else:\n",
    "            # dict 스키마는 별도 검증 없이 반환 (원하면 jsonschema로 검증 가능)\n",
    "            return {\"parsed\": obj, \"raw\": text} if include_raw else obj\n",
    "\n",
    "    def with_structured_output(\n",
    "        self,\n",
    "        schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]],\n",
    "        *,\n",
    "        method: Literal[\"json_mode\"] = \"json_mode\",\n",
    "        include_raw: bool = False,\n",
    "        system_prefix: Optional[str] = None,\n",
    "        deterministic: bool = True,\n",
    "    ):\n",
    "        schema_text = self._schema_to_text(schema)\n",
    "        sys_prompt = self._build_json_system_prompt(schema_text)\n",
    "        if system_prefix:\n",
    "            sys_prompt = system_prefix.rstrip() + \"\\n\\n\" + sys_prompt\n",
    "\n",
    "        def _invoke(messages_or_any):\n",
    "            # 입력을 메시지 리스트로 정규화\n",
    "            if isinstance(messages_or_any, list) and all(isinstance(m, BaseMessage) for m in messages_or_any):\n",
    "                msgs = [SystemMessage(content=sys_prompt)] + messages_or_any\n",
    "            else:\n",
    "                # 문자열/딕셔너리 등도 처리\n",
    "                msgs = [SystemMessage(content=sys_prompt), HumanMessage(content=str(messages_or_any))]\n",
    "\n",
    "            # 결정론 옵션\n",
    "            kw = {}\n",
    "            if deterministic:\n",
    "                kw = {\"do_sample\": False, \"temperature\": 0.0, \"top_p\": 1.0}\n",
    "\n",
    "            # 1차 시도\n",
    "            result = self._generate(msgs, **kw)\n",
    "            text = result.generations[0].message.content\n",
    "            try:\n",
    "                return self._parse_structured(text, schema, include_raw)\n",
    "            except Exception:\n",
    "                # 재시도: 더 강한 지시\n",
    "                retry_msgs = [SystemMessage(content=sys_prompt + \"\\nOutput must be valid JSON. Try again.\")] + msgs[1:]\n",
    "                result2 = self._generate(retry_msgs, **kw)\n",
    "                text2 = result2.generations[0].message.content\n",
    "                return self._parse_structured(text2, schema, include_raw)\n",
    "\n",
    "        # Runnable 로 래핑해서 반환 (체인 파이프에 바로 사용 가능)\n",
    "        return RunnableLambda(_invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4L37Lhf-eQgn"
   },
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt와 model 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message = chain.invoke({\"dish\": \"카레\"})\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parser에 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"dish\": \"카레\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PydanticOutputParser 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
    "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
    "    tips: list[str] = Field(description=\"Tips for making food delicious.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = PydanticOutputParser(pydantic_object=Recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\\n\\n{format_instructions}\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_format_instructions = prompt.partial(\n",
    "    format_instructions=output_parser.get_format_instructions()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*5, temperature=0.3).bind(\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_with_format_instructions | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = chain.invoke({\"dish\": \"카레\"})\n",
    "print(type(recipe))\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with_structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
    "    steps: list[str] = Field(description=\"steps to make the dish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model.with_structured_output(Recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = chain.invoke({\"dish\": \"카레\"})\n",
    "print(type(recipe))\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

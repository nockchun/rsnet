{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-06T04:13:18.268434Z",
     "iopub.status.idle": "2025-08-06T04:13:18.268538Z",
     "shell.execute_reply": "2025-08-06T04:13:18.268490Z",
     "shell.execute_reply.started": "2025-08-06T04:13:18.268484Z"
    },
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:07:57.989520Z",
     "iopub.status.busy": "2025-08-06T06:07:57.989350Z",
     "iopub.status.idle": "2025-08-06T06:08:04.528058Z",
     "shell.execute_reply": "2025-08-06T06:08:04.527744Z",
     "shell.execute_reply.started": "2025-08-06T06:07:57.989508Z"
    },
    "id": "1oQenpTCPMyh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-06 15:08:02 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:04.528540Z",
     "iopub.status.busy": "2025-08-06T06:08:04.528453Z",
     "iopub.status.idle": "2025-08-06T06:08:17.497697Z",
     "shell.execute_reply": "2025-08-06T06:08:17.497319Z",
     "shell.execute_reply.started": "2025-08-06T06:08:04.528531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.8: Fast Gemma3 patching. Transformers: 4.54.0. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 2. Max memory: 23.494 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-4b-it\",\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.498136Z",
     "iopub.status.busy": "2025-08-06T06:08:17.498035Z",
     "iopub.status.idle": "2025-08-06T06:08:17.501243Z",
     "shell.execute_reply": "2025-08-06T06:08:17.500899Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.498126Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.501859Z",
     "iopub.status.busy": "2025-08-06T06:08:17.501758Z",
     "iopub.status.idle": "2025-08-06T06:08:17.612956Z",
     "shell.execute_reply": "2025-08-06T06:08:17.612541Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.501850Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Any, ClassVar\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.613307Z",
     "iopub.status.busy": "2025-08-06T06:08:17.613223Z",
     "iopub.status.idle": "2025-08-06T06:08:17.617771Z",
     "shell.execute_reply": "2025-08-06T06:08:17.617485Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.613298Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[Any]) -> str:\n",
    "        prompt = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _generate(self, messages: List[Any], **kwargs) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=kwargs.get(\"temperature\", 0.7),\n",
    "                top_p=kwargs.get(\"top_p\", 0.9),\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = decoded.split(\"<|assistant|>\\n\")[-1].strip()\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.618104Z",
     "iopub.status.busy": "2025-08-06T06:08:17.618010Z",
     "iopub.status.idle": "2025-08-06T06:08:17.642515Z",
     "shell.execute_reply": "2025-08-06T06:08:17.642226Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.618094Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.642880Z",
     "iopub.status.busy": "2025-08-06T06:08:17.642795Z",
     "iopub.status.idle": "2025-08-06T06:08:42.266438Z",
     "shell.execute_reply": "2025-08-06T06:08:42.266075Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.642871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 LLM(Large Language Model)을 활용한 애플리케이션 개발을 위한 프레임워크입니다. 간단히 말해, LLM을 더 쉽게 사용할 수 있도록 도와주는 도구 모음이라고 생각하면 됩니다. \n",
      "\n",
      "좀 더 자세히 설명하자면, LangChain은 다음과 같은 주요 기능들을 제공합니다.\n",
      "\n",
      "**1. 핵심 구성 요소:**\n",
      "\n",
      "*   **모델 (Models):** OpenAI, Google, Hugging Face 등 다양한 LLM과 쉽게 연동할 수 있도록 지원합니다.\n",
      "*   **프롬프트 (Prompts):** LLM에 제공할 프롬프트를 효과적으로 구성하고 관리하는 데 도움을 줍니다.\n",
      "*   **체인 (Chains):** 여러 컴포넌트를 연결하여 복잡한 작업을 수행할 수 있도록 합니다. 예를 들어, 프롬프트, 데이터베이스 검색, LLM 호출 등의 단계를 연결하여 하나의 작업 흐름을 만들 수 있습니다.\n",
      "*   **인덱스 (Indexes):** LLM이 접근할 수 있는 외부 데이터를 저장하고 검색하는 데 사용됩니다. 벡터 데이터베이스, 문서 저장소 등을 활용하여 LLM이 컨텍스트를 이해하고 답변을 생성하는 데 필요한 정보를 제공합니다.\n",
      "*   **에이전트 (Agents):** LLM이 주어진 작업에 따라 어떤 도구를 사용해야 할지 스스로 결정하고 실행할 수 있도록 합니다. 예를 들어, 검색 엔진, 계산기, 데이터베이스 등 다양한 도구를 활용하여 복잡한 작업을 수행할 수 있습니다.\n",
      "*   **메모리 (Memory):** LLM이 이전 대화 내용을 기억하고 활용할 수 있도록 합니다. 이를 통해 LLM은 이전 대화의 맥락을 이해하고 일관성 있는 답변을 제공할 수 있습니다.\n",
      "\n",
      "**2. LangChain의 특징:**\n",
      "\n",
      "*   **모듈성:** LangChain은 다양한 컴포넌트를 조합하여 원하는 애플리케이션을 구축할 수 있도록 모듈화되어 있습니다.\n",
      "*   **확장성:** 사용자는 LangChain의 기능을 확장하거나 새로운 컴포넌트를 추가하여 자신만의 애플리케이션을 만들 수 있습니다.\n",
      "*   **커뮤니티:** 활발한 커뮤니티를 통해 다양한 예제, 튜토리얼, 도구를 공유하고 있습니다.\n",
      "\n",
      "**3. LangChain을 사용하면 어떤 것을 만들 수 있나요?**\n",
      "\n",
      "*   **챗봇:** 사용자 질문에 답변하는 챗봇을 만들 수\n"
     ]
    }
   ],
   "source": [
    "result = chat_model.invoke([\n",
    "    SystemMessage(content=\"너는 친절하고 머신러닝 기술을 잘아는 전문가 AI야.\"),\n",
    "    HumanMessage(content=\"LangChain은 무엇인가요?\"),\n",
    "])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat History In Memory 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:42.266880Z",
     "iopub.status.busy": "2025-08-06T06:08:42.266744Z",
     "iopub.status.idle": "2025-08-06T06:08:42.286128Z",
     "shell.execute_reply": "2025-08-06T06:08:42.285818Z",
     "shell.execute_reply.started": "2025-08-06T06:08:42.266870Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:35.162293Z",
     "iopub.status.busy": "2025-08-06T06:09:35.162082Z",
     "iopub.status.idle": "2025-08-06T06:09:35.164424Z",
     "shell.execute_reply": "2025-08-06T06:09:35.164100Z",
     "shell.execute_reply.started": "2025-08-06T06:09:35.162278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prompt 템플릿 정의 : GemmaChatModel이 원하는 형태로 변경하기 위해.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:35.520477Z",
     "iopub.status.busy": "2025-08-06T06:09:35.520295Z",
     "iopub.status.idle": "2025-08-06T06:09:35.522476Z",
     "shell.execute_reply": "2025-08-06T06:09:35.522171Z",
     "shell.execute_reply.started": "2025-08-06T06:09:35.520463Z"
    }
   },
   "outputs": [],
   "source": [
    "# 세션별 history 저장 함수\n",
    "store = {}\n",
    "def get_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:35.998866Z",
     "iopub.status.busy": "2025-08-06T06:09:35.998592Z",
     "iopub.status.idle": "2025-08-06T06:09:36.001027Z",
     "shell.execute_reply": "2025-08-06T06:09:36.000734Z",
     "shell.execute_reply.started": "2025-08-06T06:09:35.998853Z"
    }
   },
   "outputs": [],
   "source": [
    "# RunnableWithMessageHistory 구성\n",
    "runnable = RunnableWithMessageHistory(\n",
    "    prompt | chat_model,  # chat_model은 GemmaChatModel 인스턴스\n",
    "    get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session별 Chat History In Memory 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user1 session 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:44.492926Z",
     "iopub.status.busy": "2025-08-06T06:09:44.492734Z",
     "iopub.status.idle": "2025-08-06T06:09:44.494639Z",
     "shell.execute_reply": "2025-08-06T06:09:44.494359Z",
     "shell.execute_reply.started": "2025-08-06T06:09:44.492914Z"
    }
   },
   "outputs": [],
   "source": [
    "# 세션 아이디 설정\n",
    "session_id = \"user1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:44.804744Z",
     "iopub.status.busy": "2025-08-06T06:09:44.804524Z",
     "iopub.status.idle": "2025-08-06T06:09:46.061136Z",
     "shell.execute_reply": "2025-08-06T06:09:46.060817Z",
     "shell.execute_reply.started": "2025-08-06T06:09:44.804732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: 안녕하세요, 민수 씨! 만나서 반갑습니다. 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "# 4. 대화 실행\n",
    "response1 = runnable.invoke(\n",
    "    {\"input\": \"나는 민수라고 해.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Response 1:\", response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:46.061638Z",
     "iopub.status.busy": "2025-08-06T06:09:46.061531Z",
     "iopub.status.idle": "2025-08-06T06:09:47.778876Z",
     "shell.execute_reply": "2025-08-06T06:09:47.778514Z",
     "shell.execute_reply.started": "2025-08-06T06:09:46.061627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 2: 네, 민수 씨라고 말씀하신 것을 기억하고 있습니다. 제가 기억력을 테스트하는 데 도움이 될 만한 질문을 할까요? 아니면 다른 질문이 있으신가요?\n"
     ]
    }
   ],
   "source": [
    "response2 = runnable.invoke(\n",
    "    {\"input\": \"내 이름 기억해?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Response 2:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:47.779275Z",
     "iopub.status.busy": "2025-08-06T06:09:47.779170Z",
     "iopub.status.idle": "2025-08-06T06:09:47.781226Z",
     "shell.execute_reply": "2025-08-06T06:09:47.780950Z",
     "shell.execute_reply.started": "2025-08-06T06:09:47.779262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conversation History ===\n",
      "HUMAN: 나는 민수라고 해.\n",
      "AI: 안녕하세요, 민수 씨! 만나서 반갑습니다. 무엇을 도와드릴까요?\n",
      "HUMAN: 내 이름 기억해?\n",
      "AI: 네, 민수 씨라고 말씀하신 것을 기억하고 있습니다. 제가 기억력을 테스트하는 데 도움이 될 만한 질문을 할까요? 아니면 다른 질문이 있으신가요?\n"
     ]
    }
   ],
   "source": [
    "# 5. History 출력 (검증)\n",
    "history = get_history(session_id)\n",
    "print(\"\\n=== Conversation History ===\")\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user2 session 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:47.781803Z",
     "iopub.status.busy": "2025-08-06T06:09:47.781689Z",
     "iopub.status.idle": "2025-08-06T06:09:47.812053Z",
     "shell.execute_reply": "2025-08-06T06:09:47.811723Z",
     "shell.execute_reply.started": "2025-08-06T06:09:47.781793Z"
    }
   },
   "outputs": [],
   "source": [
    "# 세션 아이디 설정\n",
    "session_id = \"user2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:47.812435Z",
     "iopub.status.busy": "2025-08-06T06:09:47.812332Z",
     "iopub.status.idle": "2025-08-06T06:09:50.329548Z",
     "shell.execute_reply": "2025-08-06T06:09:50.329182Z",
     "shell.execute_reply.started": "2025-08-06T06:09:47.812426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 2: 네, 철수 씨라고 기억하고 있어요. 혹시 다른 이름으로 불러드릴까요? 아니면 철수 씨에 대해 이야기하고 싶으신가요?\n"
     ]
    }
   ],
   "source": [
    "response1 = runnable.invoke(\n",
    "    {\"input\": \"나는 철수라고 해.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "response2 = runnable.invoke(\n",
    "    {\"input\": \"내 이름 기억해?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Response 2:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:50.329941Z",
     "iopub.status.busy": "2025-08-06T06:09:50.329840Z",
     "iopub.status.idle": "2025-08-06T06:09:50.331876Z",
     "shell.execute_reply": "2025-08-06T06:09:50.331600Z",
     "shell.execute_reply.started": "2025-08-06T06:09:50.329932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conversation History ===\n",
      "HUMAN: 나는 철수라고 해.\n",
      "AI: 안녕하세요, 철수 씨! 만나서 반가워요. 무엇을 도와드릴까요?\n",
      "HUMAN: 내 이름 기억해?\n",
      "AI: 네, 철수 씨라고 기억하고 있어요. 혹시 다른 이름으로 불러드릴까요? 아니면 철수 씨에 대해 이야기하고 싶으신가요?\n"
     ]
    }
   ],
   "source": [
    "# History 출력 (검증)\n",
    "history = get_history(session_id)\n",
    "print(\"\\n=== Conversation History ===\")\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user1 session에 추가 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:50.332188Z",
     "iopub.status.busy": "2025-08-06T06:09:50.332103Z",
     "iopub.status.idle": "2025-08-06T06:09:50.361292Z",
     "shell.execute_reply": "2025-08-06T06:09:50.360903Z",
     "shell.execute_reply.started": "2025-08-06T06:09:50.332180Z"
    }
   },
   "outputs": [],
   "source": [
    "# 세션 아이디 설정\n",
    "session_id = \"user1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:50.361688Z",
     "iopub.status.busy": "2025-08-06T06:09:50.361590Z",
     "iopub.status.idle": "2025-08-06T06:10:12.105235Z",
     "shell.execute_reply": "2025-08-06T06:10:12.104903Z",
     "shell.execute_reply.started": "2025-08-06T06:09:50.361679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 2: 정직이라는 가치를 중요하게 생각하시는군요. 민수 씨는 성실하고 신뢰할 수\n"
     ]
    }
   ],
   "source": [
    "response2 = runnable.invoke(\n",
    "    {\"input\": \"내 이름 기억해?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Response 2:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:10:12.105625Z",
     "iopub.status.busy": "2025-08-06T06:10:12.105528Z",
     "iopub.status.idle": "2025-08-06T06:10:12.107561Z",
     "shell.execute_reply": "2025-08-06T06:10:12.107334Z",
     "shell.execute_reply.started": "2025-08-06T06:10:12.105615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conversation History ===\n",
      "HUMAN: 나는 민수라고 해.\n",
      "AI: 안녕하세요, 민수 씨! 만나서 반갑습니다. 무엇을 도와드릴까요?\n",
      "HUMAN: 내 이름 기억해?\n",
      "AI: 네, 민수 씨라고 말씀하신 것을 기억하고 있습니다. 제가 기억력을 테스트하는 데 도움이 될 만한 질문을 할까요? 아니면 다른 질문이 있으신가요?\n",
      "HUMAN: 내 이름 기억해?\n",
      "AI: 정직이라는 가치를 중요하게 생각하시는군요. 민수 씨는 성실하고 신뢰할 수\n"
     ]
    }
   ],
   "source": [
    "# History 출력 (검증)\n",
    "history = get_history(session_id)\n",
    "print(\"\\n=== Conversation History ===\")\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

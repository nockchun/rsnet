{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# ê¸°ë³¸í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-06T04:13:18.268434Z",
     "iopub.status.idle": "2025-08-06T04:13:18.268538Z",
     "shell.execute_reply": "2025-08-06T04:13:18.268490Z",
     "shell.execute_reply.started": "2025-08-06T04:13:18.268484Z"
    },
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# ëª¨ë¸ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:07:57.989520Z",
     "iopub.status.busy": "2025-08-06T06:07:57.989350Z",
     "iopub.status.idle": "2025-08-06T06:08:04.528058Z",
     "shell.execute_reply": "2025-08-06T06:08:04.527744Z",
     "shell.execute_reply.started": "2025-08-06T06:07:57.989508Z"
    },
    "id": "1oQenpTCPMyh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-06 15:08:02 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:04.528540Z",
     "iopub.status.busy": "2025-08-06T06:08:04.528453Z",
     "iopub.status.idle": "2025-08-06T06:08:17.497697Z",
     "shell.execute_reply": "2025-08-06T06:08:17.497319Z",
     "shell.execute_reply.started": "2025-08-06T06:08:04.528531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.8: Fast Gemma3 patching. Transformers: 4.54.0. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 2. Max memory: 23.494 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-4b-it\",\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.498136Z",
     "iopub.status.busy": "2025-08-06T06:08:17.498035Z",
     "iopub.status.idle": "2025-08-06T06:08:17.501243Z",
     "shell.execute_reply": "2025-08-06T06:08:17.500899Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.498126Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.501859Z",
     "iopub.status.busy": "2025-08-06T06:08:17.501758Z",
     "iopub.status.idle": "2025-08-06T06:08:17.612956Z",
     "shell.execute_reply": "2025-08-06T06:08:17.612541Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.501850Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Any, ClassVar\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.613307Z",
     "iopub.status.busy": "2025-08-06T06:08:17.613223Z",
     "iopub.status.idle": "2025-08-06T06:08:17.617771Z",
     "shell.execute_reply": "2025-08-06T06:08:17.617485Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.613298Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[Any]) -> str:\n",
    "        prompt = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _generate(self, messages: List[Any], **kwargs) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=kwargs.get(\"temperature\", 0.7),\n",
    "                top_p=kwargs.get(\"top_p\", 0.9),\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = decoded.split(\"<|assistant|>\\n\")[-1].strip()\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.618104Z",
     "iopub.status.busy": "2025-08-06T06:08:17.618010Z",
     "iopub.status.idle": "2025-08-06T06:08:17.642515Z",
     "shell.execute_reply": "2025-08-06T06:08:17.642226Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.618094Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:17.642880Z",
     "iopub.status.busy": "2025-08-06T06:08:17.642795Z",
     "iopub.status.idle": "2025-08-06T06:08:42.266438Z",
     "shell.execute_reply": "2025-08-06T06:08:42.266075Z",
     "shell.execute_reply.started": "2025-08-06T06:08:17.642871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ LLM(Large Language Model)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê°„ë‹¨íˆ ë§í•´, LLMì„ ë” ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë„êµ¬ ëª¨ìŒì´ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤. \n",
      "\n",
      "ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•˜ìë©´, LangChainì€ ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê¸°ëŠ¥ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "**1. í•µì‹¬ êµ¬ì„± ìš”ì†Œ:**\n",
      "\n",
      "*   **ëª¨ë¸ (Models):** OpenAI, Google, Hugging Face ë“± ë‹¤ì–‘í•œ LLMê³¼ ì‰½ê²Œ ì—°ë™í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "*   **í”„ë¡¬í”„íŠ¸ (Prompts):** LLMì— ì œê³µí•  í”„ë¡¬í”„íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ êµ¬ì„±í•˜ê³  ê´€ë¦¬í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n",
      "*   **ì²´ì¸ (Chains):** ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í”„ë¡¬í”„íŠ¸, ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰, LLM í˜¸ì¶œ ë“±ì˜ ë‹¨ê³„ë¥¼ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ ì‘ì—… íë¦„ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "*   **ì¸ë±ìŠ¤ (Indexes):** LLMì´ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ì™¸ë¶€ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤, ë¬¸ì„œ ì €ì¥ì†Œ ë“±ì„ í™œìš©í•˜ì—¬ LLMì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "*   **ì—ì´ì „íŠ¸ (Agents):** LLMì´ ì£¼ì–´ì§„ ì‘ì—…ì— ë”°ë¼ ì–´ë–¤ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í• ì§€ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê²€ìƒ‰ ì—”ì§„, ê³„ì‚°ê¸°, ë°ì´í„°ë² ì´ìŠ¤ ë“± ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "*   **ë©”ëª¨ë¦¬ (Memory):** LLMì´ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLMì€ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•˜ê³  ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**2. LangChainì˜ íŠ¹ì§•:**\n",
      "\n",
      "*   **ëª¨ë“ˆì„±:** LangChainì€ ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¡°í•©í•˜ì—¬ ì›í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡ ëª¨ë“ˆí™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
      "*   **í™•ì¥ì„±:** ì‚¬ìš©ìëŠ” LangChainì˜ ê¸°ëŠ¥ì„ í™•ì¥í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ìì‹ ë§Œì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "*   **ì»¤ë®¤ë‹ˆí‹°:** í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì˜ˆì œ, íŠœí† ë¦¬ì–¼, ë„êµ¬ë¥¼ ê³µìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**3. LangChainì„ ì‚¬ìš©í•˜ë©´ ì–´ë–¤ ê²ƒì„ ë§Œë“¤ ìˆ˜ ìˆë‚˜ìš”?**\n",
      "\n",
      "*   **ì±—ë´‡:** ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì±—ë´‡ì„ ë§Œë“¤ ìˆ˜\n"
     ]
    }
   ],
   "source": [
    "result = chat_model.invoke([\n",
    "    SystemMessage(content=\"ë„ˆëŠ” ì¹œì ˆí•˜ê³  ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì„ ì˜ì•„ëŠ” ì „ë¬¸ê°€ AIì•¼.\"),\n",
    "    HumanMessage(content=\"LangChainì€ ë¬´ì—‡ì¸ê°€ìš”?\"),\n",
    "])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat History In Memory êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:08:42.266880Z",
     "iopub.status.busy": "2025-08-06T06:08:42.266744Z",
     "iopub.status.idle": "2025-08-06T06:08:42.286128Z",
     "shell.execute_reply": "2025-08-06T06:08:42.285818Z",
     "shell.execute_reply.started": "2025-08-06T06:08:42.266870Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:35.162293Z",
     "iopub.status.busy": "2025-08-06T06:09:35.162082Z",
     "iopub.status.idle": "2025-08-06T06:09:35.164424Z",
     "shell.execute_reply": "2025-08-06T06:09:35.164100Z",
     "shell.execute_reply.started": "2025-08-06T06:09:35.162278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prompt í…œí”Œë¦¿ ì •ì˜ : GemmaChatModelì´ ì›í•˜ëŠ” í˜•íƒœë¡œ ë³€ê²½í•˜ê¸° ìœ„í•´.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:35.520477Z",
     "iopub.status.busy": "2025-08-06T06:09:35.520295Z",
     "iopub.status.idle": "2025-08-06T06:09:35.522476Z",
     "shell.execute_reply": "2025-08-06T06:09:35.522171Z",
     "shell.execute_reply.started": "2025-08-06T06:09:35.520463Z"
    }
   },
   "outputs": [],
   "source": [
    "# ì„¸ì…˜ë³„ history ì €ì¥ í•¨ìˆ˜\n",
    "store = {}\n",
    "def get_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:35.998866Z",
     "iopub.status.busy": "2025-08-06T06:09:35.998592Z",
     "iopub.status.idle": "2025-08-06T06:09:36.001027Z",
     "shell.execute_reply": "2025-08-06T06:09:36.000734Z",
     "shell.execute_reply.started": "2025-08-06T06:09:35.998853Z"
    }
   },
   "outputs": [],
   "source": [
    "# RunnableWithMessageHistory êµ¬ì„±\n",
    "runnable = RunnableWithMessageHistory(\n",
    "    prompt | chat_model,  # chat_modelì€ GemmaChatModel ì¸ìŠ¤í„´ìŠ¤\n",
    "    get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sessionë³„ Chat History In Memory ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user1 session í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:44.492926Z",
     "iopub.status.busy": "2025-08-06T06:09:44.492734Z",
     "iopub.status.idle": "2025-08-06T06:09:44.494639Z",
     "shell.execute_reply": "2025-08-06T06:09:44.494359Z",
     "shell.execute_reply.started": "2025-08-06T06:09:44.492914Z"
    }
   },
   "outputs": [],
   "source": [
    "# ì„¸ì…˜ ì•„ì´ë”” ì„¤ì •\n",
    "session_id = \"user1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:44.804744Z",
     "iopub.status.busy": "2025-08-06T06:09:44.804524Z",
     "iopub.status.idle": "2025-08-06T06:09:46.061136Z",
     "shell.execute_reply": "2025-08-06T06:09:46.060817Z",
     "shell.execute_reply.started": "2025-08-06T06:09:44.804732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: ì•ˆë…•í•˜ì„¸ìš”, ë¯¼ìˆ˜ ì”¨! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n"
     ]
    }
   ],
   "source": [
    "# 4. ëŒ€í™” ì‹¤í–‰\n",
    "response1 = runnable.invoke(\n",
    "    {\"input\": \"ë‚˜ëŠ” ë¯¼ìˆ˜ë¼ê³  í•´.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Response 1:\", response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:46.061638Z",
     "iopub.status.busy": "2025-08-06T06:09:46.061531Z",
     "iopub.status.idle": "2025-08-06T06:09:47.778876Z",
     "shell.execute_reply": "2025-08-06T06:09:47.778514Z",
     "shell.execute_reply.started": "2025-08-06T06:09:46.061627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 2: ë„¤, ë¯¼ìˆ˜ ì”¨ë¼ê³  ë§ì”€í•˜ì‹  ê²ƒì„ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì œê°€ ê¸°ì–µë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë° ë„ì›€ì´ ë  ë§Œí•œ ì§ˆë¬¸ì„ í• ê¹Œìš”? ì•„ë‹ˆë©´ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "response2 = runnable.invoke(\n",
    "    {\"input\": \"ë‚´ ì´ë¦„ ê¸°ì–µí•´?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Response 2:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:47.779275Z",
     "iopub.status.busy": "2025-08-06T06:09:47.779170Z",
     "iopub.status.idle": "2025-08-06T06:09:47.781226Z",
     "shell.execute_reply": "2025-08-06T06:09:47.780950Z",
     "shell.execute_reply.started": "2025-08-06T06:09:47.779262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conversation History ===\n",
      "HUMAN: ë‚˜ëŠ” ë¯¼ìˆ˜ë¼ê³  í•´.\n",
      "AI: ì•ˆë…•í•˜ì„¸ìš”, ë¯¼ìˆ˜ ì”¨! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "HUMAN: ë‚´ ì´ë¦„ ê¸°ì–µí•´?\n",
      "AI: ë„¤, ë¯¼ìˆ˜ ì”¨ë¼ê³  ë§ì”€í•˜ì‹  ê²ƒì„ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì œê°€ ê¸°ì–µë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë° ë„ì›€ì´ ë  ë§Œí•œ ì§ˆë¬¸ì„ í• ê¹Œìš”? ì•„ë‹ˆë©´ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "# 5. History ì¶œë ¥ (ê²€ì¦)\n",
    "history = get_history(session_id)\n",
    "print(\"\\n=== Conversation History ===\")\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user2 session í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:47.781803Z",
     "iopub.status.busy": "2025-08-06T06:09:47.781689Z",
     "iopub.status.idle": "2025-08-06T06:09:47.812053Z",
     "shell.execute_reply": "2025-08-06T06:09:47.811723Z",
     "shell.execute_reply.started": "2025-08-06T06:09:47.781793Z"
    }
   },
   "outputs": [],
   "source": [
    "# ì„¸ì…˜ ì•„ì´ë”” ì„¤ì •\n",
    "session_id = \"user2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:47.812435Z",
     "iopub.status.busy": "2025-08-06T06:09:47.812332Z",
     "iopub.status.idle": "2025-08-06T06:09:50.329548Z",
     "shell.execute_reply": "2025-08-06T06:09:50.329182Z",
     "shell.execute_reply.started": "2025-08-06T06:09:47.812426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 2: ë„¤, ì² ìˆ˜ ì”¨ë¼ê³  ê¸°ì–µí•˜ê³  ìˆì–´ìš”. í˜¹ì‹œ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ ë¶ˆëŸ¬ë“œë¦´ê¹Œìš”? ì•„ë‹ˆë©´ ì² ìˆ˜ ì”¨ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "response1 = runnable.invoke(\n",
    "    {\"input\": \"ë‚˜ëŠ” ì² ìˆ˜ë¼ê³  í•´.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "response2 = runnable.invoke(\n",
    "    {\"input\": \"ë‚´ ì´ë¦„ ê¸°ì–µí•´?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Response 2:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:50.329941Z",
     "iopub.status.busy": "2025-08-06T06:09:50.329840Z",
     "iopub.status.idle": "2025-08-06T06:09:50.331876Z",
     "shell.execute_reply": "2025-08-06T06:09:50.331600Z",
     "shell.execute_reply.started": "2025-08-06T06:09:50.329932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conversation History ===\n",
      "HUMAN: ë‚˜ëŠ” ì² ìˆ˜ë¼ê³  í•´.\n",
      "AI: ì•ˆë…•í•˜ì„¸ìš”, ì² ìˆ˜ ì”¨! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "HUMAN: ë‚´ ì´ë¦„ ê¸°ì–µí•´?\n",
      "AI: ë„¤, ì² ìˆ˜ ì”¨ë¼ê³  ê¸°ì–µí•˜ê³  ìˆì–´ìš”. í˜¹ì‹œ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ ë¶ˆëŸ¬ë“œë¦´ê¹Œìš”? ì•„ë‹ˆë©´ ì² ìˆ˜ ì”¨ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "# History ì¶œë ¥ (ê²€ì¦)\n",
    "history = get_history(session_id)\n",
    "print(\"\\n=== Conversation History ===\")\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user1 sessionì— ì¶”ê°€ ì§ˆë¬¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:50.332188Z",
     "iopub.status.busy": "2025-08-06T06:09:50.332103Z",
     "iopub.status.idle": "2025-08-06T06:09:50.361292Z",
     "shell.execute_reply": "2025-08-06T06:09:50.360903Z",
     "shell.execute_reply.started": "2025-08-06T06:09:50.332180Z"
    }
   },
   "outputs": [],
   "source": [
    "# ì„¸ì…˜ ì•„ì´ë”” ì„¤ì •\n",
    "session_id = \"user1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:09:50.361688Z",
     "iopub.status.busy": "2025-08-06T06:09:50.361590Z",
     "iopub.status.idle": "2025-08-06T06:10:12.105235Z",
     "shell.execute_reply": "2025-08-06T06:10:12.104903Z",
     "shell.execute_reply.started": "2025-08-06T06:09:50.361679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 2: ì •ì§ì´ë¼ëŠ” ê°€ì¹˜ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ì‹œëŠ”êµ°ìš”. ë¯¼ìˆ˜ ì”¨ëŠ” ì„±ì‹¤í•˜ê³  ì‹ ë¢°í•  ìˆ˜\n"
     ]
    }
   ],
   "source": [
    "response2 = runnable.invoke(\n",
    "    {\"input\": \"ë‚´ ì´ë¦„ ê¸°ì–µí•´?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Response 2:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T06:10:12.105625Z",
     "iopub.status.busy": "2025-08-06T06:10:12.105528Z",
     "iopub.status.idle": "2025-08-06T06:10:12.107561Z",
     "shell.execute_reply": "2025-08-06T06:10:12.107334Z",
     "shell.execute_reply.started": "2025-08-06T06:10:12.105615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conversation History ===\n",
      "HUMAN: ë‚˜ëŠ” ë¯¼ìˆ˜ë¼ê³  í•´.\n",
      "AI: ì•ˆë…•í•˜ì„¸ìš”, ë¯¼ìˆ˜ ì”¨! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "HUMAN: ë‚´ ì´ë¦„ ê¸°ì–µí•´?\n",
      "AI: ë„¤, ë¯¼ìˆ˜ ì”¨ë¼ê³  ë§ì”€í•˜ì‹  ê²ƒì„ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì œê°€ ê¸°ì–µë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë° ë„ì›€ì´ ë  ë§Œí•œ ì§ˆë¬¸ì„ í• ê¹Œìš”? ì•„ë‹ˆë©´ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”?\n",
      "HUMAN: ë‚´ ì´ë¦„ ê¸°ì–µí•´?\n",
      "AI: ì •ì§ì´ë¼ëŠ” ê°€ì¹˜ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ì‹œëŠ”êµ°ìš”. ë¯¼ìˆ˜ ì”¨ëŠ” ì„±ì‹¤í•˜ê³  ì‹ ë¢°í•  ìˆ˜\n"
     ]
    }
   ],
   "source": [
    "# History ì¶œë ¥ (ê²€ì¦)\n",
    "history = get_history(session_id)\n",
    "print(\"\\n=== Conversation History ===\")\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

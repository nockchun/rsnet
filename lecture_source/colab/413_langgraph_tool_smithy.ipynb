{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yLT1RFEeQgm"
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esrEO7yteQgm"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEBCNVjffATu"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oQenpTCPMyh"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU6jp1fqeQgn"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btydbHNpeQgn"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 1024*5, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    # device_map = {\"\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Dict\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[Any]) -> str:\n",
    "        prompt = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _generate(self, messages: List[Any], **kwargs) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                do_sample=kwargs.get(\"do_sample\", self.do_sample),\n",
    "                temperature=kwargs.get(\"temperature\", self.temperature),\n",
    "                top_p=kwargs.get(\"top_p\", self.top_p),\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = decoded.split(\"<|assistant|>\\n\")[-1].strip()\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 툴 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city: str) -> str:\n",
    "    return f\"{city}: 맑음, 25℃ (데모)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: float, b: float) -> float:\n",
    "    return float(a) + float(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS: Dict[str, Dict[str, Any]] = {\n",
    "    \"get_weather\": {\n",
    "        \"description\": \"도시의 현재 날씨를 조회\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"city\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"city\"],\n",
    "        },\n",
    "    },\n",
    "    \"add\": {\n",
    "        \"description\": \"두 수를 더한다\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"a\": {\"type\": \"number\"}, \"b\": {\"type\": \"number\"}},\n",
    "            \"required\": [\"a\", \"b\"],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_FUNCS = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"add\": add,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt & Chain 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tool chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## param : tool_names, tool_schema, input\n",
    "select_instruct = \"\"\"\\\n",
    "You are a tool router. Read the user's request and decide whether to call a tool.\n",
    "Return ONLY one JSON object and NOTHING ELSE (no code fences, no commentary).\n",
    "\n",
    "Strict output JSON (one object):\n",
    "{{\n",
    "  \"tool\": \"<one of: {tool_names} | none>\",\n",
    "  \"args\": <object>\n",
    "}}\n",
    "\n",
    "Global rules:\n",
    "- Use ONLY tools defined in the TOOL SCHEMA below. If nothing matches, set \"tool\" to \"none\" and \"args\" to {{}}.\n",
    "- Output must be valid JSON with double quotes and no trailing commas.\n",
    "- Only choose a tool if:\n",
    "  (a) the request clearly matches the tool’s description/purpose, AND\n",
    "  (b) you can supply ALL required parameters from the user input.\n",
    "- Do NOT invent, guess, or hallucinate parameter values. If a required value is missing/unclear, choose \"none\".\n",
    "- Conform exactly to the selected tool's parameter schema (names, types, enums). Do not add extra keys not in the schema.\n",
    "- If multiple tools could work, prefer the most specific one that best matches the user’s intent.\n",
    "- Keep numbers as numbers, booleans as booleans, arrays as arrays, strings as strings. Do not convert types arbitrarily.\n",
    "- Preserve user-provided text as-is (do not translate or rewrite); only extract values for \"args\".\n",
    "- Think silently; DO NOT include chain-of-thought or explanations in the output.\n",
    "\n",
    "TOOL SCHEMA (names, descriptions, and JSON parameter schemas):\n",
    "{tool_schema}\n",
    "\n",
    "Examples (for style only; do NOT copy literally):\n",
    "User: What's the weather in Paris?\n",
    "Output:\n",
    "{{\"tool\":\"get_weather\",\"args\":{{\"city\":\"Paris\"}}}}\n",
    "\n",
    "User: add 7.5 and 2\n",
    "Output:\n",
    "{{\"tool\":\"add\",\"args\":{{\"a\":7.5,\"b\":2}}}}\n",
    "\n",
    "User: Tell me a joke\n",
    "Output:\n",
    "{{\"tool\":\"none\",\"args\":{{}}}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", select_instruct),\n",
    "    (\"human\", \"Now produce the JSON for this user request:\\n{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser()  # {\"tool\": str, \"args\": dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_tool_chain = select_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instruct = \"\"\"\\\n",
    "You must answer concisely and accurately in korean.\n",
    "\n",
    "Tool result:\n",
    "{observation}\n",
    "\n",
    "Instructions:\n",
    "- If the Tool result is NON-EMPTY, produce ONE short paragraph grounded ONLY in that result. Do not contradict it. If it is incomplete or conflicting, state what is missing and answer only with what can be supported.\n",
    "- If the Tool result is EMPTY, answer directly from your knowledge. If you do not know, say \"I don't know.\" Do NOT invent or guess facts.\n",
    "- Do NOT include analysis, chain-of-thought, meta commentary, or mentions of tools/pipelines. Output only the final answer.\n",
    "- Keep it concise (about 1–5 sentences) unless the user explicitly requested another format.\n",
    "- If the user asked for a specific format (e.g., code or bullet points), follow it; otherwise use plain text.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", question_instruct),\n",
    "    (\"human\", \"User question:\\n{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_chain = question_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Smithy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![langgraph_example](res/langgraph_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상태 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateToolQA(TypedDict):\n",
    "    input: str\n",
    "    tool_schema: Dict[str, Any]\n",
    "    tool_names: str\n",
    "    selection: Dict[str, Any]\n",
    "    observation: Optional[Any]\n",
    "    answer: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_toolqa = StateToolQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tool select 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_tool_select(state: StateToolQA) -> StateToolQA:\n",
    "    print(f\"[+] node_tool_select\\n{state}\")\n",
    "    selection = select_tool_chain.invoke({\n",
    "        \"input\": state[\"input\"],\n",
    "        \"tool_schema\": state.get(\"tool_schema\", TOOLS),\n",
    "        \"tool_names\": state.get(\"tool_names\", \", \".join(TOOLS.keys())),\n",
    "    })\n",
    "    print(f\"[+] node_tool_select\\n{selection}\")\n",
    "    return {**state, \"selection\": selection}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_toolqa = {**state_toolqa, \"input\": \"서울 날씨 어때?\", \"tool_schema\": TOOLS, \"tool_names\": ', '.join(TOOLS.keys())}\n",
    "state_toolqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_toolqa = node_tool_select(state_toolqa)\n",
    "state_toolqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_toolqa = {**state_toolqa, \"input\": \"2와 3을 더한 결과는?\", \"tool_schema\": TOOLS, \"tool_names\": ', '.join(TOOLS.keys())}\n",
    "state_toolqa = node_tool_select(state_toolqa)\n",
    "state_toolqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_toolqa = {**state_toolqa, \"input\": \"좋은 회의 아이스브레이커 알려줘\", \"tool_schema\": TOOLS, \"tool_names\": ', '.join(TOOLS.keys())}\n",
    "state_toolqa = node_tool_select(state_toolqa)\n",
    "state_toolqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tool 실행 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_tool_call(state: StateToolQA) -> StateToolQA:\n",
    "    print(f\"[+] node_tool_call\\n{state}\")\n",
    "    selection = state.get(\"selection\", {}) or {}\n",
    "    tool = selection.get(\"tool\", \"none\")\n",
    "    args = selection.get(\"args\") or {}\n",
    "\n",
    "    if tool not in TOOL_FUNCS:\n",
    "        return {**state, \"observation\": None}\n",
    "\n",
    "    try:\n",
    "        result = TOOL_FUNCS[tool](**args)\n",
    "    except Exception as e:\n",
    "        result = f\"TOOL_ERROR: {e}\"\n",
    "    return {**state, \"observation\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_toolqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_toolqa = node_tool_call(state_toolqa)\n",
    "state_toolqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_question(state: StateToolQA) -> StateToolQA:\n",
    "    print(f\"[+] node_question\\n{state}\")\n",
    "    msg = question_chain.invoke({\n",
    "        \"input\": state[\"input\"],\n",
    "        \"observation\": state.get(\"observation\")\n",
    "    })\n",
    "    content = getattr(msg, \"content\", str(msg))\n",
    "    return {**state, \"answer\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_toolqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_toolqa = node_question(state_toolqa)\n",
    "state_toolqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

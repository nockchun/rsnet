{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"PyTorch version: {torch.__version__}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "- 텐서는 PyTorch에서 기본이 되는 데이터 구조로, NumPy 배열과 유사하지만 GPU 가속과 자동 미분 같은 추가 기능을 제공."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트에서 생성\n",
    "tensor = torch.tensor([1, 2, 3, 4])\n",
    "tensor_matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "print(f\"[+] Tensor from list: {tensor}, Type: {type(tensor)} {tensor.dtype}\")\n",
    "print(f\"[+] Matrix: {tensor_matrix}, Type: {type(tensor_matrix)} {tensor_matrix.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 지정하여 생성\n",
    "tensor_float = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "tensor_int = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
    "\n",
    "print(f\"[+] Float tensor: {tensor_float}, Type: {type(tensor_float)} {tensor_float.dtype}\")\n",
    "print(f\"[+] Integer tensor: {tensor_int}, Type: {type(tensor_int)} {tensor_int.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy에서 생성\n",
    "np_array = np.array([1, 2, 3, 4], dtype=np.float16)\n",
    "tensor_np = torch.from_numpy(np_array)\n",
    "np_recover = tensor_np.numpy()\n",
    "\n",
    "print(f\"[+] Tensor: {tensor_np}, Type: {type(tensor_np)} {tensor_np.dtype}\")\n",
    "print(f\"[+] Numpy Recover: {np_recover}, Type: {type(np_recover)} {np_recover.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리를 공유함\n",
    "tensor_np[0] = 10\n",
    "np_array[1] = 20\n",
    "np_recover[2] = 30\n",
    "print(f\"[+] Tensor: {tensor_np}, Type: {type(tensor_np)} {tensor_np.dtype}\")\n",
    "print(f\"[+] Numpy Original: {np_array}, Type: {type(np_array)} {np_array.dtype}\")\n",
    "print(f\"[+] Numpy Recover: {np_recover}, Type: {type(np_recover)} {np_recover.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape지정하여 무작위/상수 생성\n",
    "tensor_zeros = torch.zeros(3, 4, dtype=torch.int8)\n",
    "tensor_ones = torch.ones(2, 3)\n",
    "tensor_rand = torch.rand(2, 2)  # Uniform distribution [0, 1)\n",
    "tensor_randn = torch.randn(2, 2)  # Normal distribution (mean=0, std=1)\n",
    "\n",
    "print(f\"[+] Zeros tensor: {tensor_zeros}, Type: {type(tensor_zeros)} {tensor_zeros.dtype}\")\n",
    "print(f\"[+] Ones tensor: {tensor_ones}, Type: {type(tensor_ones)} {tensor_ones.dtype}\")\n",
    "print(f\"[+] Random uniform tensor: {tensor_rand}, Type: {type(tensor_rand)} {tensor_rand.dtype}\")\n",
    "print(f\"[+] Random normal tensor: {tensor_randn}, Type: {type(tensor_randn)} {tensor_randn.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 Tensor의 shape과 datatype을 유지하여 생성.\n",
    "tensor_zeros_like = torch.zeros_like(tensor_matrix)\n",
    "tensor_ones_like = torch.ones_like(tensor_zeros)\n",
    "tensor_rand_like = torch.rand_like(tensor_np)\n",
    "tensor_randn_like = torch.randn_like(tensor_np)\n",
    "\n",
    "print(f\"[+] Zeros tensor: {tensor_zeros_like}, Type: {type(tensor_zeros_like)} {tensor_zeros_like.dtype}\")\n",
    "print(f\"[+] Ones tensor: {tensor_ones_like}, Type: {type(tensor_ones_like)} {tensor_ones_like.dtype}\")\n",
    "print(f\"[+] Random uniform tensor: {tensor_rand_like}, Type: {type(tensor_rand_like)} {tensor_rand_like.dtype}\")\n",
    "print(f\"[+] Random normal tensor: {tensor_randn_like}, Type: {type(tensor_randn_like)} {tensor_randn_like.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 속성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cpu = torch.randn(3, 4, 5)\n",
    "tensor_gpu = torch.randn(3, 4, 5).to(device)\n",
    "\n",
    "print(f\"[+] Tensor shape: {tensor_cpu.shape}\")\n",
    "print(f\"[+] Tensor size: {tensor_cpu.size()}\")\n",
    "print(f\"[+] Number of dimensions: {tensor_cpu.dim()}\")\n",
    "print(f\"[+] Data type: {tensor_cpu.dtype}\")\n",
    "print(f\"[+] Device: {tensor_cpu.device}\")\n",
    "print(f\"[+] Device: {tensor_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing & Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "print(f\"x[0, 0] = {x[0, 0]}\")\n",
    "print(f\"x[1, 2] = {x[1, 2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[+] First column as row:\\n{x[:, 0]}\\n\")\n",
    "print(f\"[+] First column as column:\\n{x[:, 0:1]}\\n\")\n",
    "print(f\"[+] Second row:\\n{x[1, :]}\\n\")\n",
    "print(f\"[+] Sub-matrix (top-right 2x2):\\n{x[0:2, 1:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = x > 5\n",
    "print(f\"\\nBoolean mask (x > 5):\\n{mask}\")\n",
    "print(f\"Elements where x > 5:\\n{x[mask]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:45:57.495126Z",
     "iopub.status.busy": "2025-09-05T06:45:57.494810Z",
     "iopub.status.idle": "2025-09-05T06:45:57.498157Z",
     "shell.execute_reply": "2025-09-05T06:45:57.497544Z",
     "shell.execute_reply.started": "2025-09-05T06:45:57.495102Z"
    }
   },
   "source": [
    "## 산술연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "c = torch.tensor([1, 1, 1])\n",
    "\n",
    "print(\"[+] Addition\")\n",
    "print(f\"a + b + c = {a + b + c}\")\n",
    "print(f\"torch.add(a, b).add(c) = {torch.add(a, b).add(c)}\")\n",
    "\n",
    "print(\"\\n[+] Subtraction\")\n",
    "print(f\"a - b = {a - b}\")\n",
    "print(f\"torch.sub(a, b) = {torch.sub(a, b)}\")\n",
    "\n",
    "print(\"\\n[+] Element-wise Multiplication\")\n",
    "print(f\"a * b = {a * b}\")\n",
    "print(f\"torch.mul(a, b) = {torch.mul(a, b)}\")\n",
    "\n",
    "print(\"\\n[+] Element-wise Division\")\n",
    "print(f\"a / b = {a / b}\")\n",
    "print(f\"torch.div(a, b) = {torch.div(a, b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-place operations (modifies the tensor)\n",
    "d = torch.tensor([1, 2, 3])\n",
    "print(f\"[+] Original d = {d}\")\n",
    "\n",
    "d.add_(b)  # underscore suffix : in-place operations\n",
    "print(f\"[+] After d.add_(b), d = {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "print(f\"[+] Matrix a:\\n{a}\")\n",
    "print(f\"[+] Matrix b:\\n{b}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(f\"\\nMatrix multiplication (torch.matmul(a, b)):\\n{torch.matmul(a, b)}\")\n",
    "print(f\"Matrix multiplication (a @ b):\\n{a @ b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose\n",
    "print(f\"\\nTranspose of a:\\n{a.t()}\\n{a.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinant : 행렬식\n",
    "print(f\"Determinant of a: {torch.det(a.float())}\")\n",
    "\n",
    "# Inverse : 역행렬\n",
    "print(f\"Inverse of a:\\n{torch.inverse(a.float())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum\n",
    "print(f\"[+] Sum of all elements: {torch.sum(x)}\")\n",
    "print(f\"[+] Sum along rows (dim=0): {x.sum(dim=0)}\")\n",
    "print(f\"[+] Sum along columns (dim=1): {x.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max and Min\n",
    "print(f\"[+] Max of all elements: {torch.max(x)}\")\n",
    "max_values, max_indices = x.max(dim=0)\n",
    "print(f\"[+] Max along rows (dim=0): values={max_values}, indices={max_indices}\")\n",
    "print(f\"[+] Min of all elements: {torch.min(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product\n",
    "print(f\"[+] Product of all elements: {torch.prod(x)}\")\n",
    "print(f\"[+] Product along rows (dim=0): {x.prod(dim=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor directly on GPU\n",
    "x_gpu = torch.tensor([1, 2, 3], device=device)\n",
    "print(f\"[+] Tensor created on GPU: {x_gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move tensor from CPU to GPU\n",
    "x_cpu = torch.tensor([4, 5, 6])\n",
    "x_gpu = x_cpu.to(device)\n",
    "print(f\"[+] Tensor moved from CPU to GPU: {x_gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move tensor back to CPU\n",
    "x_cpu_again = x_gpu.cpu()\n",
    "print(f\"[+] Tensor moved back to CPU: {x_cpu_again}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors with requires_grad=True to track operations\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "print(f\"x = {x}, y = {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a computational graph\n",
    "z = x**2 + y**3\n",
    "print(f\"z = x^2 + y^3 = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "print(f\"Gradient of z with respect to x (dz/dx): {x.grad}\")\n",
    "print(f\"Gradient of z with respect to y (dz/dy): {y.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad 초기화 (파이토치는 grad를 누적합니다!)\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print(f\"y.grad = {y.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward()가 끝나면, x.grad와 y.grad에 각각 ∂z/∂x, ∂z/∂y가 \"누적\"됩니다.\n",
    "# 그리고 사용된 그래프는 기본 설정상 메모리에서 해제됩니다(free).\n",
    "z = x**2 + y**3\n",
    "z.backward()\n",
    "print(f\"After first backward pass:\")\n",
    "print(f\"x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward()가 끝나면, x.grad와 y.grad에 각각 ∂z/∂x, ∂z/∂y가 \"누적\"됩니다.\n",
    "# 그리고 사용된 그래프는 기본 설정상 메모리에서 해제됩니다(free).\n",
    "z = x**2 + y**3\n",
    "z.backward()\n",
    "print(f\"After first backward pass:\")\n",
    "print(f\"x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detach a tensor from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x.detach()\n",
    "print(f\"Detached tensor a = {a}\")\n",
    "print(f\"a.requires_grad = {a.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of x values\n",
    "x_range = torch.linspace(-3, 3, 100, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function: f(x) = x^2\n",
    "y = x_range**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients for each x value\n",
    "gradients = []\n",
    "for i in range(len(x_range)):\n",
    "    if x_range.grad is not None:\n",
    "        x_range.grad.zero_()\n",
    "    y_i = x_range[i]**2\n",
    "    y_i.backward()\n",
    "    gradients.append(x_range.grad[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy for plotting\n",
    "x_np = x_range.detach().numpy()\n",
    "y_np = y.detach().numpy()\n",
    "gradients_np = np.array(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the function and its gradient\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_np, y_np, 'b-', label='f(x) = x^2')\n",
    "plt.plot(x_np, gradients_np, 'r-', label=\"f'(x) = 2x\")\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function and its Gradient')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

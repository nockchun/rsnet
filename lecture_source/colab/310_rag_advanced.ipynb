{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yLT1RFEeQgm"
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esrEO7yteQgm"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEBCNVjffATu"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oQenpTCPMyh"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU6jp1fqeQgn"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btydbHNpeQgn"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 1024*5, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    # device_map = {\"\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPtcr98MeQgn"
   },
   "outputs": [],
   "source": [
    "model = FastModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Ej1_yjeQgn"
   },
   "source": [
    "# Custom ChatModel 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf4trrKVeQgn"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, Iterator, List, Literal, Optional, Type, Union\n",
    "from pydantic import BaseModel as PydanticBaseModel\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk, HumanMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult, ChatGenerationChunk\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from transformers import TextIteratorStreamer\n",
    "import threading\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9, verbose: bool = False, **kwargs: Any):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "        object.__setattr__(self, \"verbose\", verbose)\n",
    "        object.__setattr__(self, \"_gen_lock\", threading.Lock())\n",
    "\n",
    "    ### 공통 유틸 ###########################################################################\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[BaseMessage]) -> str:\n",
    "        conv: List[Dict[str, str]] = []\n",
    "        for m in messages:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                conv.append({\"role\": \"system\", \"content\": m.content})\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                conv.append({\"role\": \"user\", \"content\": m.content})\n",
    "            elif isinstance(m, AIMessage):\n",
    "                conv.append({\"role\": \"assistant\", \"content\": m.content})\n",
    "\n",
    "        formatted = self.tokenizer.apply_chat_template(\n",
    "            conv,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,  # 어시스턴트 턴 시작만 넣고 종료 토큰은 모델이 생성\n",
    "        )\n",
    "        return formatted\n",
    "\n",
    "    def _apply_stop(self, text: str, stop: Optional[List[str]]) -> str:\n",
    "        if not stop:\n",
    "            return text\n",
    "        cut = len(text)\n",
    "        for s in stop:\n",
    "            idx = text.find(s)\n",
    "            if idx != -1:\n",
    "                cut = min(cut, idx)\n",
    "        return text[:cut]\n",
    "\n",
    "    def _build_gen_kwargs(self, **kwargs: Any) -> Dict[str, Any]:\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        if pad_id is None:\n",
    "            pad_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        eot_id = None\n",
    "        try:\n",
    "            eot_id = self.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "            if isinstance(eot_id, list):\n",
    "                eot_id = None\n",
    "        except Exception:\n",
    "            eot_id = None\n",
    "\n",
    "        return {\n",
    "            \"max_new_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"do_sample\": kwargs.get(\"do_sample\", self.do_sample),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"top_p\": kwargs.get(\"top_p\", self.top_p),\n",
    "            \"eos_token_id\": eot_id or self.tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": pad_id,\n",
    "        }\n",
    "\n",
    "    ### Invoke ############################################################################\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        if getattr(self, \"verbose\", False):\n",
    "            print(\"\\n[GemmaChatModel/_generate] ==== FINAL PROMPT ====\")\n",
    "            print(prompt)\n",
    "            print(\"=================================================\\n\")\n",
    "\n",
    "        # [CHANGED] 기본 stop 시퀀스에 종료 마커 추가\n",
    "        if stop is None:\n",
    "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
    "\n",
    "        with self._gen_lock:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        # [CHANGED] 입력 길이 이후 생성 토큰만 디코딩\n",
    "        in_len = inputs[\"input_ids\"].shape[1]\n",
    "        gen_tokens = outputs[0][in_len:]\n",
    "        decoded = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        # [CHANGED] 후단 종료 마커 정리(안전망)\n",
    "        decoded = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", decoded)\n",
    "\n",
    "        decoded = self._apply_stop(decoded, stop)\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=decoded))])\n",
    "\n",
    "    ### Batch #############################################################################\n",
    "    def _generate_batch(self, messages_list: List[List[BaseMessage]], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> List[ChatResult]:\n",
    "        \"\"\"\n",
    "        여러 개의 대화를 한 번에 패딩 인코딩하여 generate 가속.\n",
    "        \"\"\"\n",
    "        # [CHANGED] 각 대화를 chat template로 포맷\n",
    "        prompts = [self._format_messages(msgs) for msgs in messages_list]\n",
    "\n",
    "        # padding=True, truncation=True 로 배치 인코딩\n",
    "        tokenized = self.tokenizer(\n",
    "            prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        tokenized = {k: v.to(self.model.device) for k, v in tokenized.items()}\n",
    "        attn = tokenized.get(\"attention_mask\", None)\n",
    "\n",
    "        gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
    "\n",
    "        # [CHANGED] 기본 stop 시퀀스\n",
    "        if stop is None:\n",
    "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**tokenized, **gen_kwargs)\n",
    "\n",
    "        results: List[ChatResult] = []\n",
    "        for i in range(len(prompts)):\n",
    "            # 각 샘플의 프롬프트 길이만큼 잘라서 신규 토큰만 디코딩\n",
    "            if attn is not None:\n",
    "                in_len = int(attn[i].sum().item())\n",
    "            else:\n",
    "                in_len = tokenized[\"input_ids\"][i].shape[0]\n",
    "\n",
    "            gen_tokens = outputs[i][in_len:]\n",
    "            text = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "            text = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", text)  # [CHANGED]\n",
    "            text = self._apply_stop(text, stop)\n",
    "\n",
    "            results.append(\n",
    "                ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    ### Stream ############################################################################\n",
    "    def _stream(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs: Any) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"\n",
    "        LangChain의 Runnable .stream() 에서 호출되는 내부 스트리밍 제너레이터.\n",
    "        각 토큰 델타를 ChatGenerationChunk(AIMessageChunk) 로 내보냅니다.\n",
    "        \"\"\"\n",
    "        prompt = self._format_messages(messages)\n",
    "        if getattr(self, \"verbose\", False):\n",
    "            print(\"\\n[GemmaChatModel/_stream] ==== FINAL PROMPT ====\")\n",
    "            print(prompt)\n",
    "            print(\"================================================\\n\")\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        gen_kwargs = self._build_gen_kwargs(**kwargs)\n",
    "\n",
    "        # transformers 스트리머 설정\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # [CHANGED] 기본 stop 시퀀스\n",
    "        if stop is None:\n",
    "            stop = [\"</s>\", \"<end_of_turn>\", \"<|endoftext|>\"]\n",
    "\n",
    "        # generate를 백그라운드에서 수행\n",
    "        def _worker():\n",
    "            with torch.no_grad():\n",
    "                self.model.generate(**inputs, **gen_kwargs, streamer=streamer)\n",
    "\n",
    "        th = threading.Thread(target=_worker, daemon=True)\n",
    "        th.start()\n",
    "\n",
    "        # 누적 후 stop 시퀀스까지 안전하게 잘라서 내보내기\n",
    "        buffer = \"\"\n",
    "        emitted = 0\n",
    "\n",
    "        for piece in streamer:\n",
    "            buffer += piece\n",
    "            # [CHANGED] 실시간으로 후단 종료 마커 제거(시각적 잔여물 방지)\n",
    "            tmp = re.sub(r\"(?:</s>|<\\|endoftext\\|>|<end_of_turn>)+\\s*$\", \"\", buffer)\n",
    "            trimmed = self._apply_stop(tmp, stop)\n",
    "\n",
    "            # 새로 생긴 구간만 델타로 방출\n",
    "            if len(trimmed) > emitted:\n",
    "                delta = trimmed[emitted:]\n",
    "                emitted = len(trimmed)\n",
    "                yield ChatGenerationChunk(message=AIMessageChunk(content=delta))\n",
    "\n",
    "            # stop 시퀀스 감지되면 중단\n",
    "            if stop and len(trimmed) < len(buffer):\n",
    "                break\n",
    "\n",
    "        # 스레드 정리(최대한 조용히 종료 대기)\n",
    "        th.join(timeout=0.1)\n",
    "\n",
    "    ### Structured output #################################################################\n",
    "    def _build_json_system_prompt(self, schema_text: str) -> str:\n",
    "        # 모델이 JSON만 내도록 강하게 지시 (hallucination 방지용 규칙 포함)\n",
    "        return (\n",
    "            \"You are a strict JSON generator.\\n\"\n",
    "            \"Return ONLY a single JSON object, no prose, no backticks, no explanations.\\n\"\n",
    "            \"Do not include trailing commas. Do not include comments.\\n\"\n",
    "            \"Conform exactly to the following JSON schema (fields, types, required):\\n\"\n",
    "            f\"{schema_text}\\n\"\n",
    "        )\n",
    "\n",
    "    def _ensure_pydantic(self):\n",
    "        if PydanticBaseModel is None:\n",
    "            raise RuntimeError(\n",
    "                \"Pydantic is not available. Install pydantic or pass a dict schema instead of a BaseModel.\"\n",
    "            )\n",
    "\n",
    "    def _schema_to_text(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]]) -> str:\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(\n",
    "            schema, PydanticBaseModel\n",
    "        ):\n",
    "            try:\n",
    "                json_schema = schema.model_json_schema()  # pydantic v2\n",
    "            except Exception:\n",
    "                json_schema = schema.schema()  # pydantic v1\n",
    "            return json.dumps(json_schema, ensure_ascii=False, indent=2)\n",
    "        elif isinstance(schema, dict):\n",
    "            return json.dumps(schema, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"schema must be a Pydantic BaseModel subclass or a dict JSON schema.\"\n",
    "            )\n",
    "\n",
    "    def _parse_structured(self, text: str, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], include_raw: bool):\n",
    "        # 코드블록 등 제거 시도(혹시 들어올 경우)\n",
    "        t = text.strip()\n",
    "        if t.startswith(\"```\"):\n",
    "            # ```json ... ``` 또는 ``` ... ```\n",
    "            t = t.strip(\"`\")\n",
    "            # 첫 줄에 json 명시가 들어있을 수 있음\n",
    "            t = \"\\n\".join(\n",
    "                line for line in t.splitlines() if not line.lower().startswith(\"json\")\n",
    "            )\n",
    "        # JSON 파싱\n",
    "        obj = json.loads(t)\n",
    "\n",
    "        # Pydantic 검증\n",
    "        if PydanticBaseModel is not None and isinstance(schema, type) and issubclass(\n",
    "            schema, PydanticBaseModel\n",
    "        ):\n",
    "            validated = (\n",
    "                schema.model_validate(obj)\n",
    "                if hasattr(schema, \"model_validate\")\n",
    "                else schema.parse_obj(obj)\n",
    "            )\n",
    "            return {\"parsed\": validated, \"raw\": text} if include_raw else validated\n",
    "        else:\n",
    "            # dict 스키마는 별도 검증 없이 반환 (원하면 jsonschema로 검증 가능)\n",
    "            return {\"parsed\": obj, \"raw\": text} if include_raw else obj\n",
    "\n",
    "    def with_structured_output(self, schema: Union[Type[\"PydanticBaseModel\"], Dict[str, Any]], *, method: Literal[\"json_mode\"] = \"json_mode\", include_raw: bool = False, system_prefix: Optional[str] = None, deterministic: bool = True):\n",
    "        schema_text = self._schema_to_text(schema)\n",
    "        sys_prompt = self._build_json_system_prompt(schema_text)\n",
    "        if system_prefix:\n",
    "            sys_prompt = system_prefix.rstrip() + \"\\n\\n\" + sys_prompt\n",
    "\n",
    "        def _invoke(messages_or_any):\n",
    "            # 입력을 메시지 리스트로 정규화\n",
    "            if isinstance(messages_or_any, list) and all(\n",
    "                isinstance(m, BaseMessage) for m in messages_or_any\n",
    "            ):\n",
    "                msgs = [SystemMessage(content=sys_prompt)] + messages_or_any\n",
    "            else:\n",
    "                # 문자열/딕셔너리 등도 처리\n",
    "                msgs = [\n",
    "                    SystemMessage(content=sys_prompt),\n",
    "                    HumanMessage(content=str(messages_or_any)),\n",
    "                ]\n",
    "\n",
    "            # 결정론 옵션\n",
    "            kw = {}\n",
    "            if deterministic:\n",
    "                kw = {\"do_sample\": False, \"temperature\": 0.0, \"top_p\": 1.0}\n",
    "\n",
    "            # 1차 시도\n",
    "            result = self._generate(msgs, **kw)\n",
    "            text = result.generations[0].message.content\n",
    "            try:\n",
    "                return self._parse_structured(text, schema, include_raw)\n",
    "            except Exception:\n",
    "                # 재시도: 더 강한 지시\n",
    "                retry_msgs = [\n",
    "                    SystemMessage(\n",
    "                        content=sys_prompt + \"\\nOutput must be valid JSON. Try again.\"\n",
    "                    )\n",
    "                ] + msgs[1:]\n",
    "                result2 = self._generate(retry_msgs, **kw)\n",
    "                text2 = result2.generations[0].message.content\n",
    "                return self._parse_structured(text2, schema, include_raw)\n",
    "\n",
    "        # Runnable 로 래핑해서 반환 (체인 파이프에 바로 사용 가능)\n",
    "        return RunnableLambda(_invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4L37Lhf-eQgn"
   },
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documments Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector DB - Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt, Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "다음 문맥만을 고려해 질문에 답하세요.\n",
    "\n",
    "문맥: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "질문: {question}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke(\"LangChain의 개요를 알려줘\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDE (Hypothetical Document Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "다음 질문에 한 문장으로 답하세요.\n",
    "\n",
    "질문: {question}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_chain = hypothetical_prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_rag_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": hypothetical_chain | retriever,\n",
    "} | prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = hyde_rag_chain.invoke(\"LangChain의 개요를 알려줘\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# 기본환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oQenpTCPMyh"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    device_map = {\"\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = FastModel.from_pretrained(\n",
    "#     model_name=\"unsloth/gemma-3-4b-it\",\n",
    "#     max_seq_length=1024*5, # Choose any for long context!\n",
    "#     load_in_4bit=True,  # 4 bit quantization to reduce memory\n",
    "#     load_in_8bit=False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "#     device_map = {\"\": device}  # ← 여기서 GPU 2번 지정\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 생성기 (한국어 포함 모델)\n",
    "MODEL_EMBED = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" # intfloat/multilingual-e5-base\n",
    "embedding = HuggingFaceEmbeddings(model_name=MODEL_EMBED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ChatModel 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, ClassVar\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[Any]) -> str:\n",
    "        prompt = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _generate(self, messages: List[Any], **kwargs) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                do_sample=kwargs.get(\"do_sample\", self.do_sample),\n",
    "                temperature=kwargs.get(\"temperature\", self.temperature),\n",
    "                top_p=kwargs.get(\"top_p\", self.top_p),\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = decoded.split(\"<|assistant|>\\n\")[-1].strip()\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF 로딩 및 문서 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 로드\n",
    "loader = PyPDFLoader(\"res/SEVD.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "documents = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩 생성기 + FAISS 벡터 저장소 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 DB 생성\n",
    "vectordb = FAISS.from_documents(documents, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택적으로 디스크에 저장하고 불러올 수도 있습니다:\n",
    "vectordb.save_local(\"faiss_index/\")\n",
    "# 이후 다시 불러오기: FAISS.load_local(\"faiss_index\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS 벡터 저장소 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 로드 (다국어 지원)\n",
    "model = SentenceTransformer(MODEL_EMBED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋: 다국어 문장들\n",
    "sentences = [\n",
    "    \"고양이가 소파 위에서 자고 있다.\",              # 한국어\n",
    "    \"The cat is sleeping on the couch.\",      # 영어\n",
    "    \"Le chat dort sur le canapé.\",            # 프랑스어\n",
    "    \"Le chien dort sur le canapé.\",           # 프랑스어 : 개가 소파에서 자고 있어요.\n",
    "    \"Die Katze schläft auf dem Sofa.\",        # 독일어\n",
    "    \"Der Hund schläft auf dem Sofa.\",         # 독일어 : 개가 소파에서 자고 있어요.\n",
    "    \"El gato duerme en el sofá.\",             # 스페인어\n",
    "    \"El caballo está corriendo por el prado.\" # 스페인어 : 말이 초원을 달리고 있어요.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector db 저장\n",
    "sentence_embeddings = model.encode(sentences, convert_to_numpy=True).astype('float32')\n",
    "dimension = sentence_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # 유클리드 거리 기반\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 함수\n",
    "def queryVector(query, result_cnt=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, result_cnt)\n",
    "    \n",
    "    print(\"\\nTop Matches:\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        print(f\"{i+1}. {sentences[idx]} (거리: {distances[0][i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고양이 질문\n",
    "queryVector(\"소파에 있는 고양이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강아지 질문\n",
    "queryVector(\"소파에 있는 강아지\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초원 질문\n",
    "queryVector(\"초원에 석양이 지고 있어요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약 기능 구현 (전체 문서 요약)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(chat_model, chain_type=\"stuff\")\n",
    "summary = chain.invoke(documents)\n",
    "print(\"[+] 문서 요약:\\n\", summary[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetrievalQA 구성 (문서 기반 질의응답)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"이 문서에 포함된 모든 CVE 리스트\"\n",
    "result = retrieval_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💬 답변:\", result[\"result\"])\n",
    "print(\"\\n📄 참조 문서 일부:\\n\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.page_content[:200], \"\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

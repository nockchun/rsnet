{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR8E6URZK1X3"
      },
      "source": [
        "# 기본환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yLT1RFEeQgm"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo langchain-community pypdf langchain_huggingface faiss-cpu\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esrEO7yteQgm"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_KEY = userdata.get(\"HF_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPGOz6I1JkBj"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login(HF_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_XVtfyiLiAi"
      },
      "source": [
        "# 모델 로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oQenpTCPMyh"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU6jp1fqeQgn"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btydbHNpeQgn"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = 1024*5, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    # device_map = {\"\": device}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPtcr98MeQgn"
      },
      "outputs": [],
      "source": [
        "model = FastModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmMD0wu_eQgn"
      },
      "outputs": [],
      "source": [
        "# 임베딩 생성기 (한국어 포함 모델)\n",
        "MODEL_EMBED = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" # intfloat/multilingual-e5-base\n",
        "embedding = HuggingFaceEmbeddings(model_name=MODEL_EMBED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Ej1_yjeQgn"
      },
      "source": [
        "# Custom ChatModel 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf4trrKVeQgn"
      },
      "outputs": [],
      "source": [
        "from typing import List, Any, ClassVar\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_core.outputs import ChatResult, ChatGeneration\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjKeUw07eQgn"
      },
      "outputs": [],
      "source": [
        "class GemmaChatModel(BaseChatModel):\n",
        "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
        "        super().__init__()\n",
        "        object.__setattr__(self, \"model\", model)\n",
        "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
        "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
        "        object.__setattr__(self, \"do_sample\", do_sample)\n",
        "        object.__setattr__(self, \"temperature\", temperature)\n",
        "        object.__setattr__(self, \"top_p\", top_p)\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"gemma-chat\"\n",
        "\n",
        "    def _format_messages(self, messages: List[Any]) -> str:\n",
        "        prompt = \"\"\n",
        "        for message in messages:\n",
        "            if isinstance(message, SystemMessage):\n",
        "                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
        "            elif isinstance(message, HumanMessage):\n",
        "                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
        "            elif isinstance(message, AIMessage):\n",
        "                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
        "        prompt += \"<|assistant|>\\n\"\n",
        "        return prompt\n",
        "\n",
        "    def _generate(self, messages: List[Any], **kwargs) -> ChatResult:\n",
        "        prompt = self._format_messages(messages)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=self.max_tokens,\n",
        "                do_sample=kwargs.get(\"do_sample\", self.do_sample),\n",
        "                temperature=kwargs.get(\"temperature\", self.temperature),\n",
        "                top_p=kwargs.get(\"top_p\", self.top_p),\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = decoded.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "\n",
        "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L37Lhf-eQgn"
      },
      "outputs": [],
      "source": [
        "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG0QK9JaeQgn"
      },
      "source": [
        "# PDF 로딩 및 문서 분할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPqaPKUueQgo"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhdCXBjNeQgo"
      },
      "outputs": [],
      "source": [
        "# PDF 로드\n",
        "loader = PyPDFLoader(\"res/mitsubishi.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht8q1IEJeQgo"
      },
      "outputs": [],
      "source": [
        "# 텍스트 분할\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
        "documents = splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSwy_wZaeQgo"
      },
      "source": [
        "# 임베딩 생성기 + FAISS 벡터 저장소 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3POsneakeQgo"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jsGq65seQgo"
      },
      "outputs": [],
      "source": [
        "# 벡터 DB 생성\n",
        "vectordb = FAISS.from_documents(documents, embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUpm_C7KeQgo"
      },
      "outputs": [],
      "source": [
        "# 선택적으로 디스크에 저장하고 불러올 수도 있습니다:\n",
        "vectordb.save_local(\"faiss_index/\")\n",
        "# 이후 다시 불러오기: FAISS.load_local(\"faiss_index\", embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLK1BCn5eQgo"
      },
      "source": [
        "# FAISS 벡터 저장소 이해"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyKgszXLeQgo"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHtX1Vt1eQgo"
      },
      "outputs": [],
      "source": [
        "# 임베딩 모델 로드 (다국어 지원)\n",
        "model = SentenceTransformer(MODEL_EMBED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFjwFrdReQgo"
      },
      "outputs": [],
      "source": [
        "# 데이터셋: 다국어 문장들\n",
        "sentences = [\n",
        "    \"고양이가 소파 위에서 자고 있다.\",              # 한국어\n",
        "    \"The cat is sleeping on the couch.\",      # 영어\n",
        "    \"Le chat dort sur le canapé.\",            # 프랑스어\n",
        "    \"Le chien dort sur le canapé.\",           # 프랑스어 : 개가 소파에서 자고 있어요.\n",
        "    \"Die Katze schläft auf dem Sofa.\",        # 독일어\n",
        "    \"Der Hund schläft auf dem Sofa.\",         # 독일어 : 개가 소파에서 자고 있어요.\n",
        "    \"El gato duerme en el sofá.\",             # 스페인어\n",
        "    \"El caballo está corriendo por el prado.\" # 스페인어 : 말이 초원을 달리고 있어요.\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQHBOCXyeQgo"
      },
      "outputs": [],
      "source": [
        "# vector db 저장\n",
        "sentence_embeddings = model.encode(sentences, convert_to_numpy=True).astype('float32')\n",
        "dimension = sentence_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)  # 유클리드 거리 기반 DB 생성\n",
        "index.add(sentence_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYwZayDGeQgo"
      },
      "outputs": [],
      "source": [
        "# query 함수\n",
        "def queryVector(query, result_cnt=3):\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
        "    distances, indices = index.search(query_embedding, result_cnt)\n",
        "\n",
        "    print(\"\\nTop Matches:\")\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        print(f\"{i+1}. {sentences[idx]} (거리: {distances[0][i]:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W3J27nqeQgo"
      },
      "outputs": [],
      "source": [
        "# 고양이 질문\n",
        "queryVector(\"소파에 있는 고양이\", 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMkjF4M4eQgo"
      },
      "outputs": [],
      "source": [
        "# 강아지 질문\n",
        "queryVector(\"소파에 있는 강아지\", 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI7iea81eQgo"
      },
      "outputs": [],
      "source": [
        "# 초원 질문\n",
        "queryVector(\"초원에 석양이 지고 있어요.\", 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdeHor8eeQgo"
      },
      "source": [
        "# 요약 기능 구현 (전체 문서 요약)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWRyr4ibeQgo"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E_DdKlxeQgo"
      },
      "outputs": [],
      "source": [
        "chain = load_summarize_chain(chat_model, chain_type=\"stuff\")\n",
        "summary = chain.invoke(documents)\n",
        "print(\"[+] 문서 요약:\\n\", summary[\"output_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thXqrikeeQgp"
      },
      "source": [
        "# RetrievalQA 구성 (문서 기반 질의응답)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVpUqTXVeQgp"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnTDX3F2eQgp"
      },
      "outputs": [],
      "source": [
        "retrieval_chain = RetrievalQA.from_chain_type(\n",
        "    llm=chat_model,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8cbfLX-eQgp"
      },
      "outputs": [],
      "source": [
        "query = \"이 문서에서 이야기 하고 있는 모든 CVE 리스트를 알려주세요.\"\n",
        "result = retrieval_chain.invoke({\"query\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH_bAKZKeQgp"
      },
      "outputs": [],
      "source": [
        "print(\"[+] 답변:\", result[\"result\"])\n",
        "print(\"\\n[+] 참조 문서 일부:\\n\")\n",
        "for doc in result[\"source_documents\"]:\n",
        "    print(doc.page_content[:200], \"\\n---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIlQrjbseQgp"
      },
      "outputs": [],
      "source": [
        "query = \"취약점으로 인해 생길수 있는 시스템의 문제점을 요약해서 시스템 관리자가 쉽게 이해할 수 있도록 문제점만 명확하게 요약해 주세요.\"\n",
        "result = retrieval_chain.invoke({\"query\": query})\n",
        "print(\"[+] 답변:\", result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUYLdkGnmKyK"
      },
      "source": [
        "# 프롬프트 구성 방식: Instruction + User Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MlWa86eeQgp"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtRM0k7aeQgp"
      },
      "outputs": [],
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "너는 문서에서 정보를 추출하여 질문에 정확하게 단계적으로 답변하는 AI야.\n",
        "\n",
        "[지시사항]\n",
        "- 반드시 제공된 문서(context)의 내용만 기반으로 답변해야 해.\n",
        "- 모호한 경우는 \"문서에 정보가 없습니다.\"라고 말해.\n",
        "- 복잡한 질문은 논리적으로 여러 단계를 거쳐 생각해.\n",
        "\n",
        "[문서 요약/본문]\n",
        "{context}\n",
        "\n",
        "[질문]\n",
        "{question}\n",
        "\n",
        "[답변]\n",
        "\"\"\".strip()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUlUBuBkeQgp"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=chat_model,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": prompt_template},\n",
        "    return_source_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yLluTS5eQgp"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"이 문서의 주요 주장 중 시스템 영향과 심각성 정도가 언급된 부분을 찾아서,\n",
        "그 내용을 요약하고 각각의 영향을 비교해 주세요.\"\"\"\n",
        "response = qa_chain.invoke({\"query\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4JEkoPDmh7r"
      },
      "outputs": [],
      "source": [
        "print(\"[+] 답변:\\n\", response[\"result\"])\n",
        "print(\"[+] 참조 문서:\\n\", response[\"source_documents\"][0].page_content[:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1JC317XmnSW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMmCQ77VR5Ww"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ5wBwx3R5Ww"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA5I24uaR5Ww"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
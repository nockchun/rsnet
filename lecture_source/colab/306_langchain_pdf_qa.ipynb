{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR8E6URZK1X3"
   },
   "source": [
    "# ê¸°ë³¸í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_KEY = userdata.get(\"HF_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPGOz6I1JkBj"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XVtfyiLiAi"
   },
   "source": [
    "# ëª¨ë¸ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oQenpTCPMyh"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    device_map = {\"\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = FastModel.from_pretrained(\n",
    "#     model_name=\"unsloth/gemma-3-4b-it\",\n",
    "#     max_seq_length=1024*5, # Choose any for long context!\n",
    "#     load_in_4bit=True,  # 4 bit quantization to reduce memory\n",
    "#     load_in_8bit=False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "#     device_map = {\"\": device}  # â† ì—¬ê¸°ì„œ GPU 2ë²ˆ ì§€ì •\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ìƒì„±ê¸° (í•œêµ­ì–´ í¬í•¨ ëª¨ë¸)\n",
    "MODEL_EMBED = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" # intfloat/multilingual-e5-base\n",
    "embedding = HuggingFaceEmbeddings(model_name=MODEL_EMBED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ChatModel í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, ClassVar\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaChatModel(BaseChatModel):\n",
    "    def __init__(self, model, tokenizer, max_tokens: int = 512, do_sample: bool = True, temperature: float = 0.7, top_p: float = 0.9):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, \"model\", model)\n",
    "        object.__setattr__(self, \"tokenizer\", tokenizer)\n",
    "        object.__setattr__(self, \"max_tokens\", max_tokens)\n",
    "        object.__setattr__(self, \"do_sample\", do_sample)\n",
    "        object.__setattr__(self, \"temperature\", temperature)\n",
    "        object.__setattr__(self, \"top_p\", top_p)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemma-chat\"\n",
    "\n",
    "    def _format_messages(self, messages: List[Any]) -> str:\n",
    "        prompt = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, SystemMessage):\n",
    "                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "        prompt += \"<|assistant|>\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def _generate(self, messages: List[Any], **kwargs) -> ChatResult:\n",
    "        prompt = self._format_messages(messages)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                do_sample=kwargs.get(\"do_sample\", self.do_sample),\n",
    "                temperature=kwargs.get(\"temperature\", self.temperature),\n",
    "                top_p=kwargs.get(\"top_p\", self.top_p),\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = decoded.split(\"<|assistant|>\\n\")[-1].strip()\n",
    "\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = GemmaChatModel(model=model, tokenizer=tokenizer, max_tokens=1024*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF ë¡œë”© ë° ë¬¸ì„œ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF ë¡œë“œ\n",
    "loader = PyPDFLoader(\"res/SEVD.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "documents = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì„ë² ë”© ìƒì„±ê¸° + FAISS ë²¡í„° ì €ì¥ì†Œ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„° DB ìƒì„±\n",
    "vectordb = FAISS.from_documents(documents, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ íƒì ìœ¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n",
    "vectordb.save_local(\"faiss_index/\")\n",
    "# ì´í›„ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°: FAISS.load_local(\"faiss_index\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS ë²¡í„° ì €ì¥ì†Œ ì´í•´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì›)\n",
    "model = SentenceTransformer(MODEL_EMBED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹: ë‹¤êµ­ì–´ ë¬¸ì¥ë“¤\n",
    "sentences = [\n",
    "    \"ê³ ì–‘ì´ê°€ ì†ŒíŒŒ ìœ„ì—ì„œ ìê³  ìˆë‹¤.\",              # í•œêµ­ì–´\n",
    "    \"The cat is sleeping on the couch.\",      # ì˜ì–´\n",
    "    \"Le chat dort sur le canapÃ©.\",            # í”„ë‘ìŠ¤ì–´\n",
    "    \"Le chien dort sur le canapÃ©.\",           # í”„ë‘ìŠ¤ì–´ : ê°œê°€ ì†ŒíŒŒì—ì„œ ìê³  ìˆì–´ìš”.\n",
    "    \"Die Katze schlÃ¤ft auf dem Sofa.\",        # ë…ì¼ì–´\n",
    "    \"Der Hund schlÃ¤ft auf dem Sofa.\",         # ë…ì¼ì–´ : ê°œê°€ ì†ŒíŒŒì—ì„œ ìê³  ìˆì–´ìš”.\n",
    "    \"El gato duerme en el sofÃ¡.\",             # ìŠ¤í˜ì¸ì–´\n",
    "    \"El caballo estÃ¡ corriendo por el prado.\" # ìŠ¤í˜ì¸ì–´ : ë§ì´ ì´ˆì›ì„ ë‹¬ë¦¬ê³  ìˆì–´ìš”.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector db ì €ì¥\n",
    "sentence_embeddings = model.encode(sentences, convert_to_numpy=True).astype('float32')\n",
    "dimension = sentence_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query í•¨ìˆ˜\n",
    "def queryVector(query, result_cnt=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, result_cnt)\n",
    "    \n",
    "    print(\"\\nTop Matches:\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        print(f\"{i+1}. {sentences[idx]} (ê±°ë¦¬: {distances[0][i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ì–‘ì´ ì§ˆë¬¸\n",
    "queryVector(\"ì†ŒíŒŒì— ìˆëŠ” ê³ ì–‘ì´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°•ì•„ì§€ ì§ˆë¬¸\n",
    "queryVector(\"ì†ŒíŒŒì— ìˆëŠ” ê°•ì•„ì§€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ˆì› ì§ˆë¬¸\n",
    "queryVector(\"ì´ˆì›ì— ì„ì–‘ì´ ì§€ê³  ìˆì–´ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìš”ì•½ ê¸°ëŠ¥ êµ¬í˜„ (ì „ì²´ ë¬¸ì„œ ìš”ì•½)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(chat_model, chain_type=\"stuff\")\n",
    "summary = chain.invoke(documents)\n",
    "print(\"[+] ë¬¸ì„œ ìš”ì•½:\\n\", summary[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetrievalQA êµ¬ì„± (ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ì´ ë¬¸ì„œì— í¬í•¨ëœ ëª¨ë“  CVE ë¦¬ìŠ¤íŠ¸\"\n",
    "result = retrieval_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ’¬ ë‹µë³€:\", result[\"result\"])\n",
    "print(\"\\nğŸ“„ ì°¸ì¡° ë¬¸ì„œ ì¼ë¶€:\\n\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.page_content[:200], \"\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

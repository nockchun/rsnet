{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s my_session -l python -u http://192.168.0.200:8998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284506fc030e4263802d0bfbcd824005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HBox(childre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of numbers is 1 and its description is:\n",
      "b'(8) ParallelCollectionRDD[23] at parallelize at PythonRDD.scala:195 []'"
     ]
    }
   ],
   "source": [
    "%%spark -s kjh-session\n",
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "print('First element of numbers is {} and its description is:\\n{}'.format(numbers.first(), numbers.toDebugString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "20/06/20 22:20:08 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.3 KB, free 353.1 MB)\n",
      "20/06/20 22:20:08 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.6 KB, free 353.1 MB)\n",
      "20/06/20 22:20:08 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on vmaster:40169 (size: 7.6 KB, free: 353.4 MB)\n",
      "20/06/20 22:20:08 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163\n",
      "20/06/20 22:20:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD[15] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "20/06/20 22:20:08 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks\n",
      "20/06/20 22:20:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7, 192.168.0.202, executor 1, partition 0, PROCESS_LOCAL, 8072 bytes)\n",
      "20/06/20 22:20:08 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.202:46159 (size: 7.6 KB, free: 366.3 MB)\n",
      "20/06/20 22:20:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 1014 ms on 192.168.0.202 (executor 1) (1/1)\n",
      "20/06/20 22:20:09 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "20/06/20 22:20:09 INFO scheduler.DAGScheduler: ResultStage 4 (runJob at PythonRDD.scala:153) finished in 1.049 s\n",
      "20/06/20 22:20:09 INFO scheduler.DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:153, took 1.050882 s\n",
      "20/06/20 22:20:50 INFO datasources.FileSourceStrategy: Pruning directories with: \n",
      "20/06/20 22:20:50 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "20/06/20 22:20:50 INFO datasources.FileSourceStrategy: Output Data Schema: struct<age: bigint, name: string>\n",
      "20/06/20 22:20:50 INFO execution.FileSourceScanExec: Pushed Filters: \n",
      "20/06/20 22:20:50 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 284.0 KB, free 352.8 MB)\n",
      "20/06/20 22:20:50 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.2 KB, free 352.8 MB)\n",
      "20/06/20 22:20:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on vmaster:40169 (size: 24.2 KB, free: 353.3 MB)\n",
      "20/06/20 22:20:50 INFO spark.SparkContext: Created broadcast 6 from toJavaRDD at NativeMethodAccessorImpl.java:0\n",
      "20/06/20 22:20:50 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/06/20 22:20:50 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
      "20/06/20 22:20:50 INFO scheduler.DAGScheduler: Got job 5 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "20/06/20 22:20:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (runJob at PythonRDD.scala:153)\n",
      "20/06/20 22:20:50 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "20/06/20 22:20:50 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "20/06/20 22:20:50 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (PythonRDD[22] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "20/06/20 22:20:50 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.1 KB, free 352.7 MB)\n",
      "20/06/20 22:20:50 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 11.4 KB, free 352.7 MB)\n",
      "20/06/20 22:20:50 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on vmaster:40169 (size: 11.4 KB, free: 353.3 MB)\n",
      "20/06/20 22:20:50 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1163\n",
      "20/06/20 22:20:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (PythonRDD[22] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "20/06/20 22:20:50 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks\n",
      "20/06/20 22:20:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 8, 192.168.0.202, executor 1, partition 0, ANY, 8251 bytes)\n",
      "20/06/20 22:20:50 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.202:46159 (size: 11.4 KB, free: 366.3 MB)\n",
      "20/06/20 22:20:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.202:46159 (size: 24.2 KB, free: 366.2 MB)\n",
      "20/06/20 22:20:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 8) in 223 ms on 192.168.0.202 (executor 1) (1/1)\n",
      "20/06/20 22:20:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "20/06/20 22:20:50 INFO scheduler.DAGScheduler: ResultStage 5 (runJob at PythonRDD.scala:153) finished in 0.237 s\n",
      "20/06/20 22:20:50 INFO scheduler.DAGScheduler: Job 5 finished: runJob at PythonRDD.scala:153, took 0.239938 s\n",
      "20/06/20 22:42:59 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Got job 6 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (runJob at PythonRDD.scala:153)\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (PythonRDD[24] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "20/06/20 22:42:59 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 4.3 KB, free 352.7 MB)\n",
      "20/06/20 22:42:59 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.0 KB, free 352.7 MB)\n",
      "20/06/20 22:42:59 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on vmaster:40169 (size: 3.0 KB, free: 353.3 MB)\n",
      "20/06/20 22:42:59 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1163\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[24] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "20/06/20 22:42:59 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks\n",
      "20/06/20 22:42:59 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 9, 192.168.0.202, executor 1, partition 0, PROCESS_LOCAL, 7856 bytes)\n",
      "20/06/20 22:42:59 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.202:46159 (size: 3.0 KB, free: 366.2 MB)\n",
      "20/06/20 22:42:59 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 9) in 60 ms on 192.168.0.202 (executor 1) (1/1)\n",
      "20/06/20 22:42:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: ResultStage 6 (runJob at PythonRDD.scala:153) finished in 0.068 s\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Job 6 finished: runJob at PythonRDD.scala:153, took 0.072386 s\n",
      "20/06/20 22:42:59 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Got job 7 (runJob at PythonRDD.scala:153) with 4 output partitions\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (runJob at PythonRDD.scala:153)\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (PythonRDD[25] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "20/06/20 22:42:59 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 4.3 KB, free 352.7 MB)\n",
      "20/06/20 22:42:59 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.0 KB, free 352.7 MB)\n",
      "20/06/20 22:42:59 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on vmaster:40169 (size: 3.0 KB, free: 353.3 MB)\n",
      "20/06/20 22:42:59 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1163\n",
      "20/06/20 22:42:59 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 7 (PythonRDD[25] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "20/06/20 22:42:59 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 4 tasks\n",
      "20/06/20 22:42:59 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, 192.168.0.202, executor 1, partition 1, PROCESS_LOCAL, 7875 bytes)\n",
      "20/06/20 22:42:59 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, 192.168.0.201, executor 3, partition 2, PROCESS_LOCAL, 7856 bytes)\n",
      "20/06/20 22:42:59 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, 192.168.0.201, executor 2, partition 3, PROCESS_LOCAL, 7875 bytes)\n",
      "20/06/20 22:42:59 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, 192.168.0.202, executor 0, partition 4, PROCESS_LOCAL, 7856 bytes)\n",
      "20/06/20 22:42:59 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.202:46159 (size: 3.0 KB, free: 366.2 MB)\n",
      "20/06/20 22:42:59 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.201:46093 (size: 3.0 KB, free: 366.3 MB)\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 184\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 79\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 171\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 77\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 164\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 143\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 100\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 88\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 169\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 120\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 76\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 148\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 113\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 92\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 163\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 83\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 94\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 181\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 150\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 95\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 185\n",
      "20/06/20 22:42:59 INFO spark.ContextCleaner: Cleaned accumulator 149"
     ]
    }
   ],
   "source": [
    "%spark -s kjh-session logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o183.json.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$class.buildReaderWithPartitionValues(FileFormat.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:165)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:310)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:306)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:103)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:98)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n",
      "\tat scala.Option.orElse(Option.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:392)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 274, in json\n",
      "    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o183.json.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$class.buildReaderWithPartitionValues(FileFormat.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:165)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:310)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:306)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:103)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:98)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n",
      "\tat scala.Option.orElse(Option.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:392)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark -s kjh-session\n",
    "df = spark.read.json(\"hdfs:///people.json\")\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o229.toJavaRDD.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2359)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.numParallelism$lzycompute(LocalTableScanExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.numParallelism(LocalTableScanExec.scala:48)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd$lzycompute(LocalTableScanExec.scala:51)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd(LocalTableScanExec.scala:51)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.doExecute(LocalTableScanExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.MapPartitionsExec.doExecute(objects.scala:185)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3043)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3041)\n",
      "\tat org.apache.spark.sql.Dataset.toJavaRDD(Dataset.scala:3053)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 123, in toJSON\n",
      "    return RDD(rdd.toJavaRDD(), self._sc, UTF8Deserializer(use_unicode))\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o229.toJavaRDD.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2359)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.numParallelism$lzycompute(LocalTableScanExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.numParallelism(LocalTableScanExec.scala:48)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd$lzycompute(LocalTableScanExec.scala:51)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd(LocalTableScanExec.scala:51)\n",
      "\tat org.apache.spark.sql.execution.LocalTableScanExec.doExecute(LocalTableScanExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.MapPartitionsExec.doExecute(objects.scala:185)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3043)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3041)\n",
      "\tat org.apache.spark.sql.Dataset.toJavaRDD(Dataset.scala:3053)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark -s kjh-session -c sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Michael</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>Andy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Justin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age     name\n",
       "0   NaN  Michael\n",
       "1  30.0     Andy\n",
       "2  19.0   Justin"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -s kjh-session -c sql -o df_people --maxrows 10\n",
    "SELECT * FROM people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Michael</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>Andy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Justin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age     name\n",
       "0   NaN  Michael\n",
       "1  30.0     Andy\n",
       "2  19.0   Justin"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovizwidget.widget.utils import display_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22284607bdc4434a5ebf5e38cfe1389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26fee5afa4a4eba8ef5274f01fcbed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8389c0790c34c51ba6f61a5e316681c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AutoVizWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_dataframe(df_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No module named 'matplotlib'\n",
      "Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'matplotlib'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark -s kjh-session\n",
    "import matplotlib.pyplot as plt\n",
    "ax = df.toPandas().plot.bar(x='name',y='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import cufflinks as cf\n",
    "cf.go_offline(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Feature3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.980672</td>\n",
       "      <td>8.584275</td>\n",
       "      <td>0.439760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.698867</td>\n",
       "      <td>3.485290</td>\n",
       "      <td>7.350106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.409296</td>\n",
       "      <td>6.333909</td>\n",
       "      <td>6.511705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.299926</td>\n",
       "      <td>6.546977</td>\n",
       "      <td>3.608067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.376197</td>\n",
       "      <td>3.835976</td>\n",
       "      <td>3.114001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature1  Feature2  Feature3\n",
       "0  3.980672  8.584275  0.439760\n",
       "1  5.698867  3.485290  7.350106\n",
       "2  7.409296  6.333909  6.511705\n",
       "3  9.299926  6.546977  3.608067\n",
       "4  9.376197  3.835976  3.114001"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.random([100,3]) * 10)\n",
    "df.columns = ['Feature1','Feature2','Feature3']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbfc664079a4dca9648f2f81ee56b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='column', options=('Feature2', 'Feature3'), value='Feature2'), IntS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def show_data_more_than(column=['Feature2','Feature3'], \n",
    "                        x=(0,10,1)):\n",
    "    return df.loc[df[column] > x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge python-cufflinks -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1122fd8cdf4d35a4c725f717cb9d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('Feature1', 'Feature2', 'Feature3'), value='Feature1'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def scatter_plot(x=list(df.select_dtypes('number').columns), \n",
    "                 y=list(df.select_dtypes('number').columns),\n",
    "                 theme=list(cf.themes.THEMES.keys()), \n",
    "                 colorscale=list(cf.colors._scales_names.keys())):\n",
    "    \n",
    "    \n",
    "    if x.title() == y.title():\n",
    "        print('Can Not Use Same Value')\n",
    "    else:\n",
    "        title=f'{y.title()} vs {x.title()}'\n",
    "        df.iplot(kind='scatter', x=x, y=y, mode='markers', \n",
    "                 xTitle=x.title(), yTitle=y.title(), \n",
    "                 #text='title',\n",
    "                 title=f'{y.title()} vs {x.title()}',\n",
    "                theme=theme, colorscale=colorscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvacText: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25\n",
      "res1: Int = 1\n"
     ]
    }
   ],
   "source": [
    "%%spark -s my-scala\n",
    "val hvacText = sc.parallelize(Array(1, 2, 3, 4))\n",
    "hvacText.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n"
     ]
    }
   ],
   "source": [
    "%%spark -s my-scala\n",
    "val df = spark.read.json(\"hdfs:///people.json\")\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Michael</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>Andy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Justin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age     name\n",
       "0   NaN  Michael\n",
       "1  30.0     Andy\n",
       "2  19.0   Justin"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -s my-scala -c sql -o my_df_from_scala --maxrows 10\n",
    "SELECT * FROM people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284506fc030e4263802d0bfbcd824005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HBox(childre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

#########################################################################################
###                              Drivers & Basic Settings                             ###
#########################################################################################
* Add Nvidia Driver Repository ----------------------------------------------------------
user:~$ sudo add-apt-repository ppa:graphics-drivers/ppa
user:~$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
user:~$ sudo apt update


* Install Basic Library & Fons ----------------------------------------------------------
user:~$ sudo apt install vim openssh-server build-essential

user# sudo apt-get install fonts-unfonts-core fonts-unfonts-extra fonts-baekmuk fonts-nanum fonts-nanum-coding fonts-nanum-extra

user:~$ wget https://github.com/naver/d2codingfont/releases/download/VER1.21/D2Coding-1.2.zip
user:~$ sudo mkdir /usr/share/fonts/truetype/D2Coding
user:~$ sudo unzip D2Coding-1.2.zip -d /usr/share/fonts/truetype/D2Coding/
user:~$ sudo rm -rf /usr/share/fonts/truetype/D2Coding/__MACOSX
user:~$ sudo fc-cache -f -v


* Change Host Name & Hosts Info ---------------------------------------------------------
root:~$ hostnamectl set-hostname node19
root:~$ cat >> /etc/hosts << EOF
10.10.1.101 master.lab master
10.10.1.111 node01.lab node01
10.10.1.112 node02.lab node02
EOF


* Change Booting mode -------------------------------------------------------------------
root:~$ systemctl set-default multi-user.target
root:~$ systemctl set-default graphical.target 


* MAC Them ------------------------------------------------------------------------------
URL : https://manjaro.site/make-linux-mint-19-looks-like-mac-os-x-mojave


* Terminal Them With Powerline ----------------------------------------------------------
root:~$ apt install powerline python-pip python-setuptools fonts-powerline fontconfig
root:~$ pip install git+git://github.com/Lokaltog/powerline
root:~$ wget https://github.com/Lokaltog/powerline/raw/develop/font/PowerlineSymbols.otf
root:~$ wget https://github.com/Lokaltog/powerline/raw/develop/font/10-powerline-symbols.conf
root:~$ mv PowerlineSymbols.otf /usr/share/fonts/
root:~$ fc-cache -vf
root:~$ mv 10-powerline-symbols.conf /etc/fonts/conf.d/

root:~$ vi /etc/bash.bashrc
if [ -f /usr/local/lib/python2.7/dist-packages/powerline/bindings/bash/powerline.sh ]; then
    source /usr/local/lib/python2.7/dist-packages/powerline/bindings/bash/powerline.sh
fi


* Vi Editor (~/.vimrc) ------------------------------------------------------------------
root:~$ cat > /etc/vim/vimrc << EOF
set number
set tabstop=4
set shiftwidth=4
set showmatch
set title
set hlsearch
set fileencodings=utf-8,euc-kr
set ruler
set title
set statusline=\ %<%l:%v\ [%P]%=%a\ %h%m%r\ %F\
set rtp+=/usr/local/lib/python2.7/dist-packages/powerline/bindings/vim/
set laststatus=2
set t_Co=256

syntax on
EOF


#########################################################################################
###                                        NFS                                        ###
#########################################################################################
* At Server ----------------------------------------------------------------
root:~$ apt install nfs-common nfs-kernel-server rpcbind portmap
root:~$ mkdir -p /data/vol_nfs
root:~$ chmod -R 777 /data/vol_nfs
root:~$ vi /etc/exports
/data/vol_nfs 192.168.0.0/24(rw,sync,no_subtree_check)
root:~$ exportfs -a
root:~$ systemctl restart nfs-kernel-server
root:~$ systemctl enable nfs-kernel-server

* At Client ----------------------------------------------------------------
root:~$ apt install nfs-common
root:~$ mkdir /vol_nfs
root:~$ mount 192.168.0.211:/data/vol_nfs /vol_nfs


#########################################################################################
###                                     GlusterFS                                     ###
#########################################################################################
* At All Distibut Volume Node -----------------------------------------------------------
root:~$ apt install software-properties-common
root:~$ add-apt-repository ppa:gluster/glusterfs-7
root:~$ apt-get update
root:~$ apt install glusterfs-server
root:~$ mkdir -p /data/vol_gluster
root:~$ chmod -R 777 /data/vol_gluster

* At One Distibut Volume Node To the others ---------------------------------------------
root:~$ gluster peer probe <other node ip>
root:~$ gluster peer status
root:~$ gluster pool list

* Create & Start Volume -----------------------------------------------------------------
root:~$ gluster volume create vol_gluster replica 2 transport tcp \
    node01:/data/vol_gluster \
    node02:/data/vol_gluster \
    node03:/data/vol_gluster \
    node04:/data/vol_gluster force
root:~$ gluster volume start vol_gluster
root:~$ gluster volume info vol_gluster

* Mount At Client -----------------------------------------------------------------------
user:~$ sudo apt install software-properties-common
user:~$ sudo add-apt-repository ppa:gluster/glusterfs-7
user:~$ sudo apt update
user:~$ sudo apt install glusterfs-client
user:~$ sudo mkdir -p /vol_gluster
user:~$ sudo mount -t glusterfs node01:/vol_gluster /vol_gluster

* Accecontrol ---------------------------------------------------------------------------
root:~$ gluster volume set vol auth.allow 127.0.0.1

* fstab ---------------------------------------------------------------------------------
root:~$ vi /etc/fstab
node01:/vol /mnt/gluster glusterfs defaults,_netdev 0 0
root:~$ mount -a


#########################################################################################
###                                       Ceph                                        ###
#########################################################################################
※ SITE : https://ssup2.github.io/record/Ceph_%EC%84%A4%EC%B9%98_%EC%8B%A4%ED%96%89_ceph-deploy_Ubuntu_18.04/

* At All Distibut Volume Node -----------------------------------------------------------
user:~$ sudo apt install ntp
user:~$ sudo vi /etc/ntp.conf
server 1.kr.pool.ntp.org
server 1.asia.pool.ntp.org
server time.bora.net

user:~$ sudo systemctl enable ntp && sudo systemctl restart ntp
user:~$ sudo ntpq -p
user:~$ sudo apt install python
user:~$ sudo adduser cephnode
user:~$ echo "cephnode ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephnode
user:~$ sudo chmod 0440 /etc/sudoers.d/cephnode

* At Deploy Node ------------------------------------------------------------------------
user:~$ sudo vi /etc/hosts
192.168.0.211 node01
192.168.0.212 node02
192.168.0.213 node03
192.168.0.214 node04

user:~$ wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -
user:~$ echo deb https://download.ceph.com/debian-luminous/ bionic main | sudo tee /etc/apt/sources.list.d/ceph.list
user:~$ sudo apt update
user:~$ sudo apt install ceph-common ceph-deploy
user:~$ ssh-keygen
user:~$ for i in node01 node02 node03;
do
    ssh-copy-id cephnode@$i
done

user:~$ vi .ssh/config
Host node01
   Hostname node01
   User cephnode
Host node02
   Hostname node02
   User cephnode
Host node03
   Hostname node03
   User cephnode

user:~$ mkdir ceph-cluster && cd ceph-cluster
user:ceph-cluster$ ceph-deploy purge node01 node02 node03
user:ceph-cluster$ ceph-deploy purgedata node01 node02 node03
user:ceph-cluster$ ceph-deploy forgetkeys
user:ceph-cluster$ rm ceph*
user:ceph-cluster$ ceph-deploy new node01
user:ceph-cluster$ ceph-deploy install node01 node02 node03
user:ceph-cluster$ ceph-deploy mon create-initial
user:ceph-cluster$ ceph-deploy admin node01 node02 node03
user:ceph-cluster$ ceph-deploy mgr create node01
user:ceph-cluster$ for i in node01 node02 node03;
do
    ceph-deploy osd create --data /dev/sdb $i
done

user:ceph-cluster$ ceph-deploy mds create node01
user:ceph-cluster$ ceph-deploy rgw create node01

* At distibut node Create block storage ---------------------------------------------
user:~$ sudo chown -R rsnet.rsnet /etc/ceph
user:~$ ceph -s
user:~$ ceph osd pool create rbd 16
user:~$ rbd pool init rbd
user:~$ rbd create foo --size 4096 --image-feature layering
user:~$ sudo rbd map foo --name client.admin

* At distibut node Create file storage ---------------------------------------------
user:~$ ceph osd pool create cephfs_data 16
user:~$ ceph osd pool create cephfs_metadata 16
user:~$ ceph fs new filesystem cephfs_metadata cephfs_data
user:~$ ceph auth get client.admin

* Using distibut node at client ---------------------------------------------
root:~$ vi /root/admin.secret
AQB9Sctey0pxLxAAfTRtV8TLe3IbXXOkID131w==
root:~$ mkdir /mnt/ceph
root:~$ mount -t ceph 192.168.0.211:6789:/ /mnt/ceph/ -o name=admin,secretfile=/root/admin.secret


#########################################################################################
###                                  Sublime Text 3                                   ###
#########################################################################################
* Install Sublime Text 3 ----------------------------------------------------------------
user:~$ wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -
user:~$ sudo apt-get install apt-transport-https
user:~$ echo "deb https://download.sublimetext.com/ apt/stable/" | sudo tee /etc/apt/sources.list.d/sublime-text.list
user:~$ sudo apt update
user:~$ sudo apt install sublime-text sublime-merge


* Setting Up Sublime Text 3 Plugins -----------------------------------------------------
1. SideBarEnhanceinsments
    > site : https://github.com/titoBouzout/SideBarEnhancements
    > Sidebar에 대해 확장기능 플러그인.

2. Emmet
    > site : https://github.com/sergeche/emmet-sublime
    > HTML 및 CSS 작업을 도와주는 플러그인.

3. SFTP
    > site : http://wbond.net/sublime_packages/sftp
    > sftp를 통해 서버와 파일 교환을 도와주는 플러그인.

4. AutoBackups
    > site : https://github.com/akalongman/sublimetext-autobackups
    > 편집하는 문서들에 대해 자동으로 백업을 해 주는 플러그인.
        {
            "backup_dir": "/data/workspace/backup/sublime",
        }

5. SublimeCodeIntel
    > site : https://github.com/SublimeCodeIntel/SublimeCodeIntel
    > 소스코드에 intelligence 기능과 smart autocomplete 기능을 지원하는 플러그인.
    > Key For Linux:
        - Jump to definition = ``Super+Click``
        - Jump to definition = ``Control+Super+Alt+Up``
        - Go back = ``Control+Super+Alt+Left``
        - Manual CodeIntel = ``Control+Shift+space``
    > Key For Windows:
        - Jump to definition = ``Alt+Click``
        - Jump to definition = ``Control+Windows+Alt+Up``
        - Go back = ``Control+Windows+Alt+Left``
        - Manual CodeIntel = ``Control+Shift+space``

6. DocBlockr
    > site : https://github.com/spadgos/sublime-jsdocs
    > 함수들의 주석문 template을 자동으로 생성.

7. DocBlockr Python
    > site : https://github.com/adambullmer/sublime_docblockr_python
    > 함수들의 주석문 template을 자동으로 생성.

8. AdvancedNewFile
    > site : https://github.com/skuroda/Sublime-AdvancedNewFile
    > 새파일을 만들게 해 주는 플러그인.

9. Terminality & Terminal
    > ctrl + alt + r : 터미널을 sublime text 안에서 열어 줌.
    > ctrl + shift + t : 시스템 터미널을 현재 파일 디렉토리에서 열어 줌.

10. Pretty JSON & HTML-CSS-JS Prettify
    > 문서 포맷을 예쁘게 만들어 줌.

11. Compare Side-By-Side
    > 2개의 문서를 비교해서 틀린부분을 보여줌.

12. GitSavvy
    > git 사용을 도와주는 플러그인.
    > 기본 설정.
        - git config --global user.email "you@example.com"
        - git config --global user.name "Your Name"

13. materialize & soda
    > site : https://github.com/saadq/Materialize
    > UI 테마
    > setting
        "color_scheme": "Packages/Materialize/schemes/Material Monokai Soda.tmTheme",
        "theme": "Soda Dark 3.sublime-theme",
        "font_face": "D2Coding",
        "font_size": 13,
        "line_padding_bottom": 2,
        "line_padding_top": 2,
        "margin": 0,
        "material_theme_compact_panel": true,
        "material_theme_compact_sidebar": true,
        "material_theme_contrast_mode": true,
        "material_theme_small_statusbar": true,
        "material_theme_small_tab": true,
        "material_theme_tabs_autowidth": true,
        "overlay_scroll_bars": "enabled",
        "indent_guide_options": [ "draw_normal", "draw_active" ]


#########################################################################################
###                                     Terminator                                    ###
#########################################################################################
* Install -------------------------------------------------------------------------------
user:~$ sudo apt install -y terminator


* Useage --------------------------------------------------------------------------------
Toggle fullscreen                          : F11.
Split terminals horizontally               : Ctrl + Shift + O.
Split terminals vertically                 : Ctrl + Shift + E.
Close current Panel                        : Ctrl + Shift + W.
Open new tab                               : Ctrl + Shift + T.
Move to the terminal above the current one : Alt + ↑
Move to the terminal below the current one : Alt + ↓


#########################################################################################
###                      Docker, Nvidia-Docker2 For Kubeflow 1.0                      ###
#########################################################################################
※ Kuberflow 1.0 system requirements
  > kubeflow 1.0 == docker-ce-18.09.8 + Kubernetes 1.15

* Docker & Docker-Compose Install -------------------------------------------------------
user:~$ sudo apt install apt-transport-https ca-certificates curl software-properties-common
user:~$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
user:~$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"
user:~$ sudo apt update
user:~$ sudo apt install -y docker-ce=5:18.09.9~3-0~ubuntu-bionic docker-ce-cli=5:18.09.9~3-0~ubuntu-bionic containerd.io=1.2.10-3
user:~$ sudo apt-mark hold docker-ce docker-ce-cli
user:~$ sudo usermod -aG docker $USER
user:~$ sudo apt install -y docker-compose


* Nvidia-Docker Install ----------------------------------------------------------------
user:~$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
user:~$ sudo vi /etc/apt/sources.list.d/nvidia-docker.list
deb https://nvidia.github.io/libnvidia-container/ubuntu18.04/$(ARCH) /
deb https://nvidia.github.io/nvidia-container-runtime/ubuntu18.04/$(ARCH) /
deb https://nvidia.github.io/nvidia-docker/ubuntu18.04/$(ARCH) /
user:~$ sudo apt update
user:~$ sudo apt install -y nvidia-docker2


* Setting Docker For nvidia gpu --------------------------------------------------------
user:~$ sudo vi /etc/docker/daemon.json
{
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}
user:~$ sudo systemctl daemon-reload
user:~$ sudo systemctl restart docker


#########################################################################################
###                            Kubernetes For Kubeflow 1.0                            ###
#########################################################################################
* swap & sysctl settings for Kubernetes networking --------------------------------------
root:~$ swapoff -a
root:~$ sed -i '/swap/d' /etc/fstab
root:~$ cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
root:~$ sysctl --system

root:~$ vi /etc/docker/daemon.json
{
    "exec-opts": ["native.cgroupdriver=systemd"],
    "log-driver": "json-file",
    "log-opts": {
        "max-size": "100m"
    },
    "storage-driver": "overlay2"
}

root# mkdir -p /etc/systemd/system/docker.service.d
root# systemctl daemon-reload
root# systemctl restart docker


* Install Kubernetes --------------------------------------------------------------------
root:~$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
root:~$ cat <<EOF > /etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
root:~$ apt update
root:~$ apt install -y kubelet=1.15.7-00 kubeadm=1.15.7-00 kubectl=1.15.7-00
root:~$ apt-mark hold kubelet kubeadm kubectl


#########################################################################################
###                          Setting Kubernetes Master Node                           ###
#########################################################################################
* Cleaning up Kubernetes Environments Completely ----------------------------------------
user:~$ sudo kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
user:~$ sudo kubectl delete node <node name>
user:~$ sudo kubeadm reset \
     && sudo rm -rf /var/lib/cni/ /var/lib/calico/ /var/lib/kubelet/ /var/lib/etcd/ /etc/kubernetes/ /etc/cni/ /var/lib/rook/ \
     && docker system prune -a -f \
     && sudo rm -rf ~/.kube \
     && sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X \
     && docker system df
user:~$ sudo reboot


* Setting Up For Master -----------------------------------------------------------------
user:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.0.101
user:~$ mkdir ~/.kube \
     && sudo cp -i /etc/kubernetes/admin.conf ~/.kube/config \
     && sudo chown -R $(id -u):$(id -g) ~/.kube \
     && kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml \
     && watch kubectl get all --all-namespaces -o wide
user:~$ kubectl taint nodes --all node-role.kubernetes.io/master-
user:~$ kubeadm token create --print-join-command


* Install GPU Node Plugin ---------------------------------------------------------------
  > URL: https://github.com/NVIDIA/k8s-device-plugin
user:~$ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta5/nvidia-device-plugin.yml
user:~$ kubectl get nodes -o=custom-columns=NAME:.metadata.name,GPUs:.status.capacity.'nvidia\.com/gpu'

> using gpu example:
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
    - name: cuda-container
      image: nvidia/cuda:9.0-devel
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 GPUs
    - name: digits-container
      image: nvidia/digits:6.0
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 GPUs


* bash setting for command --------------------------------------------------------------
user:~$ cat >> ~/.bashrc << EOF
alias kc="kubectl"
alias ka="kubeadm"
complete -F __start_kubectl kc

source <(kubectl completion bash)
EOF
user:~$ . ~/.bashrc


* Install Helm 3 ------------------------------------------------------------------------
user:~$ sudo rm -rf /usr/local/bin/helm
user:~$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \
     && chmod u+x get_helm.sh \
     && ./get_helm.sh

user:~$ helm repo add stable https://kubernetes-charts.storage.googleapis.com \
     && helm repo update \
     && helm search repo stable


#########################################################################################
###                                Kubernetes Dashboard                               ###
#########################################################################################
* Deploy Dashboard To k8s ---------------------------------------------------------------
user:~$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.1/aio/deploy/recommended.yaml
user:~$ vi recommended.yaml
      nodeSelector:
        node-role.kubernetes.io/master: ""

user:~$ kubectl apply -f recommended.yaml

user:~$ cat <<EOF | kubectl create -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF


# Create Bearer Token -------------------------------------------------------------------
user:~$ nohup kubectl proxy --port=8001 &
user:~$ kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')


* SSH-Tunneling at client ---------------------------------------------------------------
user:~$ ssh -f -N rsnet@master -L9901:localhost:8001
>>> Connection URL : http://localhost:9901/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/


#########################################################################################
###                                      MetalLB                                      ###
#########################################################################################
* Install MetalLB -----------------------------------------------------------------------
user:~$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml
user:~$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml
user:~$ kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"

* Layer 2 Configuration -----------------------------------------------------------------
user:~$ cat <<EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.0.180-192.168.0.199
EOF

* For Dashboard -----------------------------------------------------------------
user:~$ mkdir certs; cd certs
user:~$ openssl genrsa -des3 -passout pass:testpass -out dashboard.pass.key 2048
user:~$ openssl rsa -passin pass:testpass -in dashboard.pass.key -out dashboard.key
user:~$ rm dashboard.pass.key
user:~$ openssl req -new -key dashboard.key -out dashboard.csr
# Create SSL Certificate
user:~$ openssl x509 -req -sha256 -days 365 -in dashboard.csr -signkey dashboard.key -out dashboard.crt
# Create k8s secret
user:~$ cd .. && kubectl create secret generic kubernetes-dashboard-certs --from-file=./certs -n kube-system
user:~$ vi recommended.yaml
 32 kind: Service
 33 apiVersion: v1
 34 metadata:
 35   labels:
 36     k8s-app: kubernetes-dashboard
 37   name: kubernetes-dashboard
 38   namespace: kubernetes-dashboard
 39 spec:
 40   type: LoadBalancer <-- change here
 41   ports:
 42     - port: 443
 43       targetPort: 8443
 44   selector:
 45     k8s-app: kubernetes-dashboard

user:~$ kubectl apply -f recommended.yaml
user:~$ kubectl get svc --all-namespaces


#########################################################################################
###                            NGINX Ingress Using Helm 3                             ###
#########################################################################################
* Install using Helm 3 ------------------------------------------------------------------
user:~$ helm install rs stable/nginx-ingress \
    --namespace kube-system \
    --set controller.replicaCount=2 \
    --set controller.publishService.enabled=true \
    --set controller.nodeSelector."beta\.kubernetes\.io/os"=linux \
    --set defaultBackend.nodeSelector."beta\.kubernetes\.io/os"=linux
user:~$ kubectl get service -l app=nginx-ingress --namespace kube-system
user:~$ kubectl patch svc rs-nginx-ingress-controller -n kube-system -p '{"spec": {"type": "LoadBalancer", "externalIPs":["192.168.0.101"]}}'


#########################################################################################
###                                  Rook For Cephfs                                  ###
#########################################################################################
※ REF SITE : https://rook.io/docs/rook/v1.1/ceph-filesystem.html

user:~$ rm -rf rook \
     && git clone https://github.com/rook/rook \
     && cd rook/cluster/examples/kubernetes/ceph/ \
     && kc apply -f common.yaml \
     && kc apply -f operator.yaml \
     && watch kubectl get pod -n rook-ceph -o wide
user:~$ kc apply -f cluster.yaml \
     && watch kubectl get pod -n rook-ceph -o wide
user:~$ kc apply -f filesystem.yaml \
     && kc apply -f toolbox.yaml \
     && watch kubectl get all -n rook-ceph
user:~$ kubectl -n rook-ceph get pod -l app=rook-ceph-mds
user:~$ kc exec -n rook-ceph rook-ceph-tools-58df7d6b5c-z77wv -it bash
     >> ceph status
     >> ceph osd status
user:~$ kc apply -f csi/cephfs/storageclass.yaml \
     && kc apply -f csi/cephfs/kube-registry.yaml

user:~$ kubectl patch storageclass rook-cephfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'


#########################################################################################
###                                      Kubeflow                                     ###
#########################################################################################
* Service Account Token Volume Projection -----------------------------------------------
user:~$ sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml
- --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
- --service-account-issuer=kubernetes.default.svc
user:~$ watch kubectl get pod --all-namespaces

* Kubeflow Install ----------------------------------------------------------------------
user:~$ rm -rf kfctl* \
     && wget https://github.com/kubeflow/kfctl/releases/download/v1.0.2/kfctl_v1.0.2-0-ga476281_linux.tar.gz \
     && tar xfv kfctl_v1.0.2-0-ga476281_linux.tar.gz \
     && sudo mv kfctl /usr/local/bin/ \
     && export KF_NAME=kubeflow \
     && export BASE_DIR=/opt \
     && export KF_DIR=${BASE_DIR}/${KF_NAME} \
     && export CONFIG_URI="https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_istio_dex.v1.0.2.yaml" \
     && sudo mkdir -p ${KF_DIR} \
     && sudo chown $USER.$GROUP ${KF_DIR} \
     && cd ${KF_DIR} \
     && rm -rf * \
     && kfctl apply -V -f ${CONFIG_URI}
user:~$ watch kubectl get all -n kubeflow
user:~$ nohup kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 &


* SSH-Tunneling at client ----------------------------------------------------------------
user:~$ ssh -f -N rsnet@master -L9902:localhost:8080
>>> Connection
  > URL : http://localhost:9902
  > ID/PASS : admin@kubeflow.org / 12341234


* Delete Kubeflow -----------------------------------------------------------------------
user:~$ export CONFIG_FILE=${KF_DIR}/kfctl_istio_dex.v1.0.2.yaml
user:~$ cd ${KF_DIR}
user:~$ kfctl delete -f ${CONFIG_FILE}


#########################################################################################
###                                   Elasticsearch                                   ###
#########################################################################################
※ REF SITE : https://github.com/elastic/helm-charts

* Create Namespace ----------------------------------------------------------------------
user:~$ kubectl create namespace elk

* Install elasticsearch -----------------------------------------------------------------
user:~$ wget https://raw.githubusercontent.com/elastic/helm-charts/master/elasticsearch/values.yaml \
     && mv values.yaml elastic_values.yaml
user:~$ helm repo add elastic https://helm.elastic.co \
     && helm repo update \
     && helm search hub elasticsearch \
     && helm install elasticsearch elastic/elasticsearch -f elastic_values.yaml --namespace elk
user:~$ kubectl port-forward -n elk svc/elasticsearch-master 9200:9200 &
user:~$ curl http://localhost:9200/

* Install Filebeat ----------------------------------------------------------------------
user:~$ wget https://raw.githubusercontent.com/elastic/helm-charts/master/filebeat/values.yaml \
     && mv values.yaml filebeat_values.yaml
user:~$ helm install filebeat elastic/filebeat -f filebeat_values.yaml --namespace elk
user:~$ curl http://localhost:9200/_cat/indices

* Install Kibana ------------------------------------------------------------------------
user:~$ wget https://raw.githubusercontent.com/elastic/helm-charts/master/kibana/values.yaml \
     && mv values.yaml kibana_values.yaml
user:~$ helm install kibana elastic/kibana -f kibana_values.yaml --namespace elk
user:~$ kubectl port-forward -n elk svc/kibana-kibana 5601:5601 &

* SSH-Tunneling at client ----------------------------------------------------------------
user:~$ ssh -f -N rsnet@master -L9903:localhost:5601
>>> Connection
  > URL : http://localhost:9903


#########################################################################################
###                             Cert-Manager Using Helm 3                             ###
#########################################################################################
※ probably already installed if cuberflow was installed.

user:~$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.0/cert-manager.crds.yaml
user:~$ kubectl label namespace kube-system certmanager.k8s.io/disable-validation=true
user:~$ helm repo add jetstack https://charts.jetstack.io
user:~$ helm repo update
user:~$ helm install cert-manager jetstack/cert-manager \
    --namespace kube-system \
    --version v0.15.0
user:~$ kubectl get pods --namespace kube-system


#########################################################################################
###                               Rancher Using Helm 3                                ###
#########################################################################################
* Install Rancher with Helm 3 -----------------------------------------------------------
user:~$ helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
user:~$ helm repo update
user:~$ kubectl create namespace cattle-system
user:~$ helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=rancher.realstudy.net
user:~$ kubectl -n cattle-system rollout status deploy/rancher




#########################################################################################
###                               Local Path Provisioner                              ###
#########################################################################################
* Install -------------------------------------------------------------------------------
user:~$ kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml \
     && kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' \
     && kubectl get storageclass




#########################################################################################
###                          Docker Hub For Public & Private                          ###
#########################################################################################
* Public Docker-hub Usage ---------------------------------------------------------------
user:~$ docker login
user:~$ docker build -t <dockerhub_id>/<image_name>:<version> .
  > ex: docker build -t nockchun/docterm:v1 .
user:~$ docker push <dockerhub_id>/<image_name>:<version>
  > ex: docker push nockchun/docterm:v1 .


* Private Docker-hub Install ------------------------------------------------------------
user:~$ docker pull registry
user:~$ docker run -d --name docker-registry --restart=always -p 5000:5000 registry


* Using Private Docker-Hub --------------------------------------------------------------
> Settings For use without https 
user:~$ sudo vi /etc/docker/daemon.json
    > {
    >     "insecure-registries" : ["xx.xx.xx.xx:5000"]
    > }
user:~$ sudo systemctl daemon-reload
user:~$ sudo systemctl restart docker

> Push Image to Private Hub
user:~$ docker tag myhello localhost:5000/myhello
user:~$ docker push localhost:5000/myhello

> Image Check
user:~$ curl -X GET http://xx.xx.xx.xx:5000/v2/_catalog

> Tag Info Check
user:~$ curl -X GET http://xx.xx.xx.xx:5000/v2/hello-world/tags/list
















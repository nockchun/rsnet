https://parksuseong.blogspot.com/2019/04/312-3-standalone-pseudo-distributed.html

#########################################################################################
###                              Private Helm Repository                              ###
#########################################################################################
user:~$ helm create <project>
user:~$ helm package <project>
user:~$ mv project-0.1.0.tgz stable
user:~$ helm repo index stable --url https://nockchun.github.io/rsnet-helm/stable
user:~$ git add .
user:~$ git commit -m "add project chart"
user:~$ git push

user:~$ helm repo add rsnet-helm https://nockchun.github.io/rsnet-helm/stable
user:~$ helm search repo rsnet-helm
user:~$ helm install rsnet-helm/project




#########################################################################################
###                                      Hadoop                                       ###
#########################################################################################
* All Nodes -----------------------------------------------------------------------------
user:~$ sudo vi /etc/hosts
192.168.0.200 vmaster
192.168.0.201 vnode01
192.168.0.202 vnode02

user:~$ sudo apt install openjdk-11-jdk-headless
user:~$ wget http://apache.mirror.cdnetworks.com/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz \
     && tar xfvz hadoop-3.2.1.tar.gz \
     && sudo mv hadoop-3.2.1 /opt/hadoop \
     && rm -rf hadoop-3.2.1.tar.gz \
     && sudo chown -R rsnet.rsnet /opt/hadoop/

user:~$ sudo vi /etc/bash.bashrc
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/opt/hadoop
export HADOOP_PID_DIR=${HADOOP_HOME}/pids
export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar

export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR"

export PATH=$PATH:$HADOOP_HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/sbin
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH

user:~$ mkdir -p ${HADOOP_HOME}/pids
user:~$ vi $HADOOP_HOME/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/opt/hadoop
export HADOOP_PID_DIR=${HADOOP_HOME}/pids

user:~$ vi $HADOOP_HOME/etc/hadoop/core-site.xml
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://vmaster:9000</value>
    </property>
</configuration>

user:~$ vi $HADOOP_HOME/etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///opt/hadoop/data/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///opt/hadoop/data/datanode</value>
    </property>
</configuration>

user:~$ vi $HADOOP_HOME/etc/hadoop/workers
vnode01
vnode02

user:~$ cd $HADOOP_HOME/share/hadoop/mapreduce/lib
user:~$ wget https://repo1.maven.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1.jar

user:~$ vi $HADOOP_HOME/etc/hadoop/yarn-site.xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>vmaster</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.acl.enable</name>
        <value>0</value>
    </property>
    <property>
        <name>yarn.application.classpath</name>
        <value>$HADOOP_CONF_DIR,${HADOOP_COMMON_HOME}/share/hadoop/common/*,${HADOOP_COMMON_HOME}/share/hadoop/common/lib/*,${HADOOP_HDFS_HOME}/share/hadoop/hdfs/*,${HADOOP_HDFS_HOME}/share/hadoop/hdfs/lib/*,${HADOOP_MAPRED_HOME}/share/hadoop/mapreduce/*,${HADOOP_MAPRED_HOME}/share/hadoop/mapreduce/lib/*,${HADOOP_YARN_HOME}/share/hadoop/yarn/*,${HADOOP_YARN_HOME}/share/hadoop/yarn/lib/*</value>
    </property>
</configuration>

user:~$ vi $HADOOP_HOME/etc/hadoop/mapred-site.xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
    </property>
</configuration>

* At Master Node ------------------------------------------------------------------------
user:~$ ssh-keygen -t rsa
user:~$ ssh-copy-id rsnet@vmaster
user:~$ ssh-copy-id rsnet@vnode01
user:~$ ssh-copy-id rsnet@vnode02

user:~$ hdfs namenode -format
user:~$ start-dfs.sh
>> test : http://vmaster:9870
user:~$ start-yarn.sh
>> test : http://vmaster:8088
user:~$ mapred --daemon start historyserver
>> test : http://vmaster:19888


#########################################################################################
###                                       Sprk                                        ###
#########################################################################################
* All Nodes -----------------------------------------------------------------------------
user:~$ wget http://apache.mirror.cdnetworks.com/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz \
     && tar xfvz spark-3.0.0-bin-hadoop3.2.tgz \
     && sudo mv spark-3.0.0-bin-hadoop3.2 /opt/spark \
     && rm -rf spark-3.0.0-bin-hadoop3.2.tgz \
     && sudo chown -R rsnet.rsnet /opt/spark/

user:~$ sudo vi /etc/bash.bashrc
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin

user:~$ cd $SPARK_HOME/conf
user:~$ cp spark-env.sh.template spark-env.sh
user:~$ vi spark-env.sh
export HADOOP_HOME=/opt/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PYSPARK_PYTHON=/usr/bin/python3
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3
export SPARK_HOME=/opt/spark
export SPARK_MASTER_HOST=vmaster
export SPARK_LOG_DIR=${SPARK_HOME}/logs
export SPARK_PID_DIR=${SPARK_HOME}/pids

user:~$ cp slaves.template slaves
user:~$ vi slaves
vnode01
vnode02

user:~$ wget https://downloads.lightbend.com/scala/2.12.11/scala-2.12.11.deb \
     && sudo dpkg -i scala-2.12.11.deb \
     && rm -rf scala-2.12.11.deb \
     && sudo apt update \
     && sudo apt install scala

user:~$ $SPARK_HOME/sbin/start-all.sh
>> test : http://vmaster:8080/


#########################################################################################
###                                       LIVY                                        ###
#########################################################################################
* At Master Node ------------------------------------------------------------------------
user:~$ wget http://apache.mirror.cdnetworks.com/incubator/livy/0.7.0-incubating/apache-livy-0.7.0-incubating-bin.zip \
     && unzip apache-livy-0.7.0-incubating-bin.zip \
     && sudo mv apache-livy-0.7.0-incubating-bin /opt/livy \
     && rm -rf apache-livy-0.7.0-incubating-bin.zip \
     && sudo chown -R rsnet.rsnet /opt/livy/

user:~$ sudo vi /etc/bash.bashrc
export LIVY_HOME=/opt/livy
export PATH=$PATH:$LIVY_HOME/bin

user:~$ cd $LIVY_HOME/conf \
     && cp livy.conf.template livy.conf \
     && vi livy.conf
livy.spark.master = spark://vmaster:7077

user:~$ cp livy-env.sh.template livy-env.sh \
     && vi $LIVY_HOME/conf/livy-env.sh
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export SPARK_HOME=/opt/spark
export SPARK_CONF_DIR=$SPARK_HOME/conf
export HADOOP_HOME=/opt/hadoop
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native
export HADOOP_OPTS="${HADOOP_OPTS} -Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR"

user:~$ livy-server start









export HADOOP_PID_DIR=${HADOOP_HOME}/pids
export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar

export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME

export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR"

export PATH=$PATH:$HADOOP_HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/sbin
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH












user:~$ livy-server start

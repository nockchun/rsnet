#########################################################################################
###                                  Basic Settings                                   ###
#########################################################################################
* Change Host Name & Hosts Info ---------------------------------------------------------
root:~$ hostnamectl set-hostname node19
root:~$ cat >> /etc/hosts << EOF
10.10.1.101 master.lab master
10.10.1.111 node01.lab node01
10.10.1.112 node02.lab node02
EOF

* Bash Function For ssh tunnel ----------------------------------------------------------
user:~$ vi .bashrc
tunnelfunction() {
  pkill -9 -ef asa@master

  ssh -f -N asa@master -L9901:localhost:8001
  ssh -f -N asa@master -L9902:localhost:8080
  ssh -f -N asa@master -L9903:localhost:5601
}




#########################################################################################
###                      Docker, Nvidia-Docker2 For Kubeflow 1.0                      ###
#########################################################################################
※ Kuberflow 1.0 system requirements
  > kubeflow 1.0 == docker-ce-18.09.8 + Kubernetes 1.15

* Docker & Docker-Compose Install -------------------------------------------------------
user:~$ sudo apt install -y apt-transport-https ca-certificates curl software-properties-common \
     && curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - \
     && sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable" \
     && sudo apt update \
     && sudo apt install -y docker-ce=5:18.09.9~3-0~ubuntu-bionic docker-ce-cli=5:18.09.9~3-0~ubuntu-bionic containerd.io=1.2.10-3 \
     && sudo apt-mark hold docker-ce docker-ce-cli \
     && sudo usermod -aG docker $USER \
     && sudo apt install -y docker-compose


* Nvidia-Docker Install ----------------------------------------------------------------
user:~$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
user:~$ sudo vi /etc/apt/sources.list.d/nvidia-docker.list
deb https://nvidia.github.io/libnvidia-container/ubuntu18.04/$(ARCH) /
deb https://nvidia.github.io/nvidia-container-runtime/ubuntu18.04/$(ARCH) /
deb https://nvidia.github.io/nvidia-docker/ubuntu18.04/$(ARCH) /
user:~$ sudo apt update
user:~$ sudo apt install -y nvidia-docker2


* Setting Docker For nvidia gpu --------------------------------------------------------
user:~$ sudo vi /etc/docker/daemon.json
{
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}
user:~$ sudo systemctl daemon-reload \
     && sudo systemctl restart docker




#########################################################################################
###                            Kubernetes For Kubeflow 1.0                            ###
#########################################################################################
* swap & sysctl settings for Kubernetes networking --------------------------------------
root:~$ swapoff -a \
     && sed -i '/swap/d' /etc/fstab \
     && cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
root:~$ sysctl --system

root:~$ vi /etc/docker/daemon.json
{
    "exec-opts": ["native.cgroupdriver=systemd"],
    "log-driver": "json-file",
    "log-opts": {
        "max-size": "100m"
    },
    "storage-driver": "overlay2"
}

root:~$ mkdir -p /etc/systemd/system/docker.service.d \
     && systemctl daemon-reload \
     && systemctl restart docker


* Install Kubernetes --------------------------------------------------------------------
root:~$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
root:~$ cat <<EOF > /etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
root:~$ apt update \
     && apt install -y kubelet=1.15.7-00 kubeadm=1.15.7-00 kubectl=1.15.7-00 \
     && apt-mark hold kubelet kubeadm kubectl




#########################################################################################
###                          Setting Kubernetes Master Node                           ###
#########################################################################################
* Cleaning up Kubernetes Environments Completely ----------------------------------------
user:~$ sudo kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
user:~$ sudo kubectl delete node <node name>
user:~$ sudo kubeadm reset \
     && sudo rm -rf /var/lib/cni/ /var/lib/calico/ /var/lib/kubelet/ /var/lib/etcd/ /etc/kubernetes/ /etc/cni/ /var/lib/rook/ \
     && docker system prune -a -f \
     && sudo rm -rf ~/.kube \
     && sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X \
     && docker system df
user:~$ sudo reboot


* Setting Up For Master -----------------------------------------------------------------
user:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.0.101
user:~$ mkdir ~/.kube \
     && sudo cp -i /etc/kubernetes/admin.conf ~/.kube/config \
     && sudo chown -R $(id -u):$(id -g) ~/.kube \
     && kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml \
     && watch kubectl get all --all-namespaces -o wide
user:~$ kubectl taint nodes --all node-role.kubernetes.io/master-
user:~$ kubeadm token create --print-join-command


* Install GPU Node Plugin ---------------------------------------------------------------
  > URL: https://github.com/NVIDIA/k8s-device-plugin
user:~$ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta5/nvidia-device-plugin.yml
user:~$ kubectl get nodes -o=custom-columns=NAME:.metadata.name,GPUs:.status.capacity.'nvidia\.com/gpu'

> using gpu example:
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
    - name: cuda-container
      image: nvidia/cuda:9.0-devel
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 GPUs
    - name: digits-container
      image: nvidia/digits:6.0
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 GPUs


* bash setting for command --------------------------------------------------------------
user:~$ cat >> ~/.bashrc << EOF
alias kc="kubectl"
alias ka="kubeadm"
complete -F __start_kubectl kc

source <(kubectl completion bash)
EOF
user:~$ . ~/.bashrc


* Install Helm 3 ------------------------------------------------------------------------
user:~$ sudo rm -rf /usr/local/bin/helm
user:~$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \
     && chmod u+x get_helm.sh \
     && ./get_helm.sh

user:~$ helm repo add stable https://kubernetes-charts.storage.googleapis.com \
     && helm repo update \
     && helm search repo stable




#########################################################################################
###                               Local Path Provisioner                              ###
#########################################################################################
* Install -------------------------------------------------------------------------------
user:~$ kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml \
     && kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' \
     && kubectl get storageclass




#########################################################################################
###                                Kubernetes Dashboard                               ###
#########################################################################################
* Deploy Dashboard To k8s ---------------------------------------------------------------
user:~$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.1/aio/deploy/recommended.yaml
user:~$ vi recommended.yaml
      nodeSelector:
        node-role.kubernetes.io/master: ""

user:~$ kubectl apply -f recommended.yaml

user:~$ cat <<EOF | kubectl create -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF


# Create Bearer Token -------------------------------------------------------------------
user:~$ nohup kubectl proxy --port=8001 &
user:~$ kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')


* SSH-Tunneling at client ---------------------------------------------------------------
user:~$ ssh -f -N rsnet@master -L9901:localhost:8001
>>> Connection URL : http://localhost:9901/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/




#########################################################################################
###                                      Kubeflow                                     ###
#########################################################################################
* Service Account Token Volume Projection -----------------------------------------------
user:~$ sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml
- --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
- --service-account-issuer=kubernetes.default.svc
user:~$ watch kubectl get pod --all-namespaces


* Kubeflow Install ----------------------------------------------------------------------
user:~$ rm -rf kfctl* \
     && wget https://github.com/kubeflow/kfctl/releases/download/v1.0.2/kfctl_v1.0.2-0-ga476281_linux.tar.gz \
     && tar xfv kfctl_v1.0.2-0-ga476281_linux.tar.gz \
     && sudo mv kfctl /usr/local/bin/ \
     && export KF_NAME=kubeflow \
     && export BASE_DIR=/opt \
     && export KF_DIR=${BASE_DIR}/${KF_NAME} \
     && export CONFIG_URI="https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_istio_dex.v1.0.2.yaml" \
     && sudo mkdir -p ${KF_DIR} \
     && sudo chown $USER.$GROUP ${KF_DIR} \
     && cd ${KF_DIR} \
     && rm -rf * \
     && kfctl apply -V -f ${CONFIG_URI}
user:~$ watch kubectl get all -n kubeflow -o wide
user:~$ nohup kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 &

user:~$ cat <<EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-kubeflow
  namespace: istio-system
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: kubeflow.rsnet
    http:
      paths:
      - path: /
        backend:
          serviceName: istio-ingressgateway
          servicePort: 8080
  tls:
      - hosts:
          - kubeflow.rsnet
        secretName: istio-ingressgateway-certs
EOF



* SSH-Tunneling at client ----------------------------------------------------------------
user:~$ ssh -f -N rsnet@master -L9902:localhost:8080
>>> Connection
  > URL : http://localhost:9902
  > ID/PASS : admin@kubeflow.org / 12341234


* Delete Kubeflow -----------------------------------------------------------------------
user:~$ export CONFIG_FILE=${KF_DIR}/kfctl_istio_dex.v1.0.2.yaml
user:~$ cd ${KF_DIR}
user:~$ kfctl delete -f ${CONFIG_FILE}


* Expose with a loadbalancer (istio-ingress) --------------------------------------------
user:~$ kubectl patch service -n istio-system istio-ingressgateway -p '{"spec": {"type": "LoadBalancer", "externalIPs":["192.168.0.101"]}}'

user:~$ cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: istio-ingressgateway-certs
  namespace: istio-system
spec:
  commonName: istio-ingressgateway.istio-system.svc
  # Use ipAddresses if your LoadBalancer issues an IP
  ipAddresses:
  - 192.168.0.101
  # Use dnsNames if your LoadBalancer issues a hostname (eg on AWS)
  dnsNames:
  - kubeflow.rsnet
  isCA: true
  issuerRef:
    kind: ClusterIssuer
    name: kubeflow-self-signing-issuer
  secretName: istio-ingressgateway-certs
EOF




#########################################################################################
###                             Cert-Manager Using Helm 3                             ###
#########################################################################################
※ probably already installed if cuberflow was installed.

user:~$ kubectl create namespace cert-manager

user:~$ helm repo add jetstack https://charts.jetstack.io \
     && helm repo update \
     && kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.crds.yaml
user:~$ helm install cert-manager jetstack/cert-manager \
    --namespace cert-manager \
    --version v0.15.1
user:~$ kubectl get pods --namespace cert-manager




#########################################################################################
###                            NGINX Ingress Using Helm 3                             ###
#########################################################################################
* Install using Helm 3 ------------------------------------------------------------------
user:~$ kubectl create namespace nginx-ingress
user:~$ helm install rsingress stable/nginx-ingress \
    --namespace nginx-ingress \
    --set controller.replicaCount=2 \
    --set controller.publishService.enabled=true \
    --set controller.nodeSelector."beta\.kubernetes\.io/os"=linux \
    --set defaultBackend.nodeSelector."beta\.kubernetes\.io/os"=linux
user:~$ kubectl get service -l app=nginx-ingress --namespace nginx-ingress
user:~$ kubectl patch svc rsingress-nginx-ingress-controller -n nginx-ingress -p '{"spec": {"type": "LoadBalancer", "externalIPs":["192.168.0.101"]}}'




#########################################################################################
###                                   Elasticsearch                                   ###
#########################################################################################
※ REF SITE : https://github.com/elastic/helm-charts

* Create Namespace ----------------------------------------------------------------------
user:~$ kubectl create namespace elk

* Install elasticsearch -----------------------------------------------------------------
user:~$ wget https://raw.githubusercontent.com/elastic/helm-charts/master/elasticsearch/values.yaml \
     && mv values.yaml elastic_values.yaml
user:~$ helm repo add elastic https://helm.elastic.co \
     && helm repo update \
     && helm search hub elasticsearch \
     && helm install elasticsearch elastic/elasticsearch -f elastic_values.yaml --namespace elk
user:~$ watch kubectl get all -n elk -o wide
user:~$ kubectl port-forward -n elk svc/elasticsearch-master 9200:9200 &
user:~$ curl http://localhost:9200/


* Install Filebeat ----------------------------------------------------------------------
user:~$ wget https://raw.githubusercontent.com/elastic/helm-charts/master/filebeat/values.yaml \
     && mv values.yaml filebeat_values.yaml
user:~$ helm install filebeat elastic/filebeat -f filebeat_values.yaml --namespace elk
user:~$ curl http://localhost:9200/_cat/indices


* Install Kibana ------------------------------------------------------------------------
user:~$ wget https://raw.githubusercontent.com/elastic/helm-charts/master/kibana/values.yaml \
     && mv values.yaml kibana_values.yaml
user:~$ helm install kibana elastic/kibana -f kibana_values.yaml --namespace elk
user:~$ kubectl port-forward -n elk svc/kibana-kibana 5601:5601 &

user:~$ cat <<EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-elasticsearch
  namespace: elk
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: elastic.rsnet
    http:
      paths:
      - path: /
        backend:
          serviceName: elasticsearch-master
          servicePort: 9200
  - host: kibana.rsnet
    http:
      paths:
      - path: /
        backend:
          serviceName: kibana-kibana
          servicePort: 5601
EOF


* Install Plugin ------------------------------------------------------------------------
/elastisearch/bin/plugin -install mobz/elasticsearch-head
/elastisearch/bin/plugin -install lukas-vlcek/bigdesk
/elastisearch/bin/plugin -install enezes/elasticsearch-kopf




#########################################################################################
###                                       Spark                                       ###
#########################################################################################
※ REF SITE : https://github.com/bitnami/charts/tree/master/bitnami/spark

* Create Namespace ----------------------------------------------------------------------
user:~$ kubectl create namespace spark

* Install elasticsearch -----------------------------------------------------------------
user:~$ wget https://raw.githubusercontent.com/bitnami/charts/master/bitnami/spark/values.yaml \
     && mv values.yaml spark_values.yaml
user:~$ helm repo add bitnami https://charts.bitnami.com/bitnami \
     && helm repo update \
     && helm install spark bitnami/spark -f spark_values.yaml --namespace spark
user:~$ nohup kubectl port-forward --namespace spark svc/spark-master-svc 9080:80 &

user:~$ cat <<EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-spark
  namespace: spark
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: spark.rsnet
    http:
      paths:
      - path: /
        backend:
          serviceName: spark-master-svc
          servicePort: 80
EOF




#########################################################################################
###                                      MetalLB                                      ###
#########################################################################################
* Install MetalLB -----------------------------------------------------------------------
user:~$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml
user:~$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml
user:~$ kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"

* Layer 2 Configuration -----------------------------------------------------------------
user:~$ cat <<EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.0.180-192.168.0.199
EOF

* For Dashboard -----------------------------------------------------------------
user:~$ mkdir certs; cd certs
user:~$ openssl genrsa -des3 -passout pass:testpass -out dashboard.pass.key 2048
user:~$ openssl rsa -passin pass:testpass -in dashboard.pass.key -out dashboard.key
user:~$ rm dashboard.pass.key
user:~$ openssl req -new -key dashboard.key -out dashboard.csr
# Create SSL Certificate
user:~$ openssl x509 -req -sha256 -days 365 -in dashboard.csr -signkey dashboard.key -out dashboard.crt
# Create k8s secret
user:~$ cd .. && kubectl create secret generic kubernetes-dashboard-certs --from-file=./certs -n kube-system
user:~$ vi recommended.yaml
 32 kind: Service
 33 apiVersion: v1
 34 metadata:
 35   labels:
 36     k8s-app: kubernetes-dashboard
 37   name: kubernetes-dashboard
 38   namespace: kubernetes-dashboard
 39 spec:
 40   type: LoadBalancer <-- change here
 41   ports:
 42     - port: 443
 43       targetPort: 8443
 44   selector:
 45     k8s-app: kubernetes-dashboard

user:~$ kubectl apply -f recommended.yaml
user:~$ kubectl get svc --all-namespaces




#########################################################################################
###                                  Rook For Cephfs                                  ###
#########################################################################################
※ REF SITE : https://rook.io/docs/rook/v1.1/ceph-filesystem.html

user:~$ rm -rf rook \
     && git clone https://github.com/rook/rook \
     && cd rook/cluster/examples/kubernetes/ceph/ \
     && kc apply -f common.yaml \
     && kc apply -f operator.yaml \
     && watch kubectl get pod -n rook-ceph -o wide
user:~$ kc apply -f cluster.yaml \
     && watch kubectl get pod -n rook-ceph -o wide
user:~$ kc apply -f filesystem.yaml \
     && kc apply -f toolbox.yaml \
     && watch kubectl get all -n rook-ceph
user:~$ kubectl -n rook-ceph get pod -l app=rook-ceph-mds
user:~$ kc exec -n rook-ceph rook-ceph-tools-58df7d6b5c-z77wv -it bash
     >> ceph status
     >> ceph osd status
user:~$ kc apply -f csi/cephfs/storageclass.yaml \
     && kc apply -f csi/cephfs/kube-registry.yaml

user:~$ kubectl patch storageclass rook-cephfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'




#########################################################################################
###                               Rancher Using Helm 3                                ###
#########################################################################################
* Install Rancher with Helm 3 -----------------------------------------------------------
user:~$ helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
user:~$ helm repo update
user:~$ kubectl create namespace cattle-system
user:~$ helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=rancher.realstudy.net
user:~$ kubectl -n cattle-system rollout status deploy/rancher




#########################################################################################
###                          Docker Hub For Public & Private                          ###
#########################################################################################
* Public Docker-hub Usage ---------------------------------------------------------------
user:~$ docker login
user:~$ docker tag SOURCE_IMAGE[:TAG] DOCKER HUB ID/TARGET_IMAGE[:TAG]
  > ex: docker tag docterm:last nockchun/docterm:1.0
user:~$ docker push <dockerhub_id>/<image_name>:<version>
  > ex: docker push nockchun/docterm:1.0


* Private Docker-hub Install ------------------------------------------------------------
user:~$ docker pull registry
user:~$ docker run -d --name docker-registry --restart=always -p 5000:5000 registry


* Using Private Docker-Hub --------------------------------------------------------------
> Settings For use without https 
user:~$ sudo vi /etc/docker/daemon.json
    > {
    >     "insecure-registries" : ["xx.xx.xx.xx:5000"]
    > }
user:~$ sudo systemctl daemon-reload
user:~$ sudo systemctl restart docker

> Push Image to Private Hub
user:~$ docker tag myhello localhost:5000/myhello
user:~$ docker push localhost:5000/myhello

> Image Check
user:~$ curl -X GET http://xx.xx.xx.xx:5000/v2/_catalog

> Tag Info Check
user:~$ curl -X GET http://xx.xx.xx.xx:5000/v2/hello-world/tags/list



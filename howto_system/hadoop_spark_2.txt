https://parksuseong.blogspot.com/2019/04/312-3-standalone-pseudo-distributed.html

#########################################################################################
###                              Private Helm Repository                              ###
#########################################################################################
user:~$ helm create <project>
user:~$ helm package <project>
user:~$ mv project-0.1.0.tgz stable
user:~$ helm repo index stable --url https://nockchun.github.io/rsnet-helm/stable
user:~$ git add .
user:~$ git commit -m "add project chart"
user:~$ git push

user:~$ helm repo add rsnet-helm https://nockchun.github.io/rsnet-helm/stable
user:~$ helm search repo rsnet-helm
user:~$ helm install rsnet-helm/project




#########################################################################################
###                                      Hadoop                                       ###
#########################################################################################
* All Nodes -----------------------------------------------------------------------------
user:~$ sudo vi /etc/hosts
192.168.0.200 vmaster
192.168.0.201 vnode01
192.168.0.202 vnode02

user:~$ sudo apt install openjdk-8-jdk-headless
user:~$ wget http://apache.mirror.cdnetworks.com/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz \
     && tar xfvz hadoop-2.7.7.tar.gz \
     && sudo mv hadoop-2.7.7 /opt/hadoop \
     && rm -rf hadoop-2.7.7.tar.gz \
     && sudo chown -R rsnet.rsnet /opt/hadoop/

user:~$ sudo vi /etc/bash.bashrc
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/opt/hadoop
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_LOG_DIR=${HADOOP_HOME}/logs

export PATH=$PATH:$HADOOP_HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/sbin
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH


user:~$ vi $HADOOP_CONF_DIR/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/opt/hadoop
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop


user:~$ vi $HADOOP_CONF_DIR/masters
vmaster

user:~$ vi $HADOOP_CONF_DIR/slaves
vnode01
vnode02


user:~$ vi $HADOOP_CONF_DIR/core-site.xml
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://vmaster:9000</value>
    </property>
</configuration>


user:~$ vi $HADOOP_CONF_DIR/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///opt/hadoop/data/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///opt/hadoop/data/datanode</value>
    </property>
    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property>
</configuration>


user:~$ vi $HADOOP_CONF_DIR/yarn-site.xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>vmaster</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.acl.enable</name>
        <value>0</value>
    </property>
</configuration>

user:~$ cp $HADOOP_CONF_DIR/mapred-site.xml.template $HADOOP_CONF_DIR/mapred-site.xml
user:~$ vi $HADOOP_CONF_DIR/mapred-site.xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

* At Master Node ------------------------------------------------------------------------
user:~$ ssh-keygen -t rsa
user:~$ ssh-copy-id rsnet@vmaster
user:~$ ssh-copy-id rsnet@vnode01
user:~$ ssh-copy-id rsnet@vnode02

user:~$ hdfs namenode -format
user:~$ start-dfs.sh
>> test : http://vmaster:50070
user:~$ start-yarn.sh
>> test : http://vmaster:8088
user:~$ mr-jobhistory-daemon.sh start historyserver
>> test : http://vmaster:19888





#########################################################################################
###                                       Sprk                                        ###
#########################################################################################
* All Nodes -----------------------------------------------------------------------------
user:~$ wget http://apache.mirror.cdnetworks.com/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz \
     && tar xfvz spark-2.4.6-bin-hadoop2.7.tgz \
     && sudo mv spark-2.4.6-bin-hadoop2.7 /opt/spark \
     && rm -rf spark-2.4.6-bin-hadoop2.7.tgz \
     && sudo chown -R rsnet.rsnet /opt/spark/

user:~$ sudo vi /etc/bash.bashrc
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin

user:~$ cd $SPARK_HOME/conf
user:~$ cp spark-env.sh.template spark-env.sh
user:~$ vi spark-env.sh
export HADOOP_HOME=/opt/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PYSPARK_PYTHON=/usr/bin/python3
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3
export SPARK_HOME=/opt/spark
export SPARK_MASTER_HOST=vmaster
export SPARK_LOG_DIR=${SPARK_HOME}/logs
export SPARK_PID_DIR=${SPARK_HOME}/pids

user:~$ cp slaves.template slaves
user:~$ vi slaves
vnode01
vnode02

user:~$ sudo apt install scala

user:~$ $SPARK_HOME/sbin/start-all.sh
>> test : http://vmaster:8080/


#########################################################################################
###                                       LIVY                                        ###
#########################################################################################
* At Master Node ------------------------------------------------------------------------
user:~$ wget http://apache.mirror.cdnetworks.com/incubator/livy/0.7.0-incubating/apache-livy-0.7.0-incubating-bin.zip \
     && unzip apache-livy-0.7.0-incubating-bin.zip \
     && sudo mv apache-livy-0.7.0-incubating-bin /opt/livy \
     && rm -rf apache-livy-0.7.0-incubating-bin.zip \
     && sudo chown -R rsnet.rsnet /opt/livy/

user:~$ sudo vi /etc/bash.bashrc
export LIVY_HOME=/opt/livy
export PATH=$PATH:$LIVY_HOME/bin

user:~$ cd $LIVY_HOME/conf \
     && cp livy.conf.template livy.conf \
     && vi livy.conf
livy.spark.master = spark://vmaster:7077

user:~$ cp livy-env.sh.template livy-env.sh \
     && vi $LIVY_HOME/conf/livy-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export SPARK_HOME=/opt/spark
export SPARK_CONF_DIR=$SPARK_HOME/conf
export HADOOP_HOME=/opt/hadoop
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native
export HADOOP_OPTS="${HADOOP_OPTS} -Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR"

user:~$ livy-server start
>> test : http://vmaster:8998


#########################################################################################
###                                Jupyterlab Setting                                 ###
#########################################################################################
user:~$ conda install sparkmagic
user:~$ jupyter nbextension enable --py --sys-prefix widgetsnbextension
user:~$ cd /opt/conda/lib/python3.7/site-packages/
user:~$ jupyter-kernelspec install sparkmagic/kernels/sparkkernel
user:~$ jupyter-kernelspec install sparkmagic/kernels/pysparkkernel
user:~$ jupyter-kernelspec install sparkmagic/kernels/sparkrkernel
user:~$ vi ~/.sparkmagic/config.json
{
  "kernel_python_credentials" : {
    "username": "",
    "password": "",
    "url": "http://192.168.0.200:8998",
    "auth": "None"
  },

  "kernel_scala_credentials" : {
    "username": "",
    "password": "",
    "url": "http://192.168.0.200:8998",
    "auth": "None"
  },
  "kernel_r_credentials": {
    "username": "",
    "password": "",
    "url": "http://192.168.0.200:8998"
  },

  "logging_config": {
    "version": 1,
    "formatters": {
      "magicsFormatter": { 
        "format": "%(asctime)s\t%(levelname)s\t%(message)s",
        "datefmt": ""
      }
    },
    "handlers": {
      "magicsHandler": { 
        "class": "hdijupyterutils.filehandler.MagicsFileHandler",
        "formatter": "magicsFormatter",
        "home_path": "~/.sparkmagic"
      }
    },
    "loggers": {
      "magicsLogger": { 
        "handlers": ["magicsHandler"],
        "level": "DEBUG",
        "propagate": 0
      }
    }
  },

  "wait_for_idle_timeout_seconds": 15,
  "livy_session_startup_timeout_seconds": 60,

  "fatal_error_suggestion": "The code failed because of a fatal error:\n\t{}.\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.",

  "ignore_ssl_errors": false,

  "session_configs": {
    "driverMemory": "1000M",
    "executorCores": 2
  },

  "use_auto_viz": true,
  "coerce_dataframe": true,
  "max_results_sql": 2500,
  "pyspark_dataframe_encoding": "utf-8",
  
  "heartbeat_refresh_seconds": 30,
  "livy_server_heartbeat_timeout_seconds": 0,
  "heartbeat_retry_seconds": 10,

  "server_extension_default_kernel_name": "pysparkkernel",
  "custom_headers": {},
  
  "retry_policy": "configurable",
  "retry_seconds_to_sleep_list": [0.2, 0.5, 1, 3, 5],
  "configurable_retry_policy_max_retries": 8
}




https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-hadoop